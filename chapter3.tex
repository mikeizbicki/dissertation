\documentclass[../main.tex]{subfiles}
 
\newcommand{\set}[1]{\mathcal {#1}}
\newcommand{\distribution}[1]{\mathcal {#1}}
\newcommand{\sized}[1]{\tilde \set {#1}}

\newcommand{\radius}{r}
\newcommand{\dist}[2]{\distf({#1},{#2})}
\newcommand{\distf}{d}
\newcommand{\diam}[1]{\textnormal{diam}({#1})}
\newcommand{\codiam}[1]{\textnormal{codiam}({#1})}
\newcommand{\aspect}[1]{\Delta}

\newcommand{\minkdim}{\text{dim}_\textnormal{Mink}}
\newcommand{\krdim}{\text{dim}_\textnormal{exp}}
\newcommand{\doubdim}{\text{dim}_\textnormal{doub}}
\newcommand{\dualdim}{\text{dim}_{\textnormal{doub}^*}}
\newcommand{\holedim}{\text{dim}_{\textnormal{hole}}}

\newcommand{\cexp}{c_\textnormal{exp}}
\newcommand{\cdoub}{c_\textnormal{doub}}
\newcommand{\cdoubstar}{c_{\textnormal{doub}^*}}
\newcommand{\chole}{c_\textnormal{hole}}

\newcommand{\p}{\ensuremath p}
\newcommand{\q}{\ensuremath q}
%\newcommand{\varfont}[1]{\ensuremath{\textup{\text{{#1}}}}}
\newcommand{\mkfunction}[1]{\ifmmode{\textnormal{{#1}}}}
\newcommand{\level}[1]      {\mkfunction{level}({#1})}
\newcommand{\parent}[1]     {\mkfunction{parent}({#1})}
\newcommand{\children}[1]   {\mkfunction{children}({#1})}
\newcommand{\covdist}[1]    {\mkfunction{covdist}({#1})}
\newcommand{\descendants}[1]{\mkfunction{descendants}({#1})}
\newcommand{\maxdist}[1]    {\mkfunction{maxdist}({#1})}
\newcommand{\height}[1]     {\mkfunction{height}({#1})}
\newcommand{\data}[1]       {\mkfunction{data}({#1})}
\newcommand{\datapoint}[1]  {\mkfunction{dp}({#1})}
\newcommand{\nn}[1]         {\mkfunction{nn}[{#1}]}
\makeatletter
\def\nn{\@ifstar\@nn\@@nn}
\def\@nn#1{\mkfunction{nn}^*[{#1}]}
\def\@@nn#1{\mkfunction{nn}[{#1}]}
\makeatother

% FIXME: should these be changed?
\newcommand{\mkprocedure}[1]{\textnormal{\ttfamily {#1}}}
\newcommand{\mergeinsert}{\mkprocedure{merge\_insert}}
\newcommand{\ctmergeloop}{\mkprocedure{merge\_loop}}
\newcommand{\findnnloop}{\mkprocedure{findnn\_loop}}
\newcommand{\findnn}{\mkprocedure{findnn}}
\newcommand{\findnnorig}{\mkprocedure{findnn\_orig}}

%\newcommand{\nn}[1]{\ensuremath{\ensuremath{{{#1}}_{nn}}}}
\newcommand{\exprad}[1]{\ensuremath{\ensuremath{2}}}
\newcommand{\pack}{\ensuremath{\textnormal{\ttfamily pack}}}
\newcommand{\rmNodes}{\ensuremath{\textnormal{\ttfamily rmNodes}}}
\newcommand{\dualnn}{\ensuremath{\textnormal{\ttfamily dualTreeNN}}}
\newcommand{\ctmerge}{\ensuremath{\textnormal{\ttfamily merge}}}
\newcommand{\ctinsert}{\ensuremath{\textnormal{\ttfamily insert}}}
\newcommand{\ctinsertloop}{\ensuremath{\textnormal{\ttfamily insert\_loop}}}
\newcommand{\rebalance}{\ensuremath{\textnormal{\ttfamily rebalance}}}
\newcommand{\rebalanceHelper}{\ensuremath{\textnormal{\ttfamily rebalance\_}}}
\newcommand{\mkvar}[1]{\ensuremath{\textnormal{\emph{{#1}}}}}
\newcommand{\nullvar}{\ensuremath{\textup{\textnormal{\ttfamily null}}}}
%\newcommand{\datapoint}[1]{\ensuremath{\textup{\textnormal{\ttfamily dp}({#1})}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\chapter{Cover Trees}

\begin{definition}
    A set $\set X$ equipped with a distance function $\distf : \set X \times \set X \to \R$ is a \emph{metric space} if it obeys the following properties:
    \begin{enumerate}
        %\item \emph{Non-negativity}.  For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} \ge 0$.
        \item \emph{Indiscernability}.  For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} = 0$ if and only if $x_1=x_2$.
        \item \emph{Symmetry}. For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} = \dist{x_2}{x_2}$.
        \item \emph{Triangle inequality}.  For all $x_1,x_2,x_3\in\set X$, $\dist{x_1}{x_2} + \dist{x_2}{x_3}\ge\dist{x_1}{x_3}$.
    \end{enumerate}
\end{definition}

\begin{table}[H]
    \small
    %\newcolumntype{Y}{>{\centering\arraybackslash}p{1in}}
    %\newcolumntype{Y}{p{0.1in}}
    %\begin{tabularx}{\textwidth}{lXXY}
    \centering
    \begin{tabular}{lccc}
        \toprule
        \vspace{-0.25in}
        &~\hspace{1.2in}~&~\hspace{1.2in}~&~\hspace{1.2in}~\\
        data structure & space & find nearest neighbor & insertion \\
        \midrule
        ball tree \cite{} & $O(n)$ & $O(n)$ & $O(n)$ \\
        metric skip list \cite{karger2002finding} & $O(n\log n)$ & $\cexp{}^{O(1)}\log n$ & $\cexp^{O(1)}\log n\log\log n$ \\
        navigating net \cite{} & $O(n)$ \\
        cover tree \cite{} & $O(n)$ & $O(\cexp^8\log n)$ & $O(\cexp^{12}\log n)$ \\
        simplified cover tree & $O(n)$ & $O(\cdoub{}\log \aspect{})$ \\
                              &        & $O(\cdoub{}\log n)$ \\
        \bottomrule
    \end{tabular}
    %\end{tabularx}
    \caption{
        Summary of the runtime and space usage of several nearest neighbor data structures.
        Here $n$ represents the size of the dataset.
    }
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods for faster nearest neighbors}

Every metric space embeds into an ultrametric space with distortion $O(1/\sqrt \varepsilon)$ \citep{abraham2007embedding}.
\cite{charikar1998approximating} creates tree metrics from finite metric spaces;
uses this result to create approximation algorithms for several NP-hard problems.

\cite{bartal2003metric} discusses embeddings into Hilbert spaces.
\cite{krauthgamer2004measured} shows that any finite metric with $n$ data points can be embedded into Hilbert space with $O(\sqrt{\log\cdoub\log n})$ distortion.
\cite{neiman2016low} embeds finite metrics into $L_\infty$.
\cite{abraham2014volume} shows finite embeddings that preserve both distance and volume.
\cite{bartal2015impossibility} shows that $L_p, p>2$ spaces cannot be embedded.

MLPACK \cite{curtin2013mlpack}.
The most recent work is the Fast Library for Approximate Nearest Neighbor (FLANN) \cite{muja2014scalable}.
FLANN uses the randomized kd-forest and the priority search $k$-means tree.

In Euclidean space, \cite{andoni2014beyond} proposes a new data structure that provably does better than LHS lower bounds.
Good references inside.

\cite{malkov2014approximate} proposes a data structure for approximate nearest neighbor search using the Daulanay graph (dual graph of the Veronoi diagram).
They provide no approximation guarantees.
Seems to have a lot of references on similar approaches though.

\cite{abraham2015approximate} show that approximate nearest neighbor search can be done efficiently when the underlying metric can be represented as a planar graph.
This is interesting because these graphs need not have bounded doubling dimension or be embeddable in L2 space.
I should read this paper closer for the exact results.

\cite{borodin1999lower,barkol2000tighter,panigrahy2008geometric} provides lower bounds and a review of time-space tradeoffs.
\cite{andoni2008hardness} provides hardness results for $L_\infty$.
\cite{andoni2016lower} provides hardness results in a much more general setting for $L_p$ metrics.
It also has a LOT of good references for LSH results positive and negative.
\cite{p2011unifying} provides a unifying framework for determining lower bounds.

\cite{gionis1999similarity} introduces the idea of LHS.
LHS in metric spaces \cite{tellez2010locality,novak2010locality} provide no theoretical guarantees and non-metric spaces \cite{mu2010non}.

Survey of LHS \cite{wang2014hashing} \cite{wang2016learning}

\cite{krauthgamer2005black} provides hardness lower bounds on the performance of any approximate nearest neighbor algorithm. 
In particular, they show that a metric space admits a $(1+\epsilon)-$ANN solution if and only if $\doubdim = O(\log \log n)$.

Data structures for nearest neighbor queries in metric spaces can be divided into two categories:
those that work only heuristically,
and those that provide theoretical guarantees.
\cite{zezula2006similarity} is a book on the heuristic techniques. 
A lot of heuristic work continues to be done on good methods to partition space trees \cite{mao2016pivot},
but unfortunately there are not detailed experimental results justifying the use of these methods over methods with more theoretical justification.

The navigating net was the first data structure to use the doubling dimension.
It requires quadratic space, polynomial dependency on the doubling constant, and logarithmic dependence on the aspect ratio \citep{krauthgamer2004navigating}.
\cite{krauthgamer2005black} improve on this result by replacing the logarithmic dependence on the aspect ratio with a logarithmic dependence on the number of data points.
\citet{beygelzimer2006cover} introduce the cover tree,
which improves on the navigating net by reducing the space complexity to linear.
The downside of the cover tree is that they analyze their data structure in terms of the less robust expansion constant.
\cite{har2006fast} provides a data structure for approximate nearest neighbors.
\cite{cole2006searching} provides a data structure for approximate nearest neighbor search with dynamic point sets that does not depend on the aspect ratio.
Ad-hoc spanner data structures are popular in the analysis of algorithms.
\cite{chan2005hierarchical}, \cite{gottlieb2008optimal}, \cite{gottlieb2014light} and \cite{gottlieb2015light} create data structures called spanners,
which are embeddings of finite dimensional metric spaces into graph metrics with a low number of edges.
\cite{ram2009linear} and \cite{curtin2015plug} improve the runtime bounds of cover trees.
\cite{curtin2013tree} provides a generic framework for solving distance problems.
\cite{clarkson2006nearest} reviews metric space properties and data structures.
\cite{czumaj2010sublinear} is a survey of sublinear time algorithms in metric spaces.
\cite{moore2014fast} use cover trees for faster gaussian process posteriors.

\cite{indyk2007nearest} uses the doubling dimension to create embeddings into low dimensional Euclidean space.
\cite{connor2010fast} uses the expansion dimension to bound the runtime of neighbor graphs in Euclidean space.

The method of compression was introduced by \citet{hart1968condensed}.
This problem is known to be NP-hard \citep{zukhba2010np}.
Hart gave a heuristic that ran in time $O(n^3)$,
and\cite{angiulli2005fast} improved this to $O(n^2)$.
\cite{gottlieb2014near} gave the first sample compression scheme with a bound,
and they show a lower bound that nearly matches their result.

Nearest neighbor asymptotics first given by \citet{cover1967nearest}.

\cite{hildrum2004note} provides a randomized data structure in growth restricted dimensions.
\cite{talwar2004bypassing} creates the randomized split trees data structure,
but uses it to solve NP-hard problems rather than nearest neighbor queries.

\cite{arya2009space} characterizes the space time trade offs for approximate nearest neighbor in Euclidean space.

\cite{cheeseman1991really} phase transitions for NP-hard problems.

\cite{chavez2001searching} survey on searching in metric spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Review of Metric Spaces in Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples of Metric Spaces}

\begin{example}
    Any subset of a metric space is also a metric space.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The dimension of metric spaces}

This section introduces three measures of the ``dimension'' of a metric space:
the expansion constant $\cexp$, the doubling constant $\cdoub$, and the aspect ratio $\aspect{}$.
Previous analysis of the cover tree relied only on the expansion constant,
but the analysis in this thesis will instead use the doubling constant and aspect ratio.
In this section, we will formally define these constants and present their basic properties.
In particular, we will see that the doubling constant and aspect ratio are more robust measures of size than the expansion constant.

Throughout this section we will work with the metric space $(\set X,d)$.
The set $\set X$ may be either finite or infinite.
We let $B(x,\delta)$ denote the \emph{ball} centered around $x$ of radius $\delta$. 
That is,
\begin{equation}
    B(x,\delta) = \{ x' : x'\in\set X, \dist{x}{x'} \le \delta \}.
\end{equation}
Let $\mu : \{\set X\} \to \R^+$ be a function that gives an abstract notion of the \emph{volume} of its first argument.%
\footnote{
    Formally, $\mu$ is a measure and $\{\set X\}$ is a $\sigma$-algebra on $\set X$.
    Since we are primarily interested in the properties of finite sets,
    we will not need to formally use measure theory.
}
For example, if $\set X=\R^n$, then $\mu$ could be the standard Euclidean volume,
and $\mu B(x,\delta) = O(\delta^n)$.
If $\set X$ is a finite set, then $\mu$ could be the counting measure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The expansion dimension}

The expansion dimension is the most popular notion of dimensionality for bounding the performance of nearest neighbor data structures in metric spaces.
It was introduced by
\citet{karger2002finding},
and has been subsequently used in
\citet{krauthgamer2004navigating},
\citet{beygelzimer2006cover},
\citet{ram2009linear},
and \citet{curtin2015plug}.
The \emph{expansion dimension} of a metric space $\set X$ is defined as
\begin{equation}
    \krdim = \log_2 \cexp
    ,
\end{equation}
where $\cexp$ is called the \emph{expansion constant} and defined as
\begin{equation}
    \cexp = \max_{x\in\set X, \delta\in\R^+} \frac{\mu B(x,2\delta)}{\mu B(x,\delta)}
    .
\end{equation}
In words, the expansion dimension measures how the volume of a ball changes as you increase the radius.
The intuition behind why this is a good notion of a metric's dimension is that it agrees with the standard dimension on Euclidean space.
This is shown formally in the following lemma.
\begin{lemma}
    Let $\set X=\R^d$ with the standard $L_2$ distance function.
    Then $\krdim\set X=\Theta(d)$.
\end{lemma}
Unfortunately, the expansion dimension is known to have many defects which make runtime bounds less useful.
In particular:
The algorithmic consequence of these facts is that runtime bounds based on the expansion dimension are often trivial.

\begin{example}
    A subset of a metric space may have arbitrarily large expansion dimension compared to the host space.
\end{example}

\begin{example}
    Adding a single point to a space may change the expansion dimension by an arbitrary amount.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The doubling dimension}

The term doubling dimension was first used by \citet{gupta2003bounded} in their study of low dimensional embeddings for nearest neighbor queries,
but the main idea was introduced by \citet{assoud1979etude} to study fractals.%
\footnote{
    There are many terms in the literature with essentially the same meaning as the doubling dimension.
    For example, the terms Assoud dimension and Minkowski dimensions are both used in fractal geometry;
    and the terms covering number, packing number, $\varepsilon$-nets, and metric entropy are all used in computer science. 
}
The doubling dimension is more robust than the expansion dimension,
so runtime bounds in terms of the doubling dimension are more useful. 
Unfortunately, they are harder to prove.
Theorem \ref{} at the end of this section shows the novel result that there is no efficient data structure for exact nearest neighbor queries even in metrics with low doubling dimension.

In the context of machine learning, the doubling dimension has found use both statistically and algorithmically.
Statistically, the doubling dimension is a standard tool for proving regret bounds.
For example, \citet{luxburg2004distance} uses the doubling dimension to provide the first large margin bounds for classifiers in metric spaces,
and Chapter 27 of \cite{shalev2014understanding} shows how to bound the Rademacher complexity of a classifier using the doubling dimension.
A particularly relevant work in this line is \citet{kontorovich2015bayes},
which uses the doubling dimension to derive the first Bayes-consistent 1-nearest neighbor algorithm.
Algorithmically, the doubling dimension is used to develop approximation algorithms.
Chapter 32 of \cite{toth2017handbook} provides a survey of how the doubling dimension is used to approximate a variety of NP-hard problems.
In the context of machine learning, \citet{gottlieb2014near} shows the first approximation algorithm for sample compression,
and
\citet{gottlieb2014efficient} and \citet{gottlieb2017efficient} both create efficiently realizable approximation algorithms for classification in metric spaces.
Little recent work on nearest neighbor data structures has used doubling dimension.
The only examples known to the author are \citet{clarkson1997nearest}\footnote{Clarkson refers to the doubling dimension by an essentially equivalent concept he calls the $\gamma$-dominator bound.} and \citet{krauthgamer2004navigating}.
Theorem \ref{} at the end of this section may explain why.


The doubling dimension is closely related to the ideas of covering and packing numbers,
so we introduce these ideas first.
A \emph{$\delta$-covering} of a metric space $\set X$ is a set $\{x_1,x_2,...,x_n\} \subseteq \set X$ such that for all $x\in\set X$, there exists an $x_i$ such that $\dist{x}{x_i} < \delta$.
The \emph{$\delta$-covering number} $N_\delta(\set X)$ is the cardinality of the smallest $\delta$-covering.
%The log of the covering number $\log N_\delta(\set X)$ is called the \emph{metric entropy} of $\set X$.
A \emph{$\delta$-packing} of a metric space $\set X$ is a set $\{x_1,x_2,...,x_M\} \subseteq \set X$ such that $\dist{x_i}{x_j} > \delta$ for all distinct $i,j\in[M]$.
The \emph{$\delta$-packing number} $M_\delta (\set X)$ is the cardinality of the largest $\delta$-packing.
The covering number and packing number of a set are closely related.
We will make heavy use of the following lemma in the analysis of the cover tree.

\begin{lemma}
    \label{lemma:coverpacking}
    For any metric space $\set X$ and any $\delta>0$,
    %$M_{2\delta}(\set X) \le N_\delta(\set X) \le M_{\delta}(\set X)$.
    \begin{equation}
        M_{2\delta}(\set X) \le N_\delta(\set X) \le M_{\delta}(\set X)
        .
    \end{equation}
\end{lemma}
\begin{proof}
    To prove the first inequality, let $P$ be a $2\delta$-packing and $C$ be a $\delta$-cover of $\set X$.
    For every point $p\in P$, there must exist a $c\in C$ such that $\dist{p}{c}\le\delta$.
    No other $p'\in P$ can also satisfy $\dist{p'}{c}\le\delta$, because then by the triangle inequality
    \begin{equation}
        \dist{p'}{p} \le \dist{p'}{c}+\dist{p}{c} \le 2\delta
        ,
    \end{equation}
    which would contradict that $P$ is a $2\delta$-packing.
    In other words, for each $c\in C$, there is at most one $p\in P$.
    So $N_\delta \ge |C| \ge |P| \ge M_{2\delta}$.

    To prove the second inequality, let $\set X'\subseteq \set X$ be a maximal $\delta$-packing.
    Then there does not exist an $x\in\set X$ such that for all $x'\in\set X'$, 
    $\dist{x}{x'} > \delta$.
    (Otherwise, $\set X' \cup \{x\}$ would be a packing larger than $\set X'$.)
    Hence, $\set X'$ is also a $\delta$-cover,
    and the smallest $\delta$-cover can be no larger.
\end{proof}

%\cite{nickl2007bracketing} uses metric entropy to prove a version of the central limit theorem.
%
%\begin{example}
%\end{example}

%\begin{definition}
    %The Minkowski dimension of a metric space is defined to be
    %\begin{equation}
        %\minkdim \set X = \lim_{\delta\to0} \frac{\log N_\delta(\set X)}{\log 1/\delta}
        %.
    %\end{equation}
%\end{definition}
%
%\begin{example}
    %Let $\set X$ be a finite metric space.
    %Then $\minkdim \set X = 0$.
%\end{example}

The \emph{doubling dimension} of a metric space $\set X$ is defined to be
\begin{equation}
    \doubdim = \log_2 \cdoub
    ,
\end{equation}
where $\cdoub$ is called the \emph{doubling constant} and is defined to be
\begin{equation}
    \cdoub = \max_{x\in\set X, \radius\in\R^+} N_\radius(B_{\set X}(x,2\radius))
    .
\end{equation}
Whereas the expansion dimension measures how the volume of a ball changes as the radius changes,
the doubling dimension measures how the packing number of a ball changes.
This minor change fixes many of the problems with the doubling dimension.
%As with the expansion dimension, the doubling dimension is good notion of a metric's dimension because it agrees with the standard dimension on Euclidean space:
The following lemma shows that the doubling dimension (like the expansion dimension) agrees with the dimension in Euclidean space.
\begin{lemma}
    \label{lemma:doubleEuclidean}
    Let $\set X=\R^d$ with the standard $L_2$ distance function.
    Then $\doubdim\set X=\Theta(d)$.
\end{lemma}
\noindent
The proof of Lemma \ref{lemma:doubleEuclidean} follows by an explicit calculation of the volume of a ball in $\mathbb{R}^d$.
The proof is not especially interesting, and so is omitted.
The major advantage of the doubling dimension over the expansion dimension is its robustness,
as shown in the following two lemmas.

%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
    Let $\set X$ be a metric space, and $X\subset\set X$.
    Then,
    \begin{equation}
        \label{eq:cdoubinclusion}
        %\doubdim\set X \ge \doubdim X,
        \cdoub X \le \cdoub \set X.
    \end{equation}
\end{lemma}

\begin{proof}
    %To prove \eqref{eq:cdoubinclusion},
    %let $x$ be a point in $\set X$ and $\delta\in\mathbb R^+$.
    %Then any valid $\delta$-packing of $B_{\set X_1}(x,2\delta)$ is also a valid $\delta$-packing of $B_{\set X_2}(x,2\delta)$.
%
\end{proof}

\begin{lemma}
    Let $\set X$ be a metric space, $X\subset\set X$, and $x\in\set X$.
    Then,
    \begin{equation}
        \label{eq:cdoubplusone}
        \cdoub (X\cup\{x\}) \le \cdoub X + 1.
    \end{equation}
\end{lemma}
\begin{proof}
    Let $y\in X$ and $\delta>0$.
    Let $C$ be a minimal $\delta$-covering of $B_X(y,\delta)$.
    Then the set $C\cup\{x\}$ must be a $\delta$-covering for $B_{X\cup\{x\}}(y,\delta)$.
    %So we have that $N_\delta(B_X(y,\delta)) \le N_\delta(B_{X\cup\{x\}}(y,\delta))+1$ for any $y\in X$ and $\delta>0$.
    So we have that 
    \begin{equation}
        N_\delta(B_X(y,\delta)) \le N_\delta(B_{X\cup\{x\}}(y,\delta))+1
        .
    \end{equation}
    Taking the maximum of both sides with respect to $y$ and $\delta$ gives \eqref{eq:cdoubplusone}.
    %for any $y\in X$ and $\delta>0$.
    %let $p$ be a maximal $\delta$-packing of $\set X$.
    %Assume for contradiction that there exists a $\delta$-packing $p'$ of $\set X\cup\{x\}$ such that $|p'| > |p| + 1$.
    %If $x\not\in p'$, then $p'$ is a $\delta$-packing of $\set X$.
    %But $|p'| > |p|$, which violates the assumption that $p$ is maximal.
    %If $x\in p$, then the set $p'-\{x\}$ is a packing of $\set X$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\noindent
\citet{gupta2003bounded} and \cite{krauthgamer2004navigating} provide the following lemma relating the size of the expansion and doubling dimensions.

\begin{lemma}
    Every finite metric $(\set X,d)$ satisfies
    $\cdoub \le \cexp^4$.
    %\begin{equation}
        %\doubdim\set X \le 4\cdot\krdim\set X
        %.
    %\end{equation}
\end{lemma}
\begin{proof}
    Fix some ball $B(x,\delta)$.
    We will show that $B(x,\delta)$ can be covered by $\cexp^4$ balls of radius $\delta$.
    Let $Y$ be a $\delta$-cover of $B(x,2\delta)$.
    Then,
    \begin{equation}
        B(x,2\delta) 
        \subseteq 
        \bigcup\limits_{y\in Y} B(y,\delta) 
        \subseteq
        B(x,4\delta)
        %~~~~~
        %\text{and}
        %~~~~~
    \end{equation}
    Also, for every $y\in Y$,
    \begin{equation}
        |B(x,4\delta)| 
        \le 
        |B(y,8\delta)| 
        \le 
        \cexp^4 |B(y,\delta/2)|
        .
    \end{equation}
    We also have that $|B(y,\delta/2)|=1$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The hole dimension}

The hole dimension is a novel measure of a set's dimensionality.

The \emph{hole number} is defined as
\begin{align}
    \chole
    = 
    &\max_{x\in\set X, \epsilon \ge 0, \delta > 0} 
    N_\delta\big( B(x,\epsilon+2\delta) \setminus B(x,\epsilon+\delta) \big)
    \\
    &
    ~~~~~~\text{s.t.}
    ~~
    B(x,\epsilon+\delta) \setminus B(x,\delta) = \{\}
    .
\end{align}
The \emph{hole dimension} is
\begin{align}
    \holedim = \log_2 \chole
    .
\end{align}

\begin{lemma}
    For any finite metric space,
    \begin{equation}
        \cdoub \le \chole + 1 \le 
    \end{equation}
\end{lemma}

\fixme{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The aspect ratio}

We will use the aspect ratio to bound the depth of the cover tree.
Unlike the covering and expansion constants,
the aspect ratio should not be thought of as a measure of dimension.
Instead, it should be thought of as a measure of how evenly spread the points in a metric space are.
The \emph{diameter} of $\set X$ is the maximum distance between any two points.
In notation,
\begin{equation}
    \diam {\set X} = \max_{x_1,x_2\in\set X} \dist{x_1}{x_2}
    .
\end{equation}
The \emph{codiameter} of $\set X$ is the minimum distance between any two points.
In notation,
\begin{equation}
    \codiam {\set X} = \min_{x_1 \ne x_2\in\set X} \dist{x_1}{x_2}
    .
\end{equation}
The \emph{aspect ratio} of $\set X$, denoted by $\aspect{\set X}$, 
is the ratio of the diameter to the dispersion.
Sets with small aspect ratio are called ``fat,''
and sets with large aspect ratio are called ``skinny.''

There is little inherent relationship between the aspect ratio and the inherent dimension of a space.
We emphasize this point with three examples.

\begin{example}
    Let $\set Y=\{y_1,...,y_n\}$ be the discrete metric space of size $n$;
    that is,
    \begin{equation}
        \dist{y_i}{y_j}=
        \begin{cases}
            0 & i = j \\
            1 & \text{otherwise}
        \end{cases}
        .
    \end{equation}
    Then the aspect ratio of $\set Y$ is 1 (i.e.\ as small as possible),
    and both the expansion and doubling constants of $\set Y$ are $n-1$ (i.e. arbitrarily large).
\end{example}

\begin{example}
    Now construct the set $\set Y'=\{y'_1, y'_2, y'_3\}$.
    Let $r>2$, and define the distance function to be
    \begin{equation}
        d(y'_i,y'_j) =
        \begin{cases}
            0 & i=j \\
            1 & i=1, j=2 \\
            r & i=1, j=3 \\
            r & i=2, j=3 \\
        \end{cases}
        .
    \end{equation}
    Then the aspect ratio is $r$ (i.e.\ arbitrarily large),
    but the expansion constant is always 2
    and the doubling constant always 1.
\end{example}

\begin{example}
    Let $Y=\{\frac 1 2, \frac 1 4, ..., \frac 1 {2^n}\}$,
    and $\dist{y_1}{y_2}=|y_1-y_2|$.
    Then the aspect ratio is $2^{n-1}$ (i.e.\ arbitrarily large),
    the expansion constant is $n-1$ (i.e.\ arbitrarily large),
    and the doubling constant is 1.
\end{example}

\begin{lemma}[\cite{krauthgamer2004navigating}]
    For any metric space $\set X$, we have that
    $
        |\set X| \le \aspect{\set X}^{O(\doubdim{\set X})}.
    $
\end{lemma}
%\begin{proof}
    %Assume wlog that the separation distance is 1
    %(we can rescale all the distances to ensure this).
    %Then the diameter of $\set X$ is $\aspect{\set X}$.
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The growth rate of the aspect ratio}

The aspect ratio and the expansion number share the unattractive property that adding a single point to a dataset can increase these quantities arbitrarily.
Under mild assumptions, however, we can show that this is unlikely to happen.
Specifically, let $\set X$ be a metric space, 
and let $X=\{x_1,...,x_n\}\subset\set X$ be a sample of $n$ i.i.d.\ points from a distribution $\distribution D$ over $\set X$.
Our goal is to show that the aspect ratio is polynomial in $n$.
We will later show that the log of the aspect ratio bounds the depth of the cover tree (see Theorem \ref{}),
and so the depth of the cover tree will be logarithmic in $n$.

We begin by bounding the diameter of $X$.
%In general, it does not make sense to take the expectation with respect to $\distribution D$.
We say the distribution $\distribution D$ has \emph{finite expected distance} if there exists an $\bar x\in\set X$ such that $\mu=\E\dist{\bar x}{x_i}$ is finite.
Note that this is a mild condition satisfied by most standard distributions on Euclidean space.
For example, the uniform, Gaussian, exponential, Weibull, and Pareto distributions all have finite expected distance.
Notice that the Weibull and Pareto distributions have heavy tails.
One easy to describe distribution which does not satisfy this property is the Cauchy distribution 
(the distribution of the reciprocal of a Gaussian random variable).
The following lemma shows that this is a sufficient condition for the diameter to grow polynomial in $n$.

\begin{lemma}
    \label{lemma:Ediam}
    %Let $\set X$ be a metric space.
    %Let $X=\{x_1,...,x_n\}\subset\set X$ be a sample of $n$ i.i.d.\ points satisfying the following property:
    %There exists a $\bar x\in\set X$ such that $\mu=\E\dist{\bar x}{x_i}$ is finite.
    %Then, 
    %\begin{equation}
        %\E\diam{X} \le 2n\mu
    %\end{equation}
    Let $\set X$ and $X$ be defined as above,
    and assume that $\distribution D$ has finite expected distance.
    Then, $\E\diam{X} \le 2n\mu$.
\end{lemma}

\begin{proof}
    By the triangle inequality, we have that
    \begin{equation*}
        \diam{X}
        = 
        \max_{i,j} \dist{x_i}{x_j}
        \le
        \max_{i,j} (\dist{\bar x}{x_i} + \dist{\bar x}{x_j})
        %\\ &=
        %\max_i \dist{\bar x}{x_i} + \max_j\dist{\bar x}{x_j}
        =
        2\max_i \dist{\bar x}{x_i}
        .
    \end{equation*}
    We now remove the max using the union bound.
    This gives
    \begin{equation*}
        \prob{\diam{X} > t}
        \le
        \prob{\max_i 2\dist{\bar x}{x_i} > t}
        \le
        \sum_{i=1}^n\prob{2\dist{\bar x}{x_i} > t}
        \label{eq:Ediamub}
        =
        n\prob{2\dist{\bar x}{x_1} > t}
        %\label{eq:Ediamiid}
        .
    \end{equation*}
    %Equation \eqref{eq:Ediamub} follows from the union bound, 
    %and \eqref{eq:Ediamiid} follows because the $x_i$s are i.i.d.
    The rightmost equality follows because the $x_i$s are i.i.d.
    Finally, since the distances are always nonnegative, we have that
    \begin{equation*}
        \E\diam{X} 
        = 
        \int_0^\infty \prob{\diam{X} > t} \dd t
        \le
        \int_0^\infty n\prob{2\dist{\bar x}{x_1} > t} \dd t
        =
        %n\int_0^\infty \prob{2\dist{\bar x}{x_1} > t} \dd t
        %\\ &=
        2n \E\dist{\bar x}{x_1}
        %\label{eq:Ediamproof}
        .
    \end{equation*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next we show that the codiameter cannot shrink too fast.
We say that the distribution $\distribution D$ has \emph{$B$-bounded density} if
for all $x\in\set X$, the density of $\dist{x}{x_i}$ is bounded by $B$.
An immediate consequence is that 
\begin{equation}
    \max_{x\in\set X} \prob{\dist{x}{x_i} \le t} \le Bt
    .
\end{equation}
Again, all the standard distributions in Euclidean space satisfy this condition.
The following lemma shows that this condition is sufficient to lower bound the codiameter.

\begin{lemma}
    \label{lemma:Ecodiam}
    Let $\set X$ and $X$ be defined as above,
    and assume that $\distribution D$ has $B$-bounded density.
    Then, $\E\codiam{X} \ge (2n^2B)^{-1}$.
\end{lemma}
\begin{proof}
    We have that
    \begin{align}
        \prob{\codiam{X} \le t}
        &=
        \prob{\min \{ \dist{x_i}{x_j} : i\in\{1,...,n\}, j\in\{i+1,...,n\} \} \le t}
        %\prob{\min_{i\ne j} \dist{x_i}{x_j} \le t}
        \\ &\le 
        \sum_{i=1}^n\sum_{j=i+1}^n \prob{\dist{x_i}{x_j} \le t}
        \label{eq:lemcodiam1}
        %\\ &\le
        %\sum_{i=1}^n n \max_{x\in X} \prob{\dist{x_i}{x} \le t}
        \\ & \le
        n^2 \max_{x\in X} \prob{\dist{x_1}{x} \le t}
        \label{eq:lemcodiam2}
        \\ & \le 
        n^2 B t
        \label{eq:lemcodiam3}
    \end{align}
    Equation \eqref{eq:lemcodiam1} follows from the union bound,
    \eqref{eq:lemcodiam2} from the fact that the $x_i$s are i.i.d.,
    and \eqref{eq:lemcodiam3} from the definition of $B$-bounded.
    We further have that since probabilities are always no greater than 1,
    \begin{equation}
        \prob{\codiam{X}\le t} \le \min\{1,n^2Bt\}
        .
    \end{equation}
    Finally, since $\codiam{X}$ is nonnegative, we have that 
    \begin{align}
        \E \codiam{X}
        &=
        \int_0^\infty (1-\prob{\codiam{X} \le t}) \dd t
        %\\ & =
        %\int_0^\infty \prob{\codiam{X} > t} dt
        \\ & \ge
        \int_0^\infty (1-\min\{1,n^2 Bt\}) \dd t
        \\ & = 
        \int_0^{(n^2B)^{-1}} (1 - n^2 Bt) \dd t
        \\ & =
        %\frac{1}{n^2B} - \frac{\left(\frac{1}{n^2B}\right)^3}{2}
        %\\ & \ge
        \frac{1}{2n^2B}
        \label{eq:Ecodiamproof}
        .
    \end{align}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An immediate consequence of Lemmas \ref{lemma:Ediam} and \ref{lemma:Ecodiam} is the following bound on the aspect ratio.

\begin{lemma}
    \label{lemma:Easpect}
    Let $\set X$ and $X$ be defined as above.
    Assume that $\distribution D$ has finite expected distance and $B$-bounded density.
    Then, $\E\aspect{X} \le2B\mu n^3$. 
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Simplified Cover Tree}

A \emph{simplified cover tree} is a data structure for efficiently representing a finite metric space.
Each node in the tree corresponds to exactly one point in the space,
and the tree obeys the following three invariants.
\begin{enumerate}
    \item \emph{Leveling invariant}.%
    \footnote{
        The original version of the simplified cover tree \citep{izbicki2015faster} had a slightly different leveling invariant.
        The original version required that the level of a child be exactly one less than the level of the parent.
        The version used in this thesis is strictly more general and facilitates the runtime analysis of the tree.
    }
    Every node $\p$ has an associated integer $\level\p$.
    For all nodes $\q\in\children\p$, $\level\q < \level\p$.
    Furthermore, $\level p$ cannot be decreased without violating the covering invariant.
    %(Note that $\level{p}$ can be negative or arbitrarily large.
    %In particular, it is not the depth of $p$ in the tree.)
    \item \emph{Covering invariant}.
    Every node $\p$ has an associated real number $\covdist\p=2^{\level\p}$.
    For all nodes $\q\in\children\p$, $\dist \p \q \le \covdist\p$.%
    \footnote{
        As in the original cover tree, practical performance is improved on most datasets by redefining $\covdist p = 1.3 ^ {\level p}$.
        All of our experiments use this modified definition.
    }
    \item \emph{Separating invariant}.
    For all nodes $\q_1,\q_2\in\children\p$, $\dist {\q_1} {\q_2} \ge \covdist\p/2$.
\end{enumerate}
It will be useful to define the function
\begin{equation}
\maxdist p = \argmax_{q\in\descendants{p}} \dist p q
.
\end{equation}
In words, $\maxdist\p$ is the greatest distance from $p$ to any of its descendants.
This value is upper bounded by $2^{\level{p}+1}$, 
and its exact value can be cached within the data structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Properties of the Simplified Cover Tree}

Before we present algorithms for manipulating the cover tree, 
we present two lemmas that bound the shape of the tree.
These lemmas are a direct consequence of the cover tree's invariants and motivate the invariants' selection.
This section can be safely skipped by the reader not interested in the details of the tree's runtime analysis.

%%%%%%%%%%%%%%%%%%%%

%\begin{lemma}
    %\label{lamma:diam}
    %%For every node $p$ in a cover tree, $\covdist{p}\le\diam{X}/2$.
    %For every node $p$ in a cover tree, $\maxdist p \ge \covdist p/2$.
%\end{lemma}
%
%\begin{proof}
    %We have that $\maxdist p$ must be greater than or equal to 
%\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
    \label{lemma:height}
    Let $p$ be any non-leaf node in a cover tree.
    Denote by $\height{p}$ the number of edges between $p$ and its most distant leaf.
    We have that $\height{p} \le \log_2\aspect{X}$.
\end{lemma}

\begin{proof}
    We will show that the following chain of inequalities holds:
    \begin{equation}
        \aspect{X} 
        = \frac{\diam{X}}{\codiam{X}} 
        \overset{(1)}\ge \frac{\diam{X}}{\covdist{p}/2^{\height p-1}} 
        \overset{(2)}\ge \frac{\diam{X}}{\diam{X}/2^{\height p}} 
        = 2^{\height p}
        .
    \end{equation}
    Solving for $\height p$ then gives the desired result.
    For inequality $(1)$, consider a point $q$ that is exactly $i$ edges away from $p$.
    By the covering invariant, 
    \begin{equation}
        \dist{q}{\parent{q}} 
        \le 
        \covdist{\parent{q}}
        \le 
        \covdist{\parent{\parent{q}}}/2
        \le
        \covdist{p}/2^{i-1}
        .
    \end{equation}
    In particular, if $\ell$ is a leaf node $\height p$ edges away from $p$,
    then $\codiam{X}\le\dist{p}{\ell}\le2^{\height p-1}$.
    For inequality (2), observe that there must exist a child $q$ of $p$ such $\dist{p}{q} \ge \covdist{p}/2$.
    Otherwise, $\level p$ could be reduced by one, which would violate the leveling invariant.
    Therefore, $\diam{X} \ge \dist{p}{q} \ge \covdist{p}/2$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
    \label{lemma:children}
    For every node $p$ in a cover tree, we have that
    $|\children\p| \le \cdoub^2$.
\end{lemma}

\begin{proof}
    To simplify notation, we let $\delta=\covdist{p}$.
    %The covering invariant ensures that all the children of $p$ are contained in $B(p,\delta)$,
    The separating invariant ensures that the children of $p$ form a $\delta/2$-packing of $B(p,\delta)$.
    So by the definition of $M_{\delta/2}$ and Lemma \ref{lemma:coverpacking}, we have
    \begin{equation}
        |\children{p}| 
        \le M_{\delta/2}(B(p,\delta)) 
        \le N_{\delta/4}(B(p,\delta)) 
        %\le \cdoub N_{\delta/2}(B(p,\delta)) 
        %\le \cdoub^2
        .
    \end{equation}
    We now show that $N_{\delta/4}(B(p,\delta))\le\cdoub$.
    Let $Y$ be a $\delta/2$-covering of $B(p,\delta)$.
    For each $y_i\in Y$, let $Y_i$ be a minimum $\delta/4$-covering of $B(y_i,\delta/2)$.
    The union of the $Y_i$s is a $\delta/4$-covering of $B(p,\delta)$.
    There are at most $\cdoub$ $Y_i$s, and each $Y_i$ contains at most $\cdoub$ elements.
    So their union contains at most $\cdoub^2$ elements.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Algorithms}

This section analyzes four common algorithms for cover trees.
The first two algorithms are called single tree algorithms because they work with only a single cover tree at once.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}
    Let $\set X$ be a metric space, $X\subset\set X$, and $x$ be in $\set X$ but not $X$.
    Then the nearest neighbor $y^*\in X$ is defined as $y^*=\argmin_{y\in X} \dist{x}{y}$.
    An $\varepsilon$-nearest neighbor (for $\varepsilon\ge1$) is a point $\hat y$ satisfying $\dist{x}{\hat y} \le \varepsilon \cdot\dist{x}{y^*}$.
\end{definition}
\newcommand{\eann}{(1+\varepsilon)\text{-ann}}

\subsubsection{Finding the nearest neighbor of a single point}

The $\findnn$ function (Algorithm \ref{alg:findnn}) finds an $\eann$ of a data point $x$ in a set of cover trees $P$.
The function would typically be called with only a single tree in the input set $P$;
but in subsequent recursive calls, $P$ may contain many subtrees. 
On each call,
$\findnn$ iterates over $Q$ (the children of nodes in $P$),
and constructs a set $Q_x \subseteq Q$ of subtrees that will be recursively searched.
%If $Q_x$ is empty, then $\hat p$ is guaranteed to be an $\eann$.
%The important property of $Q_x$ is that it always contains an $\eann$ of $x$
%(see next paragraph).
%If $Q_x$ contains only one element, it must be an $\eann$ and the algorithm returns it.
%Otherwise recursively calling $\findnn(Q_x,x)$ will return an $\eann$.
We typically have that $|Q_x| <\!\!< |Q|$,
so only a small portion of the tree is searched.
We now prove the algorithm's correctness and runtime.

\begin{algorithm}[t]
\caption{$\findnnorig$(set of cover trees $P$, query  point $x$, tolerance $\varepsilon$)}
\label{alg:findnnorig}
\vspace{0.1in}
returns a $(1+\varepsilon)$-ann of $x$ in $\data{P}$
\begin{algorithmic}[1]
    %\State let $Q = \{ q : q\in\children{p}, p\in P \}$, $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \State let $Q = \{ q : q\in\children{p}, p\in P \}$
    \State let $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \If {$Q=\{\hat q\}$ \textbf{or} $\dist{x}{\hat q} \ge 2\cdot\covdist{\hat q}(1+1/\varepsilon)$}
        \label{line:findnnorig:if}
        \State \Return $\hat q$
        \label{line:findnnorg:returnq}
    \Else
        \State let $Q_x = \{ q : q\in Q, \dist{x}{q} \le \dist{x}{\hat q}+\maxdist{q}\}$
        \label{line:findnnorig:Q_x}
        \State\Return $\findnn(Q_x,x,\varepsilon)$
        \label{line:findnnorig:recurse}
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{theorem:findnnorig:correct}
    $\findnnorig(\{p\}, x, \varepsilon)$ returns a $\eann$ of $x$ in $\data{p}$.
    In particular, $\findnnorig(\{p\},x,0)$ returns the exact nearest neighbor of $x$.
\end{theorem}
\begin{proof}
    We show that every $\hat q$ satisfying the if statement in line $\ref{line:findnnorig:if}$ is a $\eann$.
    For the first case where $Q=\{\hat q\}$, 
    we show that $\hat q$ must be the true nearest neighbor.
    Let $y^*$ denote the true nearest neighbor of $x$ in $\data{p}$.
    Assume for induction that $y^*$ is in $\data{P}$ and hence in $\data{Q}$.
    Let $q^*$ denote the node in $Q$ that contains $y^*$.
    Then we have that
    \begin{equation}
        \label{eq:findnnorig:eq1}
        \dist{x}{q^*}
        \le \dist{x}{y^*} + \dist{y^*}{q^*}
        \le \dist{x}{\hat q} + \maxdist{q^*}
        .
    \end{equation}
    The set $Q_x$ is defined to be all elements of $q$ satisfying \eqref{eq:findnnorig:eq1}.
    So $q^*$ is always included in $Q_x$,
    which becomes $P$ in subsequent recursive calls.

    Now consider the case where $\dist{x}{\hat q} \ge 2\cdot\covdist{\hat q}(1+1/\varepsilon)$.
    We have,
    \begin{align}
        \dist{x}{\hat q}
        &\le \dist{x}{q^*}
        \\&\le \dist{x}{y^*} + \dist{y^*}{q^*}
        \label{eq:findnnorig:1}
        \\
        &\le \dist{x}{y^*} + 2\cdot\covdist{q^*}
        \label{eq:findnnorig:2}
        \\
        &\le \dist{x}{y^*} + \frac{\dist{x}{\hat q}}{1+1/\varepsilon}
        \label{eq:findnnorig:3}
        \\
        &= (1+\varepsilon)\dist{x}{y^*}
        \label{eq:findnnorig:4}
    \end{align}
    Line \eqref{eq:findnnorig:1} is the triangle inequality,
    line \eqref{eq:findnnorig:2} uses the maximum distance between any node and a descendent,
    line \eqref{eq:findnnorig:3} uses the condition in the if statement,
    and line \eqref{eq:findnnorig:4} follows from algebraic manipulations.
\end{proof}

\begin{theorem}
    \label{theorem:findnn:runtime:approx}
    %Let $\varepsilon\ge2$.
    %Then $\findnn(\{p\},x)$ has run time $O(\cdoub^4\log\aspect{})$.
    %If the data is generated i.i.d.\ from a well behaved distribution,
    %then the run time is $O(\cdoub^4\log n)$.
    %If $\varepsilon > 0$,
    %then $\findnnorig(\{p\},x,\varepsilon)$ finds a $\eann$ in time $O(\cdoub^4\log\aspect{} + (1/\varepsilon)^{\log \cdoub})$.
    For any $\varepsilon$, $\findnnorig(\{p\},x,\varepsilon)$ finds a $\eann$ in time $O(\chole^4\log\aspect{})$.
    In particular, $\findnnorig$ finds exact nearest neighbors in this time.
\end{theorem}

\begin{proof}
    The total runtime is bounded by the product of the number of recursive calls and the time spent in each call.
    The number of recursions is bounded by the height of the tree because $\findnn$ descends one level of the tree with each recursion.
    This height is bounded by $O(\log\aspect{})$ in Lemma \ref{lemma:height}.
    The cost of each call is $O(|Q|)$,
    since we perform one distance computation for each node in $Q$.
    The size of $Q$ is the size of $P$ times the number of children per node.
    The maximum number of children per node is $O(\cdoub^2) = O(\chole^2)$ by Lemma \ref{lemma:children}.
    The size of $P$ is the same as the size of $Q_x$ in the previous recursion.
    So we have that the overall runtime is $O(|Q_x|\cdoub^2\log\aspect{})$,
    and all that remains is to bound $|Q_x|$.

    Let $\gamma=\dist{x}{\hat q}$ and $\delta=\covdist{\hat q}$.
    By definition of $Q_x$ on line \ref{line:findnnorig:Q_x}, all $q\in Q_x$ satisfy
    \begin{equation}
        \dist{x}{q} \le \dist{x}{\hat q} + \maxdist{q} \le \gamma+2\delta
        .
    \end{equation}
    So $Q_x$ is a subset of $A(x,\gamma,2\delta)$,
    and by the global separating invariant $Q_x$ is a $\delta$-packing of this annulus.
    By Lemma \ref{lemma:coverpacking},
    \begin{equation}
        |Q_x| 
        \le M_\delta A(x,\gamma,2\delta)     
        \le N_{\delta/2} A(x,\gamma,2\delta)
        \le \chole^2
        .
    \end{equation}
    The overall runtime is then bounded by $O(\chole^4\log\aspect{})$.
\end{proof}

\begin{theorem}
    If $\varepsilon > 0$,
    then $\findnnorig(\{p\},x,\varepsilon)$ finds a $\eann$ in time $O(\cdoub^5\log\aspect{} + (1/\varepsilon)^{\log \cdoub})$.
\end{theorem}

\begin{proof}
    As in the proof of Theorem \ref{theorem:findnn:runtime:approx}, 
    the runtime is bounded by $O(|Q_x|\cdoub^2\log\aspect{})$,
    and we need to bound $|Q_x|$.
    With respect to the doubling dimension,
    $Q_x$ exhibits a phase transition that complicates analysis.
    In early recursions, $Q_x$ is ball shaped,
    and we shall see that its size can be bounded using the doubling constant as $O(\cdoub^3)$.
    So the overall runtime through the ball shaped phase is $O(\cdoub^5\log\aspect{})$.
    In later recursions, $Q_x$ is annulus shaped and the doubling constant cannot be applied directly to bound the runtime of an iteration.
    Instead, we will show that the total time spent in this annulus phase is $(1/\varepsilon)^{O(\log \cdoub)}$. 
    The theorem statement then follows because the overall runtime is the runtime of the ball phase plus the runtime of the annulus phase.

    Let $\delta = \covdist{\hat q}$.
    %and $y^*$ denote the true nearest neighbor of $x$.
    %By the triangle inequality, we have
    %\begin{equation}
        %\dist{x}{\hat q} 
        %\le \dist{x}{q^*} 
        %\le \dist{x}{y^*} + \dist{y^*}{q^*}
        %\le \dist{x}{y^*} + 2\delta
        %,
    %\end{equation}
    %and so 
    %\begin{equation}
        %\label{eq:findnnorig:runtime:eq1}
        %\dist{x}{y^*} \ge \dist{x}{\hat q} - 2\delta
        %.
    %\end{equation}
    The ball phase occurs when $\dist{x}{\hat q} \le 6\delta$.
    %By \eqref{eq:findnnorig:runtime:eq1} the nearest neighbor $y^*$ of $x$ may be arbitrarily close to $x$,
    %and hence $y^*$ may be located anywhere in a ball determined by $Q_x$.
    By definition of $Q_x$ on line \ref{line:findnnorig:Q_x}, all $q\in Q_x$ satisfy
    \begin{equation}
        \dist{x}{q} \le \dist{x}{\hat q} + \maxdist{q} \le 8\delta
        .
    \end{equation}
    By the global separating invariant,
    we also have that $Q$ is a $\delta$-packing of $B(x,8\delta)$.
    So,
    \begin{equation}
        |Q_x| 
        \le M_\delta B(x,8\delta) 
        \le N_{\delta/2}B(x,8\delta) 
        \le \cdoub^4
        .
    \end{equation}
    Thus the ball phase takes time $O(\cdoub^6\log\aspect{})$.
    %We used Lemma \ref{lemma:coverpacking} to connect the packing and covering numbers.

    In the annulus begins on the first recursion where
    $\dist{x}{\hat q} > 6\delta$.
    %Substituting into \eqref{eq:findnnorig:runtime:eq1} gives $\dist{x}{y^*} > 0$.
    %That is, there is a ball around $x$ that cannot contain $y^*$,
    %so $y^*$ must be contained in an annulus determined by $Q_x$.
    %Let $\gamma=\dist{x}{\hat q}$.
    %Then $Q_x \subseteq B(x,\gamma+2\delta)\setminus B(x,\gamma)$,
    %and $Q_x$ is a $\delta$-packing of this annulus.
    %Using the hole constant, we get
    %\begin{equation}
        %|Q_x| 
        %\le M_\delta A(x,\gamma,2\delta)     
        %\le N_{\delta/2} A(x,\gamma,2\delta)
        %\le \chole^2
        %.
    %\end{equation}
    %In each subsequent recursive call, $\delta$ shrinks by half, but the 
    \fixme{The number of iterations at this point is bounded by $m=\log_2 1/\varepsilon$; the cost of each iteration is $O(\cdoub^{4+m})$}
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[t]
\caption{$\findnn$(set of cover trees $P$, query  point $x$, tolerance $\varepsilon$)}
\label{alg:findnn}
\vspace{0.1in}
returns a $(1+\varepsilon)$-ann of $x$ in $\data{P}$
faster than $\findnnorig$
\begin{algorithmic}[1]
    \State let $Q = \{ q : q\in\children{p}, p\in P \}$
    \State let $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    %\State let $Q_x = \{ q : q\in Q, \dist{x}{q} \le \dist{x}{\hat q}/\varepsilon+\maxdist{q}\}$
    %\If {$Q_x = \{\}$}
    \If {$Q=\{\hat q\}$ \textbf{or} $\dist{x}{\hat q} \ge 2\cdot\covdist{\hat q}(1+1/\varepsilon)$}
        \State \Return $\hat q$
        \label{line:findnn:returnq}
    \Else
    \State let $Q_x = \{ q : q\in Q, \dist{x}{q} \le \dist{x}{\hat q}\hl{$/(1+\varepsilon)$}+\maxdist{q}\} \hl{$\cup \{\hat q\}$}$
        %\State\Return $\findnn(Q_x\cup\{\hat q\},x,\varepsilon)$
        \State\Return $\findnn(Q_x,x,\varepsilon)$
        \label{line:findnn:recurse}
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    \label{theorem:findnn:correct}
    $\findnn(\{p\},x,\varepsilon)$ returns an $\eann$ of $x$ in $p$.
\end{theorem}
\begin{proof}
    Let $y^*$ denote the true nearest neighbor of $x$ in $\data{p}$.
    We will show that on each recursion,
    either $y^* \in Q_x$ or $\hat q$ is an $\eann$.
    Since the algorithm only terminates when $Q_x$ is empty,
    %(and Theorem \ref{theorem:findnn:runtime} below shows that $\findnn$ must terminate),
    the returned $\hat q$ must be an $\eann$.

    In the first recursive call where $y^* \not\in\data{Q_x}$,
    we must have that $y^* \in \data{Q}$.
    Let $q^*$ be the node in $Q$ such that $y^* \in\data{\q^*}$.
    Then by the definition of $Q_x$, we have that
    \begin{equation}
        \label{eq:findnn:correct:1}
        \dist{x}{q^*}
        > \dist{x}{\hat q}/\varepsilon + \maxdist{q^*} 
        .
    \end{equation}
    Using the triangle inequality on the left and the definition of $\mkfunction{maxdist}$ on the right gives
    \begin{equation}
        \dist{x}{y^*} + \dist{y^*}{q^*} 
        \ge \stackrel{\eqref{eq:findnn:correct:1}}{~~~~...~~~~}
        %\ge \dist{x}{q^*} 
        %>   \dist{x}{\hat q}/\varepsilon + \maxdist{q^*}
        \ge \dist{x}{\hat q}/\varepsilon + \dist{y^*}{q^*}
        .
    \end{equation}
    Subtracting $\dist{y^*}{q^*}$ from each side gives
    \begin{equation}
        \dist{x}{y^*} \ge \dist{x}{\hat q}/\varepsilon
        ,
    \end{equation}
    and so $\hat q$ is an $\eann$.
    %In particular, if $\varepsilon=1$, then $y^* = \hat q$.
    In the recursive call on line \ref{line:findnn:recurse},
    we call $\findnn$ with $P=Q_x\cup\{\hat q\}$ to ensure that on all future iterations,
    the distance between $\hat q$ and $x$ is non-increasing.
\end{proof}

\begin{theorem}
    \label{theorem:findnn:runtime:approx}
    Let $\varepsilon\ge1$.
    Then $\findnn(\{p\},x,\varepsilon)$ has run time $O(\cdoub^5\log\aspect{})$.
    %If the data is generated i.i.d.\ from a well behaved distribution,
    %then the run time is $O(\cdoub^4\log n)$.
    %If $\varepsilon > 0$,
    %then $\findnn(\{p\},x,\varepsilon)$ finds a $\eann$ in time $O(\cdoub^4\log\aspect{} + (1/\varepsilon)^{\log \cdoub})$.
    %Furthermore, $\findnn(\{p\},x,0)$ finds an exact nearest neighbor in time $O(\chole^4\log\aspect{})$.
\end{theorem}

\begin{proof}
    As in the proof of Theorem \ref{theorem:findnn:runtime:approx}, 
    the runtime is bounded by $O(|Q_x|\cdoub^2\log\aspect{})$,
    and we need to bound $|Q_x|$.
    Let $\delta=\covdist{\hat q}$.
    Then by construction, all $q\in Q_x$ satisfy
    \begin{equation}
        \label{eq:findnn:runtime:proof}
        \dist{x}{q} 
        \le \dist{x}{\hat q}/\varepsilon + \maxdist{q}
        \le \dist{x}{q}/2 + 2\delta
        %\le 2^{i+1}
        .
    \end{equation}
    In the last inequality we used the fact that $\dist{x}{\hat q} \le \dist{x}{q}$, 
    $\varepsilon\ge 1$,
    and $\maxdist{q}\le2\delta$.
    Subtracting $\dist{x}{q}/2$ from the left and rightmost sides of \eqref{eq:findnn:runtime:proof} gives $\dist{x}{q} \le 4\delta$.
    Then by the global separating invariant, 
    $Q_x$ is a $\delta$-packing of $B(x,4\delta)$.
    So,
    \begin{equation}
        |Q_x|
        \le M_\delta(B(x,4\delta))
        \le N_\delta/2(B(x,4\delta))
        \le \cdoub^3
        .
    \end{equation}
\end{proof}

%\begin{theorem}
    %$\findnn(\{p\},x,1)$ has runtime $O(\chole^4\log\aspect{})$.
%\end{theorem}
%\begin{proof}
    %As in Theorem \ref{theorem:findnn:runtime:approx},
    %the runtime is bounded by the height of the tree times the number of children per node times the size of $Q_x$.
    %Let $\delta = \covdist{P}$.
    %Then $Q_x$ is an annulus of outer radius $\gamma = \dist{x}{\hat q}/\varepsilon + \maxdist{q}$,
    %and inner radius $\gamma-\delta$.
    %The ball $B(x,\gamma-\delta)$ is guaranteed to have no points in the data set,
    %so $|Q_x| \le \chole^2$.
%\end{proof}

%\begin{remark}
    %\fixme{}
%As written, $\findnn$ shows $\hat p$ being recalculated on each recursive call.
%This does not affect the asymptotic runtime of the algorithm,
%but a substantial constant factor improvement can be made in practical implementations.
%Notice that for each $p\in P$, the value of $\dist{p}{x}$ was calculated in the previous iteration of the algorithm.
%This value can be cached to prevent the need for future calls to calculate it.
%\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Dual tree nearest neighbor search}

In many situations, we have two sets of data $X$ and $Y$,
and we want to find for each $y\in Y$ the nearest neighbor in $X$.
For simplicity of notation, we assume that both datasets have size $n$,
although the procedures generalize to unequally sized datasets.
A simple way to calculate the nearest neighbors is by repeatedly calling the $\findnn$ function.
This takes time $O(\cdoub^4 n\log \aspect{})$.
In some situations, however, a better algorithm exists.
In particular, if we have a cover tree on both data sets $X$ and $Y$,
we can perform a ``dual tree'' traversal of the datasets.


\newpage
\begin{algorithm}[h]
    \caption{$\dualnn$(set of cover trees $P$, set of cover trees $R$, tolerance $\varepsilon$)}
\vspace{0.1in}
finds a $\eann$ in $P$ for each node $r\in R$
\begin{algorithmic}[1]
    \State let $Q = \{ q: q \in \children p, p\in P \}$
    \For {$r \in R$}
        \State let $\hat q = \argmin_{q\in Q} \dist{q}{r}$
        \If {$Q = \{\hat q\}$ \textbf{or} $\dist{x}{\hat q} \ge 2\cdot\covdist{\hat q}(1+1/\varepsilon)$}
            \label{line:dualnnorig:if}
            %\For {$s \in \data{r}$}
                %\State $\nn s \leftarrow \hat q$
            %\EndFor
            %\State set the nearest neighbor of each $s\in\data r$ to $\hat q$
            \State set the result for each $s\in\data r$ to $\hat q$
        \Else
            \State let $Q_r = \{ q : q \in Q, \dist{r}{q} \le \dist{r}{\hat q} + \maxdist{q} + 2\cdot \maxdist{r} \}$
            \State $\dualnn(Q_r,\children r)$
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    $\dualnn(\{p\},\{r\},\varepsilon)$ finds a $\eann$ in $\data{p}$ for each $s\in\data{r}$.
\end{theorem}
\begin{proof}
    %First notice that every $s\in\data{r}$ will have some $\hat q$ assigned to it.
    %This is because on each recursive call to $\dualnn$,
    %either we assign some $\hat q$ to $s$ or we recursively call $\dualnn$
    %(which will assign some $\hat q$ to $s$).
    We show that every $\hat q$ satisfying the if statement in line \ref{line:dualnnorig:if} is a $\eann$ of every $s\in\data{r}$.
    The main idea is similar to the proof of Theorem \ref{theorem:findnnorig:correct},
    but the details are more complicated due to the additional need to track descendants of the query point.

    For the first case where $Q=\{\hat q\}$, 
    we show that $\hat q$ must be the true nearest neighbor of all $s\in\data{r}$.
    Let $s$ be an arbitrary point in $\data r$,
    and $y^*_s$ denote the true nearest neighbor of $s$ in $\data{p_0}$.
    Assume for induction that $y^*_s$ is in $\data{P}$ and hence in $\data{Q}$.
    Let $q^*_s$ denote the node in $Q$ that contains $y^*_s$.
    Then we have that
    \begin{align}
        \dist{r}{q^*_s}
        &\le \dist{r}{s} + \dist{s}{y^*_s} + \dist{y^*_s}{q^*_s}
        \\&\le \maxdist{r} + \dist{s}{\hat q} + \maxdist{q^*}
        \\&\le \maxdist{r} + \dist{s}{r} + \dist{r}{\hat q} + \maxdist{q^*}
        \\&\le 2\cdot\maxdist{r} + \dist{r}{\hat q} + \maxdist{q^*}
        .
    \end{align}
    This is precisely the requirement that all $q\in Q_r$ must satisfy,
    and so $q^*_s \in Q_r$.

    The second case of follows from a long sequence of calculations.
    By the triangle inequality, we have
    $
        \dist {r}{s} + \dist{s}{\hat q}
        \ge \dist{r}{\hat q}
        $,
        and so
    \begin{align}
        %\\
        \dist{s}{\hat q} 
        &\ge \dist{r}{\hat q} - \dist{s}{r}
        \label{eq:dualnnorig:a}
        \\&\ge (3\cdot\maxdist r+ 2\cdot\covdist{\hat q})(1+1/\varepsilon) - \maxdist{r}
        \label{eq:dualnnorig:b}
        \\&\ge (2\cdot\maxdist r+ 2\cdot\covdist{\hat q})(1+1/\varepsilon) 
        \label{eq:dualnnorig:c}
    \end{align}
    We also have that
    \begin{align}
        \dist{s}{\hat q} 
        &\le \dist{s}{r} + \dist{r}{\hat q}
        \label{eq:dualnnorig:1}
        \\&\le \maxdist{r} + \dist{r}{q^*_s}
        \label{eq:dualnnorig:2}
        \\&\le \maxdist{r} + \dist{r}{s} + \dist{s}{y^*_s} + \dist{y^*_s}{q^*_s}
        \label{eq:dualnnorig:3}
        \\&\le 2\cdot\maxdist{r} + \dist{s}{y^*_s} + 2\cdot\covdist{q^*}
        \label{eq:dualnnorig:4}
        .
    \end{align}
    %Line \eqref{eq:dualnnorig:1} is the triangle inequality,
    %line \eqref{eq:dualnnorig:2} follows from the definition of $\mkfunction{maxdist}$ and $\hat q$;
    %line \eqref{eq:dualnnorig:3} is again the triangle inequality,
    %and line \eqref{eq:dualnnorig:2} 
    Substituting \eqref{eq:dualnnorig:c} into \eqref{eq:dualnnorig:4} gives
    \begin{align}
        \dist{s}{\hat q} 
        &\le \dist{s}{y^*_s} + \frac{\dist{s}{\hat q}}{1+1/\varepsilon}
        .
    \end{align}
    Finally, simplifying gives $\dist{s}{\hat q} \le (1+\varepsilon)\dist{s}{y^*_s}$.
    Hence, $\hat q$ is a $\eann$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{algorithm}[h]
    \caption{$\dualnn$(set of cover trees $P$, set of cover trees $R$, tolerance $\varepsilon$)}
\vspace{0.1in}
finds a $\eann$ in $P$ for each node $r\in R$
\begin{algorithmic}[1]
    \State let $Q = \{ q: q \in \children p, p\in P \}$
    \For {$r \in R$}
        \State let $\hat q = \argmin_{q\in Q} \dist{q}{r}$
        \If {$Q = \{\hat q\}$ \textbf{or} $\dist{x}{\hat q} \ge 2\cdot\covdist{\hat q}(1+1/\varepsilon)$}
            \label{line:dualnnorig:if}
            %\For {$s \in \data{r}$}
                %\State $\nn s \leftarrow \hat q$
            %\EndFor
            %\State set the nearest neighbor of each $s\in\data r$ to $\hat q$
            \State set the result for each $s\in\data r$ to $\hat q$
        \Else
        \State let $Q_r = \{ q : q \in Q, \dist{r}{q} \le \dist{r}{\hat q}\hl{$/(1+\varepsilon)$} + \maxdist{q} + 2\cdot \maxdist{r} \}$ 
            \State $\dualnn(Q_r \hl{$\cup\{\hat q\}$},\children r)$
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}


\begin{theorem}
    Let $p$ be a cover tree of data set $X$ and $r$ be a cover tree of data set $Y$.
    Then for each $y\in Y$, $\dualnn(\{p\},\{r\},\varepsilon)$ finds a nearest neighbor in $X$. 
\end{theorem}
\begin{proof}
    %Let $s$ be some data point in $\data{r}$,
    %and let $s^*$ be the true nearest neighbor in $X$.
    %Then on each iteration,
    We first show that for each $s\in\data{r}$, 
    the true nearest neighbor $s^*$ is in $\data{Q_r}$.
    Assume for induction that $s^* \in \data{P}$.
    This is clearly satisfied in the first call of $\dualnn$ where $P=\{p\}$.
    In subsequent iterations, $s^*$ will be in $\data{P}$ precisely when $s^*$ was in $Q_r$ on the previous iteration.

    For each node $r\in R$, let $y^*_r$ denote the true nearest neighbor of $r$ in $X$.
    We will show that on each recursion,
    for all $s\in\descendants r$,
    either $y^*_s \in Q_r$ or $\hat q$ is an $\eann$.
    %Since the algorithm only terminates when $Q_x$ is empty,
    %(and Theorem \ref{theorem:findnn:runtime} below shows that $\findnn$ must terminate),
    %the returned $\hat q$ must be an $\eann$.

    In the first recursive call where $\nn*s \not\in\data{Q_r}$,
    we must have that $\nn*s \in \data{Q}$.
    Let $q_s^*$ be the node in $Q$ such that $\nn*s \in\data{\q_s^*}$.
    Then by the definition of $Q_r$, we have that
    \begin{equation}
        \label{eq:dualknn:correctness:1}
        \dist{r}{q_s^*}
        > \dist{r}{\hat q}/\varepsilon + \maxdist{q_s^*} + \maxdist{r}
        .
    \end{equation}
    Using the triangle inequality on the left and the definition of $\mkfunction{maxdist}$ on the right gives
    \begin{equation}
        \dist{r}{s} + \dist{s}{\nn*s} + \dist{\nn*s}{q_s^*} 
        %\ge \dist{r}{q^*} 
        %>   \dist{r}{\hat q}/\varepsilon + \maxdist{q^*}
        \ge
        \stackrel{\eqref{eq:dualknn:correctness:1}}{~~~~...~~~~}
        \ge \dist{r}{\hat q}/\varepsilon + \dist{q_s^*}{\nn*s} + \dist{r}{s}
        .
    \end{equation}
    Subtracting $\dist{q^*}{y^*}$ from the leftmost and rightmost sides gives
    \begin{equation}
        \dist{s}{\nn*s} \ge \dist{r}{\hat q}/\varepsilon
        ,
    \end{equation}
    and so $\hat q$ is an $\eann$.
    In the recursive call on line \ref{line:findnn:recurse},
    we call $\findnn$ with $P=Q_x\cup\{\hat q\}$ to ensure that on all future iterations,
    the distance between $\hat q$ and $x$ is non-increasing.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Inserting a single point}

The algorithm for inserting points is split into two functions.
The most important is $\ctinsertloop$ (Algorithm \ref{alg:ctinsertloop}).
Like $\findnn$, $\ctinsertloop$ recursively traverses a set of input trees to find a suitable location for insertion.
On each iteration,
a set $Q_x$ is created that contains all the subtrees that may need to be searched.
The global separating invariant requires that we traverse several subtrees simultaneously to ensure that the distance between $x$ is not too close to other nodes on the same level in a different subtree.
(A simple depth first search, for example, would not ensure this property.)
Interestingly, the maximum size of $Q_x$ is smaller in $\ctinsertloop$ than in $\findnn$,
so the runtime dependence on $\cdoub$ is somewhat better
(to the 3rd power instead of the 4th).
$\ctinsertloop$ requires that the inserted data point be ``close enough'' to one of the sub trees to be inserted as a child. 
This prerequisite will not be satisfied by all data, 
so $\ctinsert$ (Algorithm \ref{alg:ctinsert}) is the main interface for insertion.
It handles the case when the inserted data point is relatively far away,
then calls $\ctinsertloop$ if the data point is close and a recursive descent is required.
We now prove the correctness and runtime of insertion.

\begin{algorithm}[t]
    \caption{$\ctinsert$(cover tree $p$, data point $x$)}
    \label{alg:ctinsert}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \If {$\dist{p}{x} > \covdist{p}$}
        \State create a cover tree rooted at $x$ with level $=\ceil{\log_2 \dist{p}{x}}$ and children $=\{p\}$
        \label{line:ctinsert:newtree}
    \ElsIf {$\dist{\hat q}{x} \ge \covdist{p}/2$}
        \label{line:ctinsert:childcondition}
        \State insert $x$ as a child into $p$
        \label{line:ctinsert:child}
    \Else
        \State $\ctinsertloop(\children{p},x)$
        \label{line:ctinsert:ctinsertloop}
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{$\ctinsertloop$(set of cover trees $P$, data point $x$)}
    \label{alg:ctinsertloop}
    \vspace{0.1in}

    Precondition 1: $\dist{x}{\hat p} \le \covdist{\hat p}$

    Precondition 2: $P$ contains all nodes at level $i$ with distance to $x$ less than $2^i$

\begin{algorithmic}[1]
    \State let $Q = \{ q : q \in \children{p}, p\in P\}$
    \State let $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \label{ctinsertloop:line:qhat}
    \If {$\dist{\hat q}{x} \ge \covdist{\hat q}$}
        \label{line:ctinsertloop:if}
        \State let $\hat p = \argmin_{p\in P} \dist{p}{x}$
        \State insert $x$ as a child into $\hat p$
    \Else
        \label{line:cdinsertloop:else}
        \State let $Q_x = \{q : q\in Q, \dist{q}{x} \le \covdist{q}\}$
        \State $\ctinsertloop(Q_x, x)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    If $p$ is a valid cover tree, then $\ctinsert(p,x)$ inserts $x$ into $p$.
\end{theorem}
\begin{proof}
    If $x$ is too far from $p$ to be a child, 
    then line \ref{line:ctinsert:newtree} creates a new node at $x$ with $p$ as its only child.
    The level of this new node is calculated to exactly satisfy the covering invariant.
    If $x$ can be inserted as a child of $p$ without breaking the separating invariant,
    line \ref{line:ctinsert:child} does so.
    Otherwise, $x$ must be inserted as a descendent of some $p\in P$.
    The fact that $x$ did not satisfy the condition on line \ref{line:ctinsert:childcondition}
    means that $\children{p}$ and $x$ must satisfy the preconditions of $\ctinsertloop$.
    So $\ctinsert$ calls this helper function.

    $\ctinsertloop$ takes as input a set of cover trees $P$ and inserts a data point $x$ into one of them.
    Let $i+1$ be the level of the nodes in $Q$.
    The if statement on line \ref{line:ctinsertloop:if} checks whether adding $x$ to level $i+1$ would violate the global separating invariant.
    Precondition 2 ensures that all such nodes in level $i+1$ are children of some node in $p$.
    If the separating invariant cannot be satisfied, 
    then $x$ must be inserted at a lower level.
    In the else clause on line \ref{line:cdinsertloop:else}, 
    we form a set $Q_x$ of nodes that $x$ may be inserted under,
    and recursively call $\ctinsertloop$.
    The definition of $Q_x$ ensures that $\ctinsertloop$'s preconditions will be met.
\end{proof}

\begin{theorem}
    The runtime of $\ctinsert(p,x)$ is $O(\cdoub^3 \log\aspect{})$.
\end{theorem}

\begin{proof}
    The runtime of $\ctinsert$ excluding the call to $\ctinsertloop$ takes only constant time.
    So the overall runtime is the runtime of $\ctinsertloop$.
    The bound on the runtime of $\ctinsertloop$ follows the same pattern as the bound on $\findnn$.

    The total runtime of $\ctinsertloop$ is bounded by the product of the number of recursive calls and the time spent in each call.
    The number of recursions is bounded by the height of the tree because $\ctinsertloop$ descends one level of the tree with each recursive call. 
    This height is bounded by $O(\log\aspect{})$ in Lemma \ref{lemma:height}.

    The cost of each recursive call is $O(|Q|)$,
    since we perform one distance computation for each node in $Q$.
    The size of $Q$ is the size of $P$ times the of children per node.
    To bound the size of $P$, we will bound the size of $Q_x$ 
    (since $P$ is always $Q_x$ of the previous iteration).
    Let $\delta=2^{i-1}$.
    Then $Q_x$ is a $\delta$-packing of $B(x,\delta)$ by construction.
    So,
    \begin{equation}
        |Q_x| \le M_\delta(B(x,\delta)) \le N_{\delta/2}(B(x,\delta)) \le \cdoub.
    \end{equation}
    Each node in $P$ has at most $\cdoub^2$ children by Lemma \ref{lemma:children},
    so $|Q| \le \cdoub^3$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Merging cover trees}

\begin{pseudocode}{$\ctmergeloop$(set of cover trees $P$, cover tree $r$)}
    \State $P' \leftarrow P$, $R' \leftarrow \{\}$
    \State let $\hat p_r = \argmax_{p \in P} \dist{p}{r}$
    \If {$\dist{\hat p_r}{r} \ge 2\cdot\covdist{p_r} + \maxdist{r}$}
        \State $R' \leftarrow \{r\}$
    \Else
        \For {$s \in \children r$}
            %\State let $Q = \{q : q \in \children p, p \in P \}$
            %\State let $\hat q_s = \argmax_{q\in Q} \dist{q}{s}$
            %\State let $\hat p_s = \argmax_{p\in P} \dist{p}{s}$

            %\State let $Q_s = \{q : q \in Q, \dist{q}{s} \le 2\cdot\covdist{q} + \maxdist{s} \}$
            \State let $Q_s = \{q : q \in \children{p}, p\in P', \dist{q}{s} \le 2\cdot\covdist{q} + \maxdist{s} \}$
            \State let $(Q_s', S') = \ctmergeloop(Q_s,s)$
            \State \fixme{update $P'$ based on $Q_s'$}
            \While {$S' \ne \{\}$}
                \State select any $s' \in S'$
                %\State let $\hat q_{s'} = \argmax_{q\in Q} \dist{q}{s'}$
                \State let $\hat p_{s'} = \argmax_{p\in P} \dist{p}{s'}$
                \If {$\dist{\hat p_{s'}}{s'} \le \covdist{\hat p_{s'}}$}
                    %\State \fixme{add $s'$ as a child of $\hat p_{s'}$}
                    \State let $\hat p'_{s'}$ be $\hat p_{s'}$ with $s'$ added as a child
                    \State $P' \leftarrow \big( P' \setminus \{\hat p_s\} \big) \cup \{\hat p'_{s'}\}$
                \Else
                    \State create node $r'$ such that:
                    \State ~~~~~$dp(r') = dp(s')$
                    \State ~~~~~$\level{r'}=\level{s'}+1$
                    \State ~~~~~$\children{r'} = \{ s : s\in S', \dist{s}{r'} \le \covdist{r'} \}$
                    \State $S' \leftarrow S'\setminus\children{r'}$
                    \State $R' \leftarrow R'\cup\{r'\}$
                \EndIf
            \EndWhile
        \EndFor
    \EndIf
    \State \Return $(P',R')$
\end{pseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
%\begin{algorithm}[H]
%\caption{Single point insertion}
%\label{alg:insert}

%\vspace{0.1in}
\noindent
{\bfseries function} \ctinsert(cover tree $p$, data point $x$)
\begin{algorithmic}[1]
    \State let $Q = \{ q : q \in \children{p}, p\in P\}$
    \State let $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \algstore{insert}
\end{algorithmic}

    \noindent
    %\hspace{-0.3in}
\begin{algorithmic}[1]
    \algrestore{insert}
    \If {$\dist{p}{x} > \covdist{p}$}
        \State \Return node(data=$x$,level=$\ceil{\log_2 \dist{p}{x}}$,children=$\{p\}$)
    \Else
        \If {$\dist{\hat q}{x} \ge \covdist{\hat q}$}
            \State let $\hat p = \argmin_{p\in P} \dist{p}{x}$
            \State let $\hat p' = \hat p$ with $x$ added as a child
            \State \Return $\big(P\setminus\{\hat p\}\big) \cup \{\hat p'\}$
        \Else
            \State let $Q_x = \{q : q\in Q, \dist{q}{x} \le \covdist{q}\}$
            \State let $Q' = \ctinsertloop(Q_x, x)$
            \State let $q$   be the unique element of $Q  \setminus(Q\cap Q_x)$
            \State let $q_x$ be the unique element of $Q_x\setminus(Q\cap Q_x)$
            \State let $p_q$ be the element of $P$ with $q$ as its child
            \State let $p_{q_x}$ be the node $p_q$ with child $q$ replaced by $q_x$
            \State \Return $\big(P\setminus\{p_q\}\big) \cup \{p_{q_x}\}$
        \EndIf
    \EndIf
\end{algorithmic}
%\end{algorithm}


\begin{algorithm}[H]
\caption{Single point insertion}
\label{alg:insert}
    \vspace{0.1in}
{\bfseries function} \ctinsert(cover tree $p$, data point $x$)

\begin{algorithmic}[1]
    \If {$\dist{p}{x} > \covdist{p}$}
        \State \Return node(data=$x$,level=$\ceil{\log_2 \dist{p}{x}}$,children=$\{p\}$)
    \Else
        \State \Return $\ctinsertloop(\children{p}, x)$
    \EndIf
\end{algorithmic}

    %\vspace{0.1in}
{\bfseries function} \ctinsertloop(set of cover trees $P$, data point $x$)

\begin{algorithmic}[1]
    \State let $Q = \{ q : q \in \children{p}, p\in P\}$
    \State let $\hat q = \argmin_{q\in Q} \dist{q}{x}$
    \If {$\dist{\hat q}{x} \ge \covdist{\hat q}$}
        \State let $\hat p = \argmin_{p\in P} \dist{p}{x}$
        \State let $\hat p' = \hat p$ with $x$ added as a child
        \State \Return $\big(P\setminus\{\hat p\}\big) \cup \{\hat p'\}$
    \Else
        \State let $Q_x = \{q : q\in Q, \dist{q}{x} \le \covdist{q}\}$
        \State let $Q' = \ctinsertloop(Q_x, x)$
        \State let $q$   be the unique element of $Q  \setminus(Q\cap Q_x)$
        \State let $q_x$ be the unique element of $Q_x\setminus(Q\cap Q_x)$
        \State let $p_q$ be the element of $P$ with $q$ as its child
        \State let $p_{q_x}$ be the node $p_q$ with child $q$ replaced by $q_x$
        \State \Return $\big(P\setminus\{p_q\}\big) \cup \{p_{q_x}\}$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
%\caption{Simplified cover tree insertion}
\label{alg:insert}

    \vspace{0.1in}
{\bfseries function} \ctinsert(cover tree $p$, data point $x$)

returns a new cover tree $p'$ that contains $x$ and all the points in $p$

\begin{algorithmic}[1]
    \If {$\dist p x > \covdist p$}
        \State create a new node $p'$ from data point $x$
        \State set $\level{p'}$ to  $\ceil{\log_2 d(p,p')}$
        \State set $\children{p'}$ to $\{p\}$
        \State\Return $p'$
    \Else
        \State let $q = \argmin_{q\in\children{p}} \dist{q}{x}$.
        \If {$\dist q x > \covdist p/2$}
            \State let $p' = p$ with $x$ added as a child
            \State\Return $p'$ 
        \Else
            \State let $q' = \ctinsert(q,x)$
            \State let $p' = p$ with child $q$ replaced by $q'$
            %\State set $\level{p'}$ to $\ceil{\log_2\argmax_{q\in\children{p'}} d(p',q)}$
            \State \Return $p'$
        \EndIf
        %\For {$q \in \children{p}$ sorted in ascending order by $\dist{q}{x}$}
            %\If {$\dist q x \le \covdist p/2$}
                %\State let $q' = \ctinsert(q,x)$
                %\State let $p' = p$ with child $q$ replaced by $q'$
                %\State set $\level{p'}$ to $\ceil{\log_2\argmax_{q\in\children{p'}} d(p',q)}$
                %\State \Return $p'$
            %\EndIf
        %\EndFor
        %\State let $p' = p$ with $x$ added as a child
        %\State\Return $p'$ 
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
If $p$ is a valid cover tree, then $\ctinsert(p,x)$ is a valid cover tree.
\end{theorem}

\begin{proof}
    There are three cases, and we consider each in turn.
    When the first if statement is satisfied, 
    the covering invariant prevents $x$ from being added as a child of $p$.
    So we create a new root node $p'$ above $p$.
    The level of $p'$ is set in line 3 as low as possible to satisfy both the leveling and covering invariants.
    (The level of $p'$ may in general be arbitrarily higher than the level of $p$.)
    The separating invariant is satisfied because $p'$ has only one child.
    Note that every time this if statement triggers,
    the $\mkfunction{covdist}$ of the root node increases by a factor of two.
    Thus when we insert a sequence of points with bounded diameter, 
    eventually the $\mkfunction{covdist}$ will be large enough that this if statement will no longer trigger and points will be forced to be inserted as a children.

    When the first if statement is not satisfied, 
    $x$ must be inserted underneath $p$.
    The if statement on line 8 checks if $x$ can be added directly as a child to $p$.
    Adding new children does not affect either the leveling or covering invariants,
    and the condition ensures that the separating invariant remains satisfied.

    If $x$ cannot be added as a child, 
    then it is added to the subtree rooted at $q$ in the else clause beginning on line 11.
    Recall that the leveling invariant allows the children of $p$ to have arbitrarily small level.
    So there are two cases we must consider.
    (Case 1:) When $\level{q}=\level{p}-1$, 
    the recursive call to $\ctinsert$ on line 9 will return a cover tree rooted at $q$.
    (The if statement on line 1 will not be triggered in the recursion).
    So the leveling, covering, and separating invariants of $p$ all remain satisfied.
    (Case 2:) When $\level{q}<\level{p}-1$, 
    the recursive call to $\ctinsert$ will return a cover tree with a root node at $x$ instead of $q$.
    (The if statement on line 1 will be triggered in the recursion).
    %The leveling invariant of $p$ may be affected if $q$ was the child of maximum distance to $p$.
    %In particular, $\level{p}$ may need to decrease in order to satisfy the leveling invariant,
    %and line 11 accounts for this possibility.
    The covering invariant of $p$ remains satisfied due to the if statement on line 1.
    %The separating invariant is subtle and depends on the order that children are considered in the for loop.
    %In particular, let $r\le q$ be a 
\end{proof}

\begin{theorem}
    \label{theorem:ctinsert1}
    The runtime of $\ctinsert$ is $O(\cdoub^2\log\aspect{X})$.
\end{theorem}
\begin{proof}
    If $\dist{p}{x} > \covdist{p}$, then the running time is constant.
    Otherwise, the algorithm loops over all the children of $p$ and recurses on at most one child.
    There are at most $\cdoub^2$ children (Lemma \ref{lemma:children}),
    and the height of the tree is at most $\log_2\aspect{X}$ (Lemma \ref{lemma:height}).
\end{proof}

\begin{theorem}
    \label{theorem:ctinsert2}
    When the data is drawn i.i.d.\ from a well behaved distribution,
    then the expected runtime of $\ctinsert$ is $O(\cdoub^2\log n)$.
\end{theorem}
\begin{proof}
    Apply Lemma \ref{lemma:Easpect} to Theorem \ref{theorem:ctinsert1}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Merging cover trees}
    \label{alg:merge}
    \vspace{0.1in}
    {\bfseries function} \ctmerge(cover tree $p$, set of cover trees $R$)

\begin{algorithmic}[1]
    \For {$r \in R$}
        \If {$\max_{q\in\children{p}} \dist{r}{q} > \covdist{p}/2$}
            \State add $r$ as a child of $p$
            \State remove $r$ from $R$
        \EndIf
    \EndFor
    \For {$q \in \children{p}$}
        \State let $R_q = \{ r : r\in R, \dist{r}{q} \le \covdist{p}/2\}$
        \State let $q' = \ctmerge(q,R_q)$
        \State replace $q$ with $q'$ as a child of $p$
    \EndFor
\end{algorithmic}
\end{algorithm}
%\State let $\ell = \max_{r\in R} \level r$ 
%\If {$\level p > \ell$}
    %\State let $R_{near} = \{ r : r\in R, \min_{q\in\children{p}}\dist{q}{r} \le \covdist p/2\}$.
    %\State let $R_{far} = R-R_{near}$
%\Else
    %\State let $R=\{\}$
    %\For {$r\in R$}
        %\If {$\level{r}=\ell$}
            %\State create node $r'$ with same data point as $r$ but level $-\infty$
            %\State $R' \leftarrow R' \cup \{r'\} \cup \children{r}$
        %\Else
            %\State $R' \leftarrow R' \cup \{r\}$
        %\EndIf
    %\EndFor
    %\State $\ctmerge(p,R')$
%\EndIf

%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{algorithm}[H]
    %\caption{Merging cover trees}
    \label{alg:merge}

\vspace{0.1in}
{\bfseries function} $\ctmerge$(cover tree $p$, cover tree $r$)
\begin{algorithmic}[1]
    \State $(P,R) \leftarrow \ctmergeloop(\children{p},r)$
    \State $p' \leftarrow$
    \While {$R \ne \{\}$}
        \State
    \EndWhile
    \State \Return $p'$
\end{algorithmic}


\vspace{0.1in}
{\bfseries function} $\ctmergeloop$(set of cover trees $P$, cover tree $r$)

%requires $\level{p} = \level{r}$

\begin{algorithmic}[1]
    \State $P' \leftarrow P$, $R' \leftarrow \{\}$
    \State let $\hat p_r = \argmax_{p \in P} \dist{p}{r}$
    \If {$\dist{\hat p_r}{r} \ge 2\cdot\covdist{p_r} + \maxdist{r}$}
        \State $R' \leftarrow \{r\}$
    \Else
        \For {$s \in \children r$}
            %\State let $Q = \{q : q \in \children p, p \in P \}$
            %\State let $\hat q_s = \argmax_{q\in Q} \dist{q}{s}$
            %\State let $\hat p_s = \argmax_{p\in P} \dist{p}{s}$

            %\State let $Q_s = \{q : q \in Q, \dist{q}{s} \le 2\cdot\covdist{q} + \maxdist{s} \}$
            \State let $Q_s = \{q : q \in \children{p}, p\in P', \dist{q}{s} \le 2\cdot\covdist{q} + \maxdist{s} \}$
            \State let $(Q_s', S') = \ctmergeloop(Q_s,s)$
            \State \fixme{update $P'$ based on $Q_s'$}
            \While {$S' \ne \{\}$}
                \State select any $s' \in S'$
                %\State let $\hat q_{s'} = \argmax_{q\in Q} \dist{q}{s'}$
                \State let $\hat p_{s'} = \argmax_{p\in P} \dist{p}{s'}$
                \If {$\dist{\hat p_{s'}}{s'} \le \covdist{\hat p_{s'}}$}
                    %\State \fixme{add $s'$ as a child of $\hat p_{s'}$}
                    \State let $\hat p'_{s'}$ be $\hat p_{s'}$ with $s'$ added as a child
                    \State $P' \leftarrow \big( P' \setminus \{\hat p_s\} \big) \cup \{\hat p'_{s'}\}$
                \Else
                    \State create node $r'$ such that:
                    \State ~~~~~$dp(r') = dp(s')$
                    \State ~~~~~$\level{r'}=\level{s'}+1$
                    \State ~~~~~$\children{r'} = \{ s : s\in S', \dist{s}{r'} \le \covdist{r'} \}$
                    \State $S' \leftarrow S'\setminus\children{r'}$
                    \State $R' \leftarrow R'\cup\{r'\}$
                \EndIf
            \EndWhile
        \EndFor
    \EndIf
    \State \Return $(P',R')$
\end{algorithmic}
\end{algorithm}

\begin{lemma}
    Let $P$ be a set of cover trees of level $i$ with minimum distance $2^i$, 
    and $r$ be a cover tree of level $i$.
    Then the sets $P'$ and $R'$ returned by $\ctmergeloop(P,r)$ satisfy:
    \begin{enumerate}
        \item every element $p' \in P'$ and $r'\in R'$ are valid cover trees with level $i$,
        \item for all $j<i$, $\cup_{p' \in P'} \descendants {p'}$
        \item 
    \end{enumerate}
\end{lemma}

\begin{lemma}
    The functions $\ctmerge$ and $\ctmergeloop$ obey the invariant that when $\ctmergeloop$ is called, $|P| \le \cdoub^3$.
\end{lemma}

%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{algorithm}[H]
    %\caption{Merging cover trees}
    \label{alg:merge}
    \vspace{0.1in}
    {\bfseries function} \ctmerge(set of cover trees $P$, cover tree $r$)

    requires $\level{p} = \level{r}$

\begin{algorithmic}[1]
    \State $S \leftarrow \children{r}$
    \State $S_\textit{far} \leftarrow \{\}$
    \State $P' \leftarrow P$
    \For {$s\in S$}
        \State $Q \leftarrow \{ q : q \in \children{p}, p \in P' \}$
        \State let $\hat p = \argmin_{p\in P} \dist{p}{s}$
        \State let $\hat q = \argmin_{q\in Q} \dist{q}{s}$
        \If {$\dist{\hat p}{s} \ge 2\cdot\covdist{\hat p} + \maxdist{s}$}
            \State $S_\textit{far} \leftarrow S_\textit{far} \cup \{s\}$
        \ElsIf {$\dist{\hat p}{s} \ge \covdist{\hat p}$}
            \State let $Q_s = \{q : q\in Q, \dist{q}{s} \le 2\cdot\covdist{q}+\maxdist{s}\}$
            \State $(P',T_\textit{far}') \leftarrow \mergeinsert(P',\{\},Q_s,s)$
            \While {$T_\textit{far}' \ne \{\}$}
                \State let $t$ be any node in $T_\textit{far}'$
                \State construct node $s'$ with $\level{s'} = \level{t}+1$
                \State ~~~~~set data point to $t$
                \State ~~~~~set children equal $\{ t : t \in T'_\textit{far}, \dist{t}{s'} \le \covdist{s'}$
                \State $S'_\textit{far} \leftarrow S'_\textit{far} \cup s'$
            \EndWhile
            %\State $T'\leftarrow \{\}$
            %\For {$t \in \children{s}$}
                %%\State let $\hat q_t = \argmin_{q\in Q} \dist{q}{t}$
                %\State let $Q_t = \{q : q\in Q, \dist{q}{t} \le 2\cdot\covdist{q}+\maxdist{t}\}$
                %\State $(Q_t',R_\textit{far}') \leftarrow \ctmerge(P'Q_t,t)$
            %\EndFor
        \ElsIf {$\dist{\hat q}{s} \ge \covdist{\hat p}/2$}
            \State let $\hat q'$ be the node $\hat q$ with $s$ added as a child
            \State let $p_{\hat q}$ be the element of $P'$ with $\hat q$ as a child
            \State let $p_{\hat q}'$ be the node $p_{\hat q}$ with child $\hat q$ replaced with $\hat q'$
            \State $P' \leftarrow \big(P' \setminus \{p_{\hat q}\}\big) \cup \{ p_{\hat q}'\}$
        \Else
            \State let $Q_s = \{ q : q \in Q, \dist{q}{s} \le 2\cdot\covdist{q} \}$
            \State $(P',S_\textit{far}') \leftarrow \mergeinsert(P',S_\textit{far}',Q_s,s)$
            %\State $(Q_s', s') \leftarrow \ctmerge(Q_s,s)$
            %\State let $q _s$  be the single element of $Q _s \setminus (Q_s \cap Q_s')$
            %\State let $q'_s$ be the single element of $Q'_s \setminus (Q_s \cap Q_s')$
            %\State let $p _{q_s}$ be the element of $P'$ with $q_s$ as a child
            %\State let $p'_{q_s}$ be the node $p_{q_s}$ with child $q_s$ replaced with $q'_s$
            %\State $P' \leftarrow \big(P' \setminus \{p_{q_s}\}\big) \cup \{p'_{q_s}\}$
            %\State $S_\textit{far} \leftarrow S_\textit{far} \cup s'$
        \EndIf
    \EndFor
    \State \Return $(P',S')$
\end{algorithmic}

    \vspace{0.1in}
    {\bfseries function} $\mkprocedure{merge\_insert}$(set of cover trees $P$, set of cover trees $Q$, cover tree $r$)
\begin{algorithmic}[1]
    \State $(Q_s', s') \leftarrow \ctmerge(Q_s,s)$
    \State let $q _s$  be the single element of $Q _s \setminus (Q_s \cap Q_s')$
    \State let $q'_s$ be the single element of $Q'_s \setminus (Q_s \cap Q_s')$
    \State let $p _{q_s}$ be the element of $P'$ with $q_s$ as a child
    \State let $p'_{q_s}$ be the node $p_{q_s}$ with child $q_s$ replaced with $q'_s$
    \State let $P' = \big(P \setminus \{p_{q_s}\}\big) \cup \{p'_{q_s}\}$
    %\State $P' \leftarrow \big(P' \setminus \{p_{q_s}\}\big) \cup \{p'_{q_s}\}$
    %\State let $S_\textit{far} \leftarrow S_\textit{far} \cup s'$
    \State \Return $(P',s')$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(set of cover trees $P$, query  point $x$, nearest neighbor so far $y$, allowable tolerance $\varepsilon$)

\begin{algorithmic}[1]
    \If {$P = \{\}$}
        \State\Return $y$
    \Else
        \For {$p\in P$}
            \If {$d(p,x) < d(y,x)$}
                \State $y \leftarrow p$
            \EndIf
        \EndFor
        \State $P' \leftarrow \{\}$
        \For {$p\in P$}
            \For {$q \in \children p$}
                \If {$\dist{x}{q} \le \dist{x}{y}/\varepsilon + \maxdist{q}$}
                    \State $P' \leftarrow P'\cup\{q\}$
                \EndIf
            \EndFor
        \EndFor
        %\State let $P' = \{ q : q\in\children{p}, p\in P, \dist{x}{y} > \dist{x}{q}/\varepsilon - \maxdist{q} \}$
        %\State let $P' = \{ q : q\in\children{p}, p\in P, \dist{x}{q} \le \varepsilon\cdot\dist{x}{y}+\maxdist{q}\}$
        \State\Return $\findnn(P',x,y,\varepsilon)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{definition}
    Let $\set X$ be a metric space, $X\subset\set X$, and $x$ be in $\set X$ but not $X$.
    Then the nearest neighbor $y^*\in X$ is defined as $y^*=\argmin_{y\in X} \dist{x}{y}$.
    An $\varepsilon$-nearest neighbor (for $\varepsilon\ge1$) is a point $\hat y$ satisfying $\dist{x}{\hat y} \le \varepsilon \cdot\dist{x}{y^*}$.
\end{definition}

\begin{theorem}
    The return value of $\findnn(\{p\},x,p,\varepsilon)$ is an $\varepsilon$-approximate nearest neighbor of $x$.
\end{theorem}
\begin{proof}
    The algorithm maintains the invariant that either $y$ is an $\varepsilon$-ann,
    or $P$ contains a tree that contains an $\varepsilon$-ann.
    Since the algorithm only terminates when $P$ is empty,
    the invariant ensures that the returned value is an $\varepsilon$-ann.

    If $y$ is already an $\varepsilon$-ann,
    then the algorithm must return an $\varepsilon$-ann because it will only ever update $y$ to be closer to $x$.
    Assume $y$ is not an $\varepsilon$-ann.
    Then if the root of some subtree in $P$ is an $\varepsilon$-ann,
    the for loop on lines 4-6 will update $y$ to be an $\varepsilon$-ann.
    Otherwise, there must exist an $\varepsilon$-ann in some child $q$ of some subtree $p\in P$.
    Let $\hat y$ denote the $\varepsilon$-ann.
    We have that
    \begin{equation}
        \varepsilon \cdot \dist{x}{y^*}
        \le \dist{x}{\hat y} 
        \le \dist{x}{y} 
        .
    \end{equation}
    Then by the triangle inequality, we have
    \begin{equation}
        \label{eq:findnn_condition}
        \dist{x}{q} 
        \le \dist{x}{\hat y} + \dist{\hat y}{q}
        \le \dist{x}{y}/\varepsilon + \maxdist{q}
        .
    \end{equation}
    The if statement in line 10 ensures that all nodes satisfying condition \eqref{eq:findnn_condition} will be added to $P_i$,
    maintaining the invariant.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(cover tree $p$, query  point $x$, tolerance $\varepsilon$)

\begin{algorithmic}[1]
    %\State let $i=0$, $P_i = \{p\}$, $y=p$
    \State $y \leftarrow p$
    \State $i\leftarrow0$
    \State $P_0 \leftarrow \{p\}$
    \While {$P_i \ne \{\}$}
        \For {$p\in P_{i}$}
            \If {$\dist{p}{x} < \dist{y}{x}$}
                \State $y \leftarrow p$
            \EndIf
        \EndFor
        \State $P_{i+1}\leftarrow \{\}$
        \For {$p\in P_{i}$}
            %\If {$\dist{p}{x} < \dist{y}{x}$}
                %\State $y \leftarrow p$
            %\EndIf
            \For {$q\in\children{p}$}
                \If {$\dist{x}{q} \le \dist{x}{y}/\varepsilon+\maxdist{q}$}
                    \State $P_i \leftarrow P_i\cup\{q\}$
                \EndIf
            \EndFor
        \EndFor
        \State $i\leftarrow i+1$
    \EndWhile 
    \State \Return y
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    The runtime of $\findnn$ with $\varepsilon \ge 2$ is $O(\cdoub^2\log\aspect{})$.
\end{theorem}
\begin{proof}
    Since the algorithm contains three nested loops, 
    the total runtime is bounded by the product of the maximum iterations of each loop.
    The innermost for loop (line 9) executes once per child,
    which by Lemma \ref{lemma:children} is $O(\cdoub^2)$.
    The outermost while loop (line 4) executes at most once per level of the tree.
    To see this, notice that $P_i$ contains only nodes that are $i$ edges away from $p$,
    and so $P_{\height p+1}$ is guaranteed to be the empty set.
    By Lemma \ref{lemma:height}, the height of $p$ is $O(\log\aspect{})$.
    The middle for loop (line 6) executes once for each node in $P_i$.
    To bound the size of $P_i$, first notice that by the global separating invariant,
    the distance between each $q_1$ and $q_2$ in $P_i$ must be at least $2^i$. 
    By our choice of $\varepsilon$, we have that all of the $q$ in $P_i$ satisfy
    \begin{equation}
        \dist{x}{q} 
        \le \dist{x}{y}/\varepsilon + \maxdist{q}
        \le \dist{x}{q}/2 + 2^i
        \le 2^{i+1}
        .
    \end{equation}
    Let $\delta=2^i$.
    Then $P_i$ is a $\delta$-packing of $B(x,2\delta)$.
    \begin{equation}
        |P_i|
        \le M_\delta(B(x,2\delta))
        \le N_\delta/2(B(x,2\delta))
        \le \cdoub^2
        .
    \end{equation}
\end{proof}

\begin{theorem}
    The runtime of $\findnn$ with $\varepsilon=1$ 
    (i.e. $\findnn$ must return the exact nearest neighbor),
    is $O(\cdoub^2 ...)$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(cover tree $p$, query  point $x$, nearest neighbor so far $y$)

\begin{algorithmic}[1]
    \If {$d(p,x) < d(y,x)$}
        \State $y \leftarrow p$
    \EndIf
    %\For {each child $q$ of $p$ sorted by distance to $x$}
    \For {$q\in\children p$ sorted in ascending order by distance to $x$}
        \If {$d(x,y) > d(x,q) - \maxdist{q}$} 
            \State $y \leftarrow \findnn(q,x,y)$
        \EndIf
    \EndFor
    \State\Return $y$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:dualnn}

    \vspace{0.1in}
    {\bfseries function} \dualnn(set of cover trees $P$, set of cover trees $R$, tolerance $\varepsilon$)

    finds all the nearest neighbors of $P$ in $R$

\begin{algorithmic}[1]
    \If {$\level P \ge \level R$}
        %\State $P' \leftarrow \{\}$
        \For {$p\in P$}
            \State $\dualnn(\children{p},R,\varepsilon)$
            %\If {$\level p = \level P$}
                %\State $P' \leftarrow P' \cup \children{p}$
                %\For {$r\in R$}
                    %\If {$nn[p] \le$}
                        %\State
                    %\EndIf
                %\EndFor
            %\Else
                %\State $P' \leftarrow P' \cup \{p\}$
            %\EndIf
        \EndFor
        %\State $\dualnn(P',R,\varepsilon)$
    \Else
        \State $R' \leftarrow \{\}$
        \For {$r\in R$}
            %\If {$\level r = \level R$}
                %\State $R' \leftarrow R' \cup \children{r}$
                \For {$p\in P$}
                    \If {$\dist{r}{p} \le \dist{nn[p]}{p}$}
                        \State $nn[p] \leftarrow r$
                    \EndIf
                    \If {$\maxdist{p} \le \dist{r}{p}/\varepsilon+\maxdist{r}$}
                        \State $R' \leftarrow R' \cup \{r\}$
                    \EndIf
                \EndFor
            %\Else
                %\State $R' \leftarrow R' \cup \{r\}$
            %\EndIf
        \EndFor
        \State $\dualnn(P,R',\varepsilon)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    Let $p^*$ and $r^*$ be the root of two cover trees.
    After running $\dualnn(\{p^*\},\{r^*\},\varepsilon)$,
    $nn[q]$ contains an $\varepsilon$-approximate nearest neighbor of $q$ in $r$ for all $q$ in $p$.
\end{theorem}

\begin{proof}
    We will show that for every node $p$ in $p^*$,
    $\dualnn$ will be called with $p\in P$ and some $\varepsilon$-ann in $R$.
    When this happens, the if statement on line 8 will ensure that $nn[p]$ gets updated to a $\varepsilon$-ann.
\end{proof}

\begin{theorem}
    Let $p$ and $r$ be the root of two cover trees.
    The runtime of $\dualnn(\{p\},\{r\},\varepsilon)$ for all $\varepsilon\ge2$ is $O(\cdoub^4\log\aspect{})$.
\end{theorem}
\begin{proof}
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:dualnn}

    \vspace{0.1in}
    {\bfseries function} \dualnn(vector of set of cover trees $V$, return value $r$)

\begin{algorithmic}[1]
    %\State let $P=\argmax_{P\in\set P} \level{P}$
    \State let $m$ be the size of $V$
    \State $i^* \leftarrow \argmax_{i\in[m]} \level{V[i]}$
    \State $P \leftarrow V[i^*]$
    \State $P' \leftarrow \{\}$
    \State $r' \leftarrow r$
    \For {$p\in P$}
        \If {$\level p = \level P$}
            %\State $P' \leftarrow P' \cup \children{p}$
            \For {$q \in \children p$}
                \State $r' \leftarrow \mkfunction{updateReturnValue}(r)$
                \If {\textbf{not} $\mkfunction{prune}(q)$}
                    \State $P' \leftarrow P' \cup \{q\}$
                \EndIf
            \EndFor
        \Else
            \State $P' \leftarrow P' \cup \{p\}$
        \EndIf
    \EndFor
    %\State $\set P' \leftarrow \set P - \{P\} \cup\{P'\}$
    \State $V[i^*] \leftarrow P'$
    \State $\dualnn(\set P')$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{algorithm}[H]
    %\caption{Merging cover trees}
    \label{alg:merge}
    \vspace{0.1in}
    {\bfseries function} \ctmerge(set of cover trees $P$, set of cover trees $R$)

\begin{algorithmic}[1]
    \State $R' \leftarrow R$
    \For {$p\in P$}
        \State $R_p \leftarrow \{\}$
    \EndFor
    \For {$r\in R'$}
        \State $p \leftarrow \argmin_{p\in P} \dist{p}{r}$
        \State $R_p \leftarrow R_p \cup \{r\}$
    \EndFor
    \For {$p \in P$}
        \State $p' \leftarrow p$
        \For {$r \in R_p$}
            \State $a \leftarrow \argmax_{q \in\children{p'}} \dist{q}{r}$
            \If {$\covdist{p}/2 \le a$}
                \State $p' \leftarrow p'$ with $r$ added as a child
            \EndIf
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}

\end{document}
