\documentclass[../main.tex]{subfiles}
 
\newcommand{\set}[1]{\mathcal {#1}}
\newcommand{\distribution}[1]{\mathcal {#1}}
\newcommand{\sized}[1]{\tilde \set {#1}}

\newcommand{\radius}{r}
\newcommand{\dist}[2]{\distf({#1},{#2})}
\newcommand{\distf}{d}
\newcommand{\diam}[1]{\textnormal{diam}({#1})}
\newcommand{\codiam}[1]{\textnormal{codiam}({#1})}
\newcommand{\aspect}[1]{\Delta}

\newcommand{\minkdim}{\text{dim}_\textnormal{Mink}}
\newcommand{\krdim}{\text{dim}_\textnormal{kr}}
\newcommand{\doubdim}{\text{dim}_\textnormal{doub}}

\newcommand{\krnum}{c_\textnormal{exp}}
\newcommand{\doubnum}{c_\textnormal{doub}}

\newcommand{\p}{\ensuremath p}
\newcommand{\q}{\ensuremath q}
%\newcommand{\varfont}[1]{\ensuremath{\textup{\text{{#1}}}}}
\newcommand{\mkfunction}[1]{\ensuremath{\text{{#1}}}}
\newcommand{\level}[1]      {\mkfunction{level}({#1})}
\newcommand{\parent}[1]     {\mkfunction{parent}({#1})}
\newcommand{\children}[1]   {\mkfunction{children}({#1})}
\newcommand{\covdist}[1]    {\mkfunction{covdist}({#1})}
\newcommand{\descendants}[1]{\mkfunction{descendants}({#1})}
\newcommand{\maxdist}[1]    {\mkfunction{maxdist}({#1})}
\newcommand{\height}[1]     {\mkfunction{height}({#1})}

% FIXME: should these be changed?
\newcommand{\nn}[1]{\ensuremath{\ensuremath{{{#1}}_{nn}}}}
\newcommand{\exprad}[1]{\ensuremath{\ensuremath{2}}}
\newcommand{\pack}{\ensuremath{\textnormal{\ttfamily pack}}}
\newcommand{\rmNodes}{\ensuremath{\textnormal{\ttfamily rmNodes}}}
\newcommand{\findnn}{\ensuremath{\textnormal{\ttfamily findNearestNeighbor}}}
\newcommand{\ctmerge}{\ensuremath{\textnormal{\ttfamily merge}}}
\newcommand{\ctinsert}{\ensuremath{\textnormal{\ttfamily insert}}}
\newcommand{\ctinsertHelper}{\ensuremath{\textnormal{\ttfamily insert\_}}}
\newcommand{\rebalance}{\ensuremath{\textnormal{\ttfamily rebalance}}}
\newcommand{\rebalanceHelper}{\ensuremath{\textnormal{\ttfamily rebalance\_}}}
\newcommand{\mkvar}[1]{\ensuremath{\textnormal{\emph{{#1}}}}}
\newcommand{\nullvar}{\ensuremath{\textup{\textnormal{\ttfamily null}}}}
\newcommand{\datapoint}[1]{\ensuremath{\textup{\textnormal{\ttfamily dp}({#1})}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\chapter{Cover Trees}

\begin{definition}
    A set $\set X$ equipped with a distance function $\distf : \set X \times \set X \to \R$ is a \emph{metric space} if it obeys the following properties:
    \begin{enumerate}
        %\item \emph{Non-negativity}.  For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} \ge 0$.
        \item \emph{Indiscernability}.  For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} = 0$ if and only if $x_1=x_2$.
        \item \emph{Symmetry}. For all $x_1,x_2\in\set X$, $\dist{x_1}{x_2} = \dist{x_2}{x_2}$.
        \item \emph{Triangle inequality}.  For all $x_1,x_2,x_3\in\set X$, $\dist{x_1}{x_2} + \dist{x_2}{x_3}\ge\dist{x_1}{x_3}$.
    \end{enumerate}
\end{definition}

\begin{table}[H]
    \small
    %\newcolumntype{Y}{>{\centering\arraybackslash}p{1in}}
    %\newcolumntype{Y}{p{0.1in}}
    %\begin{tabularx}{\textwidth}{lXXY}
    \centering
    \begin{tabular}{lccc}
        \toprule
        \vspace{-0.25in}
        &~\hspace{1.2in}~&~\hspace{1.2in}~&~\hspace{1.2in}~\\
        data structure & space & find nearest neighbor & insertion \\
        \midrule
        ball tree \cite{} & $O(n)$ & $O(n)$ & $O(n)$ \\
        metric skip list \cite{karger2002finding} & $O(n\log n)$ & $\krnum{}^{O(1)}\log n$ & $\krnum^{O(1)}\log n\log\log n$ \\
        navigating net \cite{} & $O(n)$ \\
        cover tree \cite{} & $O(n)$ & $O(\krnum^8\log n)$ & $O(\krnum^{12}\log n)$ \\
        simplified cover tree & $O(n)$ & $O(\doubnum{}\log \aspect{})$ \\
                              &        & $O(\doubnum{}\log n)$ \\
        \bottomrule
    \end{tabular}
    %\end{tabularx}
    \caption{
        Summary of the runtime and space usage of several nearest neighbor data structures.
        Here $n$ represents the size of the dataset.
    }
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods for faster nearest neighbors}

Every metric space embeds into an ultrametric space with distortion $O(1/\sqrt \epsilon)$ \citep{abraham2007embedding}.
\cite{charikar1998approximating} creates tree metrics from finite metric spaces;
uses this result to create approximation algorithms for several NP-hard problems.

\cite{bartal2003metric} discusses embeddings into Hilbert spaces.
\cite{krauthgamer2004measured} shows that any finite metric with $n$ data points can be embedded into Hilbert space with $O(\sqrt{\log\doubnum\log n})$ distortion.
\cite{neiman2016low} embeds finite metrics into $L_\infty$.
\cite{abraham2014volume} shows finite embeddings that preserve both distance and volume.
\cite{bartal2015impossibility} shows that $L_p, p>2$ spaces cannot be embedded.

The most recent work is the Fast Library for Approximate Nearest Neighbor (FLANN) \cite{muja2014scalable}.
FLANN uses the randomized kd-forest and the priority search $k$-means tree.

In Euclidean space, \cite{andoni2014beyond} proposes a new data structure that provably does better than LHS lower bounds.
Good references inside.

\cite{malkov2014approximate} proposes a data structure for approximate nearest neighbor search using the Daulanay graph (dual graph of the Veronoi diagram).
They provide no approximation guarantees.
Seems to have a lot of references on similar approaches though.

\cite{abraham2015approximate} show that approximate nearest neighbor search can be done efficiently when the underlying metric can be represented as a planar graph.
This is interesting because these graphs need not have bounded doubling dimension or be embeddable in L2 space.
I should read this paper closer for the exact results.

\cite{borodin1999lower,barkol2000tighter,panigrahy2008geometric} provides lower bounds and a review of time-space tradeoffs.
\cite{andoni2008hardness} provides hardness results for $L_\infty$.
\cite{andoni2016lower} provides hardness results in a much more general setting for $L_p$ metrics.
It also has a LOT of good references for LSH results positive and negative.
\cite{p2011unifying} provides a unifying framework for determining lower bounds.

\cite{gionis1999similarity} introduces the idea of LHS.
LHS in metric spaces \cite{tellez2010locality,novak2010locality} provide no theoretical guarantees and non-metric spaces \cite{mu2010non}.

Survey of LHS \cite{wang2014hashing} \cite{wang2016learning}

\cite{zezula2006similarity} is a book on metric space similarity search.

\cite{ram2009linear} and \cite{curtin2015plug} improve the runtime bounds of cover trees.
\cite{curtin2013tree} provides a generic framework for solving distance problems.
\cite{clarkson2006nearest} reviews metric space properties and data structures.
\cite{czumaj2010sublinear} is a survey of sublinear time algorithms in metric spaces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Review of Metric Spaces in Machine Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examples of Metric Spaces}

\begin{example}
    Any subset of a metric space is also a metric space.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The dimension of metric spaces}

This section introduces three measures of the ``dimension'' of a metric space:
the expansion constant $\krnum$, the doubling constant $\doubnum$, and the aspect ratio $\aspect{}$.
Previous analysis of the cover tree relied only on the expansion constant,
but the analysis in this thesis will instead use the doubling constant and aspect ratio.
In this section, we will formally define these constants and present their basic properties.
In particular, we will see that the doubling constant and aspect ratio are more robust measures of size than the expansion constant.

Throughout this section we will work with the metric space $(\set X,d)$.
The set $\set X$ may be either finite or infinite.
We let $B(x,\delta)$ denote the \emph{ball} centered around $x$ of radius $\delta$. 
That is,
\begin{equation}
    B(x,\delta) = \{ x' : x'\in\set X, \dist{x}{x'} \le \delta \}.
\end{equation}
Let $\mu : \{\set X\} \to \R^+$ be a function that gives an abstract notion of the \emph{volume} of its first argument.%
\footnote{
    Formally, $\mu$ is a measure and $\{\set X\}$ is a $\sigma$-algebra on $\set X$.
    Since we are primarily interested in the properties of finite sets,
    we will not need to formally use measure theory.
}
For example, if $\set X=\R^n$, then $\mu$ could be the standard Euclidean volume,
and $\mu B(x,\delta) = O(\delta^n)$.
If $\set X$ is a finite set, then $\mu$ could be the counting measure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The expansion dimension}

Then the \emph{expansion constant} is defined as
\begin{equation}
    \krnum = \max_{x\in\set X, \delta\in\R^+} \frac{\mu B(x,2\delta)}{\mu B(x,\delta)}
    ,
\end{equation}
and the \emph{expansion dimension} is defined as
\begin{equation}
    \krdim\set X = \log_2 c_\set X
    .
\end{equation}
The expansion dimension was introduced by \citet{karger2002finding}.
Intuitively, it is a reasonable notion of dimensionality because it agrees

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The doubling dimension}

A \emph{$\delta$-covering} of a metric space $\set X$ is a set $\{x_1,x_2,...,x_n\} \subseteq \set X$ such that for all $x\in\set X$, there exists an $x_i$ such that $\dist{x}{x_i} < \delta$.
The \emph{$\delta$-covering number} $N_\delta(\set X)$ is the cardinality of the smallest $\delta$-covering.
%The log of the covering number $\log N_\delta(\set X)$ is called the \emph{metric entropy} of $\set X$.
A \emph{$\delta$-packing} of a metric space $\set X$ is a set $\{x_1,x_2,...,x_M\} \subseteq \set X$ such that $\dist{x_i}{x_j} > \delta$ for all distinct $i,j\in[M]$.
The \emph{$\delta$-packing number} $M_\delta (\set X)$ is the cardinality of the largest $\delta$-packing.
The following standard lemma shows that packing and covering numbers are equivalent up to a factor of 2.

\begin{lemma}
    \label{lemma:coverpacking}
    For any metric space $\set X$ and any $\delta>0$,
    $M_{2\delta}(\set X) \le N_\delta(\set X) \le M_{\delta}(\set X)$.
    %\begin{equation}
        %M_{2\delta}(\set X) \le N_\delta(\set X) \le M_{\delta}(\set X)
        %.
    %\end{equation}
\end{lemma}

\begin{proof}
    To prove the first inequality, let $P$ be a $2\delta$-packing and $C$ be a $\delta$-cover of $\set X$.
    For every point $p\in P$, there must exist a $c\in C$ such that $\dist{p}{c}\le\delta$.
    No other $p'\in P$ can also satisfy $\dist{p'}{c}\le\delta$, because then by the triangle inequality
    \begin{equation}
        \dist{p'}{p} \le \dist{p'}{c}+\dist{p}{c} \le 2\delta
        ,
    \end{equation}
    which would contradict that $P$ is a $2\delta$-packing.
    In other words, for each $c\in C$, there is at most one $p\in P$.
    So $N_\delta \ge |C| \ge |P| \ge M_{2\delta}$.

    To prove the second inequality, let $\set X'\subseteq \set X$ be a maximal $\delta$-packing.
    Then there does not exist an $x\in\set X$ such that for all $x'\in\set X'$, 
    $\dist{x}{x'} > \delta$.
    (Otherwise, $\set X' \cup \{x\}$ would be a packing larger than $\set X'$.)
    Hence, $\set X'$ is also a $\delta$-cover,
    and the smallest $\delta$-cover can be no larger.
\end{proof}

%\cite{nickl2007bracketing} uses metric entropy to prove a version of the central limit theorem.
%
%\begin{example}
%\end{example}

%\begin{definition}
    %The Minkowski dimension of a metric space is defined to be
    %\begin{equation}
        %\minkdim \set X = \lim_{\delta\to0} \frac{\log N_\delta(\set X)}{\log 1/\delta}
        %.
    %\end{equation}
%\end{definition}
%
%\begin{example}
    %Let $\set X$ be a finite metric space.
    %Then $\minkdim \set X = 0$.
%\end{example}

\begin{definition}
    The \emph{doubling number} of a metric space is
    \begin{equation}
        \doubnum(\set X) = \max_{x\in\set X, \radius\in\R^+} N_\radius(B_{\set X}(x,2\radius))
        .
    \end{equation}
    The \emph{doubling dimension} of a metric space $\set X$ is the log of the doubling number.
    Specifically,
    \begin{equation}
        \doubdim \set X = \log \doubnum\set X = \max_{x\in\set X, \radius\in\R^+} \log N_\radius(B_{\set X}(x,2\radius))
        .
    \end{equation}
\end{definition}
\cite{gupta2003bounded}

\begin{lemma}[\cite{krauthgamer2004navigating},\cite{gupta2003bounded}]
    Every finite metric $(\set X,d)$ satisfies
    $\doubnum \le \krnum^4$.
    %\begin{equation}
        %\doubdim\set X \le 4\cdot\krdim\set X
        %.
    %\end{equation}
\end{lemma}
\begin{proof}
    Fix some ball $B(x,\delta)$.
    We will show that $B(x,\delta)$ can be covered by $\krnum^4$ balls of radius $\delta$.
    Let $Y$ be a $\delta$-cover of $B(x,2\delta)$.
    Then,
    \begin{equation}
        B(x,2\delta) 
        \subseteq 
        \bigcup\limits_{y\in Y} B(y,\delta) 
        \subseteq
        B(x,4\delta)
        %~~~~~
        %\text{and}
        %~~~~~
    \end{equation}
    Also, for every $y\in Y$,
    \begin{equation}
        |B(x,4\delta)| 
        \le 
        |B(y,8\delta)| 
        \le 
        \krnum^4 |B(y,\delta/2)|
        .
    \end{equation}
    We also have that $|B(y,\delta/2)|=1$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{The aspect ratio}

We will use the aspect ratio to bound the depth of the cover tree.
Unlike the covering and expansion constants,
the aspect ratio should not be thought of as a measure of dimension.
Instead, it should be thought of as a measure of how evenly spread the points in a metric space are.
The \emph{diameter} of $\set X$ is the maximum distance between any two points.
In notation,
\begin{equation}
    \diam {\set X} = \max_{x_1,x_2\in\set X} \dist{x_1}{x_2}
    .
\end{equation}
The \emph{codiameter} of $\set X$ is the minimum distance between any two points.
In notation,
\begin{equation}
    \codiam {\set X} = \min_{x_1 \ne x_2\in\set X} \dist{x_1}{x_2}
    .
\end{equation}
The \emph{aspect ratio} of $\set X$, denoted by $\aspect{\set X}$, 
is the ratio of the diameter to the dispersion.
Sets with small aspect ratio are called ``fat,''
and sets with large aspect ratio are called ``skinny.''

There is little inherent relationship between the aspect ratio and the inherent dimension of a space.
We emphasize this point with three examples.

\begin{example}
    Let $\set Y=\{y_1,...,y_n\}$ be the discrete metric space of size $n$;
    that is,
    \begin{equation}
        \dist{y_i}{y_j}=
        \begin{cases}
            0 & i = j \\
            1 & \text{otherwise}
        \end{cases}
        .
    \end{equation}
    Then the aspect ratio of $\set Y$ is 1 (i.e.\ as small as possible),
    and both the expansion and doubling constants of $\set Y$ are $n-1$ (i.e. arbitrarily large).
\end{example}

\begin{example}
    Now construct the set $\set Y'=\{y'_1, y'_2, y'_3\}$.
    Let $r>2$, and define the distance function to be
    \begin{equation}
        d(y'_i,y'_j) =
        \begin{cases}
            0 & i=j \\
            1 & i=1, j=2 \\
            r & i=1, j=3 \\
            r & i=2, j=3 \\
        \end{cases}
        .
    \end{equation}
    Then the aspect ratio is $r$ (i.e.\ arbitrarily large),
    but the expansion constant is always 2
    and the doubling constant always 1.
\end{example}

\begin{example}
    Let $Y=\{\frac 1 2, \frac 1 4, ..., \frac 1 {2^n}\}$,
    and $\dist{y_1}{y_2}=|y_1-y_2|$.
    Then the aspect ratio is $2^{n-1}$ (i.e.\ arbitrarily large),
    the expansion constant is $n-1$ (i.e.\ arbitrarily large),
    and the doubling constant is 1.
\end{example}

\begin{lemma}[\cite{krauthgamer2004navigating}]
    For any metric space $\set X$, we have that
    $
        |\set X| \le \aspect{\set X}^{O(\doubdim{\set X})}.
    $
\end{lemma}
%\begin{proof}
    %Assume wlog that the separation distance is 1
    %(we can rescale all the distances to ensure this).
    %Then the diameter of $\set X$ is $\aspect{\set X}$.
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The growth rate of the aspect ratio}

The aspect ratio and the expansion number share the unattractive property that adding a single point to a dataset can increase these quantities arbitrarily.
Under mild assumptions, however, we can show that this is unlikely to happen.
Specifically, let $\set X$ be a metric space, 
and let $X=\{x_1,...,x_n\}\subset\set X$ be a sample of $n$ i.i.d.\ points from a distribution $\distribution D$ over $\set X$.
Our goal is to show that the aspect ratio is polynomial in $n$.
We will later show that the log of the aspect ratio bounds the depth of the cover tree (see Theorem \ref{}),
and so the depth of the cover tree will be logarithmic in $n$.

We begin by bounding the diameter of $X$.
%In general, it does not make sense to take the expectation with respect to $\distribution D$.
We say the distribution $\distribution D$ has \emph{finite expected distance} if there exists an $\bar x\in\set X$ such that $\mu=\E\dist{\bar x}{x_i}$ is finite.
Note that this is a mild condition satisfied by most standard distributions on Euclidean space.
For example, the uniform, Gaussian, exponential, Weibull, and Pareto distributions all have finite expected distance.
Notice that the Weibull and Pareto distributions have heavy tails.
One easy to describe distribution which does not satisfy this property is the Cauchy distribution 
(the distribution of the reciprocal of a Gaussian random variable).
The following lemma shows that this is a sufficient condition for the diameter to grow polynomial in $n$.

\begin{lemma}
    \label{lemma:Ediam}
    %Let $\set X$ be a metric space.
    %Let $X=\{x_1,...,x_n\}\subset\set X$ be a sample of $n$ i.i.d.\ points satisfying the following property:
    %There exists a $\bar x\in\set X$ such that $\mu=\E\dist{\bar x}{x_i}$ is finite.
    %Then, 
    %\begin{equation}
        %\E\diam{X} \le 2n\mu
    %\end{equation}
    Let $\set X$ and $X$ be defined as above,
    and assume that $\distribution D$ has finite expected distance.
    Then, $\E\diam{X} \le 2n\mu$.
\end{lemma}

\begin{proof}
    By the triangle inequality, we have that
    \begin{equation*}
        \diam{X}
        = 
        \max_{i,j} \dist{x_i}{x_j}
        \le
        \max_{i,j} (\dist{\bar x}{x_i} + \dist{\bar x}{x_j})
        %\\ &=
        %\max_i \dist{\bar x}{x_i} + \max_j\dist{\bar x}{x_j}
        =
        2\max_i \dist{\bar x}{x_i}
        .
    \end{equation*}
    We now remove the max using the union bound.
    This gives
    \begin{equation*}
        \prob{\diam{X} > t}
        \le
        \prob{\max_i 2\dist{\bar x}{x_i} > t}
        \le
        \sum_{i=1}^n\prob{2\dist{\bar x}{x_i} > t}
        \label{eq:Ediamub}
        =
        n\prob{2\dist{\bar x}{x_1} > t}
        %\label{eq:Ediamiid}
        .
    \end{equation*}
    %Equation \eqref{eq:Ediamub} follows from the union bound, 
    %and \eqref{eq:Ediamiid} follows because the $x_i$s are i.i.d.
    The rightmost equality follows because the $x_i$s are i.i.d.
    Finally, since the distances are always nonnegative, we have that
    \begin{equation*}
        \E\diam{X} 
        = 
        \int_0^\infty \prob{\diam{X} > t} \dd t
        \le
        \int_0^\infty n\prob{2\dist{\bar x}{x_1} > t} \dd t
        =
        %n\int_0^\infty \prob{2\dist{\bar x}{x_1} > t} \dd t
        %\\ &=
        2n \E\dist{\bar x}{x_1}
        %\label{eq:Ediamproof}
        .
    \end{equation*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next we show that the codiameter cannot shrink too fast.
We say that the distribution $\distribution D$ has \emph{$B$-bounded density} if
for all $x\in\set X$, the density of $\dist{x}{x_i}$ is bounded by $B$.
An immediate consequence is that 
\begin{equation}
    \max_{x\in\set X} \prob{\dist{x}{x_i} \le t} \le Bt
    .
\end{equation}
Again, all the standard distributions in Euclidean space satisfy this condition.
The following lemma shows that this condition is sufficient to lower bound the codiameter.

\begin{lemma}
    \label{lemma:Ecodiam}
    Let $\set X$ and $X$ be defined as above,
    and assume that $\distribution D$ has $B$-bounded density.
    Then, $\E\codiam{X} \ge (2n^2B)^{-1}$.
\end{lemma}
\begin{proof}
    We have that
    \begin{align}
        \prob{\codiam{X} \le t}
        &=
        \prob{\min \{ \dist{x_i}{x_j} : i\in\{1,...,n\}, j\in\{i+1,...,n\} \} \le t}
        %\prob{\min_{i\ne j} \dist{x_i}{x_j} \le t}
        \\ &\le 
        \sum_{i=1}^n\sum_{j=i+1}^n \prob{\dist{x_i}{x_j} \le t}
        \label{eq:lemcodiam1}
        %\\ &\le
        %\sum_{i=1}^n n \max_{x\in X} \prob{\dist{x_i}{x} \le t}
        \\ & \le
        n^2 \max_{x\in X} \prob{\dist{x_1}{x} \le t}
        \label{eq:lemcodiam2}
        \\ & \le 
        n^2 B t
        \label{eq:lemcodiam3}
    \end{align}
    Equation \eqref{eq:lemcodiam1} follows from the union bound,
    \eqref{eq:lemcodiam2} from the fact that the $x_i$s are i.i.d.,
    and \eqref{eq:lemcodiam3} from the definition of $B$-bounded.
    We further have that since probabilities are always no greater than 1,
    \begin{equation}
        \prob{\codiam{X}\le t} \le \min\{1,n^2Bt\}
        .
    \end{equation}
    Finally, since $\codiam{X}$ is nonnegative, we have that 
    \begin{align}
        \E \codiam{X}
        &=
        \int_0^\infty (1-\prob{\codiam{X} \le t}) \dd t
        %\\ & =
        %\int_0^\infty \prob{\codiam{X} > t} dt
        \\ & \ge
        \int_0^\infty (1-\min\{1,n^2 Bt\}) \dd t
        \\ & = 
        \int_0^{(n^2B)^{-1}} (1 - n^2 Bt) \dd t
        \\ & =
        %\frac{1}{n^2B} - \frac{\left(\frac{1}{n^2B}\right)^3}{2}
        %\\ & \ge
        \frac{1}{2n^2B}
        \label{eq:Ecodiamproof}
        .
    \end{align}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

An immediate consequence of Lemmas \ref{lemma:Ediam} and \ref{lemma:Ecodiam} is the following bound on the aspect ratio.

\begin{lemma}
    \label{lemma:Easpect}
    Let $\set X$ and $X$ be defined as above.
    Assume that $\distribution D$ has finite expected distance and $B$-bounded density.
    Then, $\E\aspect{X} \le2B\mu n^3$. 
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Simplified Cover Tree}

A \emph{simplified cover tree} is a data structure for efficiently representing a finite metric space.
Each node in the tree corresponds to exactly one point in the space,
and the tree obeys the following three invariants.
\begin{enumerate}
    \item \emph{Leveling invariant}.%
    \footnote{
        The original version of the simplified cover tree \citep{izbicki2015faster} had a slightly different leveling invariant.
        The original version required that the level of a child be exactly one less than the level of the parent.
        The version used in this thesis is strictly more general and facilitates the runtime analysis of the tree.
    }
    Every node $\p$ has an associated integer $\level\p$.
    For all nodes $\q\in\children\p$, $\level\q < \level\p$.
    Furthermore, $\level p$ cannot be decreased without violating the covering invariant.
    %(Note that $\level{p}$ can be negative or arbitrarily large.
    %In particular, it is not the depth of $p$ in the tree.)
    \item \emph{Covering invariant}.
    Every node $\p$ has an associated real number $\covdist\p=2^{\level\p}$.
    For all nodes $\q\in\children\p$, $\dist \p \q \le \covdist\p$.%
    \footnote{
        As in the original cover tree, practical performance is improved on most datasets by redefining $\covdist p = 1.3 ^ {\level p}$.
        All of our experiments use this modified definition.
    }
    \item \emph{Separating invariant}.
    For all nodes $\q_1,\q_2\in\children\p$, $\dist {\q_1} {\q_2} \ge \covdist\p/2$.
\end{enumerate}
It will be useful to define the function
\begin{equation}
\maxdist p = \argmax_{q\in\descendants{p}} \dist p q
.
\end{equation}
In words, $\maxdist\p$ is the greatest distance from $p$ to any of its descendants.
This value is upper bounded by $2^{\level{p}+1}$, 
and its exact value can be cached within the data structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Properties of the Simplified Cover Tree}

Before we present algorithms for manipulating the cover tree, 
we present two lemmas that bound the shape of the tree.
These lemmas are a direct consequence of the cover tree's invariants and motivate the invariants' selection.
This section can be safely skipped by the reader not interested in the details of the tree's runtime analysis.

%%%%%%%%%%%%%%%%%%%%

%\begin{lemma}
    %\label{lamma:diam}
    %%For every node $p$ in a cover tree, $\covdist{p}\le\diam{X}/2$.
    %For every node $p$ in a cover tree, $\maxdist p \ge \covdist p/2$.
%\end{lemma}
%
%\begin{proof}
    %We have that $\maxdist p$ must be greater than or equal to 
%\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
    \label{lemma:height}
    Let $p$ be any non-leaf node in a cover tree.
    Denote by $\height{p}$ the number of edges between $p$ and its most distant leaf.
    We have that $\height{p} \le \log_2\aspect{X}$.
\end{lemma}

\begin{proof}
    We will show that the following chain of inequalities holds:
    \begin{equation}
        \aspect{X} 
        = \frac{\diam{X}}{\codiam{X}} 
        \overset{(1)}\ge \frac{\diam{X}}{\covdist{p}/2^{\height p-1}} 
        \overset{(2)}\ge \frac{\diam{X}}{\diam{X}/2^{\height p}} 
        = 2^{\height p}
        .
    \end{equation}
    Solving for $\height p$ then gives the desired result.
    For inequality $(1)$, consider a point $q$ that is exactly $i$ edges away from $p$.
    By the covering invariant, 
    \begin{equation}
        \dist{q}{\parent{q}} 
        \le 
        \covdist{\parent{q}}
        \le 
        \covdist{\parent{\parent{q}}}/2
        \le
        \covdist{p}/2^{i-1}
        .
    \end{equation}
    In particular, if $\ell$ is a leaf node $\height p$ edges away from $p$,
    then $\codiam{X}\le\dist{p}{\ell}\le2^{\height p-1}$.
    For inequality (2), observe that there must exist a child $q$ of $p$ such $\dist{p}{q} \ge \covdist{p}/2$.
    Otherwise, $\level p$ could be reduced by one, which would violate the leveling invariant.
    Therefore, $\diam{X} \ge \dist{p}{q} \ge \covdist{p}/2$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
    \label{lemma:children}
    For every node $p$ in a cover tree, we have that
    $|\children\p| \le \doubnum^2$.
\end{lemma}

\begin{proof}
    To simplify notation, we let $\delta=\covdist{p}$.
    %The covering invariant ensures that all the children of $p$ are contained in $B(p,\delta)$,
    The separating invariant ensures that the children of $p$ form a $\delta/2$-packing of $B(p,\delta)$.
    So by the definition of $M_{\delta/2}$ and Lemma \ref{lemma:coverpacking}, we have
    \begin{equation}
        |\children{p}| 
        \le M_{\delta/2}(B(p,\delta)) 
        \le N_{\delta/4}(B(p,\delta)) 
        %\le \doubnum N_{\delta/2}(B(p,\delta)) 
        %\le \doubnum^2
        .
    \end{equation}
    We now show that $N_{\delta/4}(B(p,\delta))\le\doubnum$.
    Let $Y$ be a $\delta/2$-covering of $B(p,\delta)$.
    For each $y_i\in Y$, let $Y_i$ be a minimum $\delta/4$-covering of $B(y_i,\delta/2)$.
    The union of the $Y_i$s is a $\delta/4$-covering of $B(p,\delta)$.
    There are at most $\doubnum$ $Y_i$s, and each $Y_i$ contains at most $\doubnum$ elements.
    So their union contains at most $\doubnum^2$ elements.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Algorithms}

\begin{algorithm}[H]
%\caption{Simplified cover tree insertion}
\label{alg:insert}

    \vspace{0.1in}
{\bfseries function} \ctinsert(cover tree $p$, data point $x$)

returns a new cover tree $p'$ that contains $x$ and all the points in $p$

\begin{algorithmic}[1]
    \If {$\dist p x > \covdist p$}
        \State create a new node $p'$ from data point $x$
        \State set $\level{p'}$ to  $\ceil{\log_2 d(p,p')}$
        \State set $\children{p'}$ to $\{p\}$
        \State\Return $p'$
    \Else
        \State let $q = \argmin_{q\in\children{p}} \dist{q}{x}$.
        \If {$\dist q x > \covdist p/2$}
            \State let $p' = p$ with $x$ added as a child
            \State\Return $p'$ 
        \Else
            \State let $q' = \ctinsert(q,x)$
            \State let $p' = p$ with child $q$ replaced by $q'$
            %\State set $\level{p'}$ to $\ceil{\log_2\argmax_{q\in\children{p'}} d(p',q)}$
            \State \Return $p'$
        \EndIf
        %\For {$q \in \children{p}$ sorted in ascending order by $\dist{q}{x}$}
            %\If {$\dist q x \le \covdist p/2$}
                %\State let $q' = \ctinsert(q,x)$
                %\State let $p' = p$ with child $q$ replaced by $q'$
                %\State set $\level{p'}$ to $\ceil{\log_2\argmax_{q\in\children{p'}} d(p',q)}$
                %\State \Return $p'$
            %\EndIf
        %\EndFor
        %\State let $p' = p$ with $x$ added as a child
        %\State\Return $p'$ 
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{theorem}
If $p$ is a valid cover tree, then $\ctinsert(p,x)$ is a valid cover tree.
\end{theorem}

\begin{proof}
    There are three cases, and we consider each in turn.
    When the first if statement is satisfied, 
    the covering invariant prevents $x$ from being added as a child of $p$.
    So we create a new root node $p'$ above $p$.
    The level of $p'$ is set in line 3 as low as possible to satisfy both the leveling and covering invariants.
    (The level of $p'$ may in general be arbitrarily higher than the level of $p$.)
    The separating invariant is satisfied because $p'$ has only one child.
    Note that every time this if statement triggers,
    the $\mkfunction{covdist}$ of the root node increases by a factor of two.
    Thus when we insert a sequence of points with bounded diameter, 
    eventually the $\mkfunction{covdist}$ will be large enough that this if statement will no longer trigger and points will be forced to be inserted as a children.

    When the first if statement is not satisfied, 
    $x$ must be inserted underneath $p$.
    The if statement on line 8 checks if $x$ can be added directly as a child to $p$.
    Adding new children does not affect either the leveling or covering invariants,
    and the condition ensures that the separating invariant remains satisfied.

    If $x$ cannot be added as a child, 
    then it is added to the subtree rooted at $q$ in the else clause beginning on line 11.
    Recall that the leveling invariant allows the children of $p$ to have arbitrarily small level.
    So there are two cases we must consider.
    (Case 1:) When $\level{q}=\level{p}-1$, 
    the recursive call to $\ctinsert$ on line 9 will return a cover tree rooted at $q$.
    (The if statement on line 1 will not be triggered in the recursion).
    So the leveling, covering, and separating invariants of $p$ all remain satisfied.
    (Case 2:) When $\level{q}<\level{p}-1$, 
    the recursive call to $\ctinsert$ will return a cover tree with a root node at $x$ instead of $q$.
    (The if statement on line 1 will be triggered in the recursion).
    %The leveling invariant of $p$ may be affected if $q$ was the child of maximum distance to $p$.
    %In particular, $\level{p}$ may need to decrease in order to satisfy the leveling invariant,
    %and line 11 accounts for this possibility.
    The covering invariant of $p$ remains satisfied due to the if statement on line 1.
    %The separating invariant is subtle and depends on the order that children are considered in the for loop.
    %In particular, let $r\le q$ be a 
\end{proof}

\begin{theorem}
    \label{theorem:ctinsert1}
    The runtime of $\ctinsert$ is $O(\doubnum^2\log\aspect{X})$.
\end{theorem}
\begin{proof}
    If $\dist{p}{x} > \covdist{p}$, then the running time is constant.
    Otherwise, the algorithm loops over all the children of $p$ and recurses on at most one child.
    There are at most $\doubnum^2$ children (Lemma \ref{lemma:children}),
    and the height of the tree is at most $\log_2\aspect{X}$ (Lemma \ref{lemma:height}).
\end{proof}

\begin{theorem}
    \label{theorem:ctinsert2}
    When the data is drawn i.i.d.\ from a well behaved distribution,
    then the expected runtime of $\ctinsert$ is $O(\doubnum^2\log n)$.
\end{theorem}
\begin{proof}
    Apply Lemma \ref{lemma:Easpect} to Theorem \ref{theorem:ctinsert1}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Merging cover trees}
    \label{alg:merge}
    \vspace{0.1in}
    {\bfseries function} \ctmerge(cover tree $p$, set of cover trees $R$)

\begin{algorithmic}[1]
    \For {$r \in R$}
        \If {$\max_{q\in\children{p}} \dist{r}{q} > \covdist{p}/2$}
            \State add $r$ as a child of $p$
            \State remove $r$ from $R$
        \EndIf
    \EndFor
    \For {$q \in \children{p}$}
        \State let $R_q = \{ r : r\in R, \dist{r}{q} \le \covdist{p}/2\}$
        \State let $q' = \ctmerge(q,R_q)$
        \State replace $q$ with $q'$ as a child of $p$
    \EndFor
\end{algorithmic}
\end{algorithm}

    %\State let $\ell = \max_{r\in R} \level r$ 
    %\If {$\level p > \ell$}
        %\State let $R_{near} = \{ r : r\in R, \min_{q\in\children{p}}\dist{q}{r} \le \covdist p/2\}$.
        %\State let $R_{far} = R-R_{near}$
    %\Else
        %\State let $R=\{\}$
        %\For {$r\in R$}
            %\If {$\level{r}=\ell$}
                %\State create node $r'$ with same data point as $r$ but level $-\infty$
                %\State $R' \leftarrow R' \cup \{r'\} \cup \children{r}$
            %\Else
                %\State $R' \leftarrow R' \cup \{r\}$
            %\EndIf
        %\EndFor
        %\State $\ctmerge(p,R')$
    %\EndIf
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(set of cover trees $P$, query  point $x$, nearest neighbor so far $y$, allowable tolerance $\epsilon$)

\begin{algorithmic}[1]
    \If {$P = \{\}$}
        \State\Return $y$
    \Else
        \For {$p\in P$}
            \If {$d(p,x) < d(y,x)$}
                \State $y \leftarrow p$
            \EndIf
        \EndFor
        \State $P' \leftarrow \{\}$
        \For {$p\in P$}
            \For {$q \in \children p$}
                \If {$\dist{x}{q} \le \dist{x}{y}/\epsilon + \maxdist{q}$}
                    \State $P' \leftarrow P'\cup\{q\}$
                \EndIf
            \EndFor
        \EndFor
        %\State let $P' = \{ q : q\in\children{p}, p\in P, \dist{x}{y} > \dist{x}{q}/\epsilon - \maxdist{q} \}$
        %\State let $P' = \{ q : q\in\children{p}, p\in P, \dist{x}{q} \le \epsilon\cdot\dist{x}{y}+\maxdist{q}\}$
        \State\Return $\findnn(P',x,y,\epsilon)$
    \EndIf
\end{algorithmic}
\end{algorithm}

\begin{definition}
    Let $\set X$ be a metric space, $X\subset\set X$, and $x$ be in $\set X$ but not $X$.
    Then the nearest neighbor $y^*\in X$ is defined as $y^*=\argmin_{y\in X} \dist{x}{y}$.
    An $\epsilon$-nearest neighbor (for $\epsilon\ge1$) is a point $\hat y$ satisfying $\dist{x}{\hat y} \le \epsilon \cdot\dist{x}{y^*}$.
\end{definition}

\begin{theorem}
    The return value of $\findnn(\{p\},x,p,\epsilon)$ is an $\epsilon$-approximate nearest neighbor of $x$.
\end{theorem}
\begin{proof}
    The algorithm maintains the invariant that either $y$ is an $\epsilon$-ann,
    or $P$ contains a tree that contains an $\epsilon$-ann.
    Since the algorithm only terminates when $P$ is empty,
    the invariant ensures that the returned value is an $\epsilon$-ann.

    If $y$ is already an $\epsilon$-ann,
    then the algorithm must return an $\epsilon$-ann because it will only ever update $y$ to be closer to $x$.
    Assume $y$ is not an $\epsilon$-ann.
    Then if the root of some subtree in $P$ is an $\epsilon$-ann,
    the for loop on lines 4-6 will update $y$ to be an $\epsilon$-ann.
    Otherwise, there must exist an $\epsilon$-ann in some child $q$ of some subtree $p\in P$.
    Let $\hat y$ denote the $\epsilon$-ann.
    We have that
    \begin{equation}
        \epsilon \cdot \dist{x}{y^*}
        \le \dist{x}{\hat y} 
        \le \dist{x}{y} 
        .
    \end{equation}
    Then by the triangle inequality, we have
    \begin{equation}
        \label{eq:findnn_condition}
        \dist{x}{q} 
        \le \dist{x}{\hat y} + \dist{\hat y}{q}
        \le \dist{x}{y}/\epsilon + \maxdist{q}
        .
    \end{equation}
    The if statement in line 10 ensures that all nodes satisfying condition \eqref{eq:findnn_condition} will be added to $P_i$,
    maintaining the invariant.
\end{proof}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(cover tree $p$, query  point $x$, tolerance $\epsilon$)

\begin{algorithmic}[1]
    %\State let $i=0$, $P_i = \{p\}$, $y=p$
    \State $y \leftarrow p$
    \State $i\leftarrow0$
    \State $P_0 \leftarrow \{p\}$
    \While {$P_i \ne \{\}$}
        \For {$p\in P_{i}$}
            \If {$\dist{p}{x} < \dist{y}{x}$}
                \State $y \leftarrow p$
            \EndIf
        \EndFor
        \State $P_{i+1}\leftarrow \{\}$
        \For {$p\in P_{i}$}
            %\If {$\dist{p}{x} < \dist{y}{x}$}
                %\State $y \leftarrow p$
            %\EndIf
            \For {$q\in\children{p}$}
                \If {$\dist{x}{q} \le \dist{x}{y}/\epsilon+\maxdist{q}$}
                    \State $P_i \leftarrow P_i\cup\{q\}$
                \EndIf
            \EndFor
        \EndFor
        \State $i\leftarrow i+1$
    \EndWhile 
    \State \Return y
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    The runtime of $\findnn$ with $\epsilon \ge 2$ is $O(\doubnum^2\log\aspect{})$.
\end{theorem}
\begin{proof}
    Since the algorithm contains three nested loops, 
    the total runtime is bounded by the product of the maximum iterations of each loop.
    The innermost for loop (line 9) executes once per child,
    which by Lemma \ref{lemma:children} is $O(\doubnum^2)$.
    The outermost while loop (line 4) executes at most once per level of the tree.
    To see this, notice that $P_i$ contains only nodes that are $i$ edges away from $p$,
    and so $P_{\height p+1}$ is guaranteed to be the empty set.
    By Lemma \ref{lemma:height}, the height of $p$ is $O(\log\aspect{})$.
    The middle for loop (line 6) executes once for each node in $P_i$.
    To bound the size of $P_i$, first notice that by the global separating invariant,
    the distance between each $q_1$ and $q_2$ in $P_i$ must be at least $2^i$. 
    By our choice of $\epsilon$, we have that all of the $q$ in $P_i$ satisfy
    \begin{equation}
        \dist{x}{q} 
        \le \dist{x}{y}/\epsilon + \maxdist{q}
        \le \dist{x}{q}/2 + 2^i
        \le 2^{i+1}
        .
    \end{equation}
    Let $\delta=2^i$.
    Then $P_i$ is a $\delta$-packing of $B(x,2\delta)$.
    \begin{equation}
        |P_i|
        \le M_\delta(B(x,2\delta))
        \le N_\delta/2(B(x,2\delta))
        \le \doubnum^2
        .
    \end{equation}
\end{proof}

\begin{theorem}
    The runtime of $\findnn$ with $\epsilon=1$ 
    (i.e. $\findnn$ must return the exact nearest neighbor),
    is $O(\doubnum^2 ...)$.
\end{theorem}

%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
    %\caption{Simplified cover tree nearest neighbor query}
    \label{alg:query}

    \vspace{0.1in}
    {\bfseries function} \findnn(cover tree $p$, query  point $x$, nearest neighbor so far $y$)

\begin{algorithmic}[1]
    \If {$d(p,x) < d(y,x)$}
        \State $y \leftarrow p$
    \EndIf
    %\For {each child $q$ of $p$ sorted by distance to $x$}
    \For {$q\in\children p$ sorted in ascending order by distance to $x$}
        \If {$d(x,y) > d(x,q) - \maxdist{q}$} 
            \State $y \leftarrow \findnn(q,x,y)$
        \EndIf
    \EndFor
    \State\Return $y$
\end{algorithmic}
\end{algorithm}

\begin{theorem}
    $\findnn(p,x,\epsilon)$ returns an $\epsilon$-approximate nearest neighbor of $x$.
\end{theorem}

\begin{theorem}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Analysis}

\end{document}
