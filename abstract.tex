\begin{abstract}

    \noindent
    This thesis improves the scalability of machine learning by studying \defn{mergeable learning algorithms}.
    In a mergeable algorithm,
    many processors independently solve the learning problem on small subsets of the data.
    Then a master processor merges the solutions together with only a single round of communication.
    %These mergeable algorithms have the following embarrassingly parallel learning procedure:
    %divide the dataset onto many different machines;
    %each machine independently learns a local model on its data;
    %finally a master machine merges the local models together.
    %Given a set of $m$ processors each with $n$ data points,
    %first train a local model $\what_i$ on the local data;
    %then merge the $\what_1,...,\what_m$ models together to create the final model.
    Mergeable algorithms are popular because they are fast, easy to implement, and have strong privacy guarantees.

    Our first contribution is a novel fast cross validation procedure suitable for any mergeable algorithm.
    %Standard cross validation has runtime linear in the number of folds $k$,
    %and so cannot be used on large scale problems.
    This fast cross validation procedure has a constant runtime independent of the number of folds and can be implemented on distributed systems. 
    This procedure is also widely applicable.
    We show that $\nummerge$ recently proposed learning algorithms are mergeable and therefore fit our cross validation framework. 
    These learning algorithms come from many subfields of machine learning,
    including density estimation, regularized loss minimization, dimensionality reduction, submodular optimization, variational inference, and markov chain monte carlo.
    %For many of these models, no prior fast cross validation procedure was known.

    We also provide two new mergeable learning algorithms.
    In the context of regularized loss minimization,
    existing merge procedures either have high bias or slow runtimes.
    We introduce the \defn{optimal weighted average} (OWA) merge procedure,
    which achieves both a low bias and fast runtime.
    We also improve the \defn{cover tree} data structure for fast nearest neighbor queries by providing a merge procedure.
    %which improves the efficiency of nonparametric estimators such as nearest neighbor classification.
    %We show that learning algorithms based on the cover tree are mergeable.
    In doing so, we improve both the theoretical guarantees of the cover tree and its practical runtime.
    For example, the original cover tree was able to find nearest neighbors in time $O(\cexp^{12}\log n)$,
    and we improve this bound to $O(\chole^4\log n)$ for i.i.d.\ data.
    Here, $\cexp$ and $\chole$ are measures of the ``intrinsic dimensionality'' of the data,
    and on typical datasets $\cexp>\chole$.
    Experiments on large scale ad-click, genomics, and image classification tasks empirically validate these algorithms.

\end{abstract}
