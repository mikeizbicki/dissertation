\documentclass[thesis.tex]{subfiles}

\newcommand{\TO}{{\bfseries to}~}
\newcommand{\set}[1]{\mathcal {#1}}
\newcommand{\sized}[1]{\tilde \set {#1}}
\newcommand{\mappend}[1]{\oplus_{\set {#1}}}
\newcommand{\mempty}[1]{\epsilon_{\set {#1}}}
\newcommand{\monoid}[1]{(\set {#1}, \mappend {#1}, \mempty {#1})}
\newcommand{\semigroup}[1]{(\set {#1}, \mappend {#1})}
\newcommand{\model}{\hat{\text{model}}}

\newcommand{\riskstar}{{\textrm{err}^*}}
\newcommand{\riskhat}{\widehat{\textrm{err}}}
\newcommand{\riskemp}{\widehat{\textrm{err}}^\textit{emp}}
\newcommand{\risksrm}{\widehat{\textrm{err}}^\textit{srm}}
\newcommand{\riskvalid}{\widehat{\textrm{err}}^\textit{valid}}
\newcommand{\riskcv}{\widehat{\textrm{err}}^\textit{cv}}
\newcommand{\riskboot}{\widehat{\textrm{err}}^\textit{boot}}

\newcommand{\cv}{\mkprocedure{cv}}
\newcommand{\fastcv}{\mkprocedure{fast\_cv}}
\newcommand{\distcv}{\mkprocedure{dist\_cv}}
\newcommand{\greedy}{\mkprocedure{greedy}}
\newcommand{\GreeDi}{\mkprocedure{GreeDi}}

\newcommand{\merge}{\mkprocedure{merge}}
\newcommand{\mse}[1]{}

\newcommand{\eye}[1]{I_{#1}}
\newcommand{\q}[1]{q\left({#1}\right)}
\newcommand{\qi}[1]{q^{(i)}\left({#1}\right)}
\newcommand{\p}[1]{p\left({#1}\right)}
\newcommand{\psup}[2]{\hat p^{#1}({#2})}
\newcommand{\phat}[1]{\psup{}{#1}}
\newcommand{\pkde}[1]{\psup{\text{kde}}{#1}}
\newcommand{\pnp}[1]{\psup{\text{NWX,nonparametric}}{#1}}
\newcommand{\psp}[1]{\psup{\text{NWX,semiparametric}}{#1}}
\newcommand{\prior}[1]{p_0\left({#1}\right)}
\newcommand{\gaussian}[3]{\mathcal N({#1};{#2},{#3})}

\newcommand{\alg}{\mkprocedure{alg}}
\newcommand{\alglocal}{\mkprocedure{alg\_local}}
\newcommand{\Wlocal}{{\W^\textit{local}}}
\newcommand{\f}{f}
\newcommand{\walg}{{\hat\w^{\alg}}}
\newcommand{\wprefix}{{\hat\w_\textit{prefix}}{}}
\newcommand{\wsuffix}{{\hat\w_\textit{suffix}}{}}
\newcommand{\wridge}{\hat\w^\textit{ridge}}
\newcommand{\wridgep}{\hat\w^\textit{ridge,par}}
\newcommand{\wvi}{\hat\w^\textit{vi}}
\newcommand{\wsda}{\hat\w^\textit{sda}}
\newcommand{\wamps}{\hat\w^\textit{amps}}
\newcommand{\wgreedy}{\hat\w^\textit{greedy}}
\newcommand{\wGreeDi}{\hat\w^\textit{GreeDi}}
\newcommand{\wpca}{\hat\w^\textit{pca}}
\newcommand{\wlbk}{\hat\w^\textit{lbk}}
\newcommand{\wexp}{\hat\w^\textit{exp}}
\newcommand{\wdave}{\hat\w^\textit{dave}}
\newcommand{\wmcmc}{\hat\w^\textit{mcmc}}
\newcommand{\wcmc}{\hat\w^\textit{cmc}}
\newcommand{\wdle}{\hat\w^\textit{dle}}
\newcommand\sample[2]{\theta_{{#1},{#2}}}

\newcommand{\ELBO}{\text{ELBO}}
\newcommand{\Pdist}{\mathcal P}
\newcommand{\Pexp}{\Pdist^\textit{exp}}
\newcommand{\GO}[2]{\text{GO}_{{#1},{#2}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\chapter{Mergeable learning algorithms}
\label{chapter:merge}

\noindent
This chapter presents the first systematic study of mergeable machine learning algorithms.
%Many recent papers have developed methods of merging machine learning models.
%These merge algorithms are used to implement embarrassingly parallel learning algorithms using the MapReduce framework \citep{dean2008mapreduce}.
%Much of this work has occurred in different subfields of machine learning.
%This is the first systematic study of merge procedures.
Section \ref{sec:notation} begins by formally defining mergeable learning algorithms 
and showing how they are used in distributed machine learning with MapReduce \citep{dean2008mapreduce}.
Section \ref{sec:cv} reviews cross validation and shows that mergeable learning algorithms have a distributed fast cross validation procedure. 
We review other frameworks for fast cross validation,
and show that none of these frameworks were designed for distributed learning.
Next we describe many examples of merge procedures in the literature.
We divide these merge procedures into three categories.
Section \ref{sec:merge:cf} describes how to merge learning algorithms with closed form solutions.
This includes exponential families, bayesian classifiers, and ridge regression.
Most estimators, however, do not have closed form solutions.
Section \ref{sec:merge:approx} describes merge procedures for general optimization problems,
which typically lack a closed form solution.
The procedures in this section apply to a large class of learning models,
one of the most important being logistic regression.
Finally, Section \ref{sec:merge:bayes} describes merge procedures for bayesian methods.
It includes subsections on parametric estimation, variational inference, and markov chain monte carlo.
In total, we review 32 papers that present merging algorithms.
None of these papers presented fast cross validation algorithms,
but we show that all of these methods have a fast cross validation algorithm.
These 32 papers come from many subfields of machine learning,
and are summarized in Table \ref{table:merge:summary}.

\begin{table}[h]
    \centering
    %\begin{tabularx}{\textwidth}{clc}
    \begin{tabular}{clc}
    Section & Problem & Number of merge methods \\
    \hline
       \ref{sec:merge:cf} & closed form estimators & 3
    \\ \ref{sec:merge:ave} & averaging methods & 9 
    \\ \ref{sec:merge:pca} & principle component analysis & 2
    \\ \ref{sec:merge:submodular} & submodular optimization & 6
    \\ \ref{sec:merge:bvm} & parametric bayesian inference  & 1
    \\ \ref{sec:merge:vi} & variational inference & 3 
    \\ \ref{sec:merge:mcmc} & markov chain monte carlo & 8 
    \end{tabular}
    \caption{ Summary of the existing mergeable learning algorithms.}
    \label{table:merge:summary}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation and formal problem statement}
\label{sec:notation}
%All of our notation is standard,
%and this section can be skimmed with no loss of understanding.
Let $\Z$ be a probability space with an unknown distribution called the \defn{data space},
let $\W$ be the \defn{parameter space},
and $\f : \Z \to \W$ be a function called the \defn{model}.
Our goal is to estimate $\E_{\z\in\Z} \f(\z)$ based on a sample of points $Z$ from $\set Z$.
We call a function $\alg : \{\Z\} \to \W$ a \defn{machine learning algorithm} or equivalently an \defn{estimator} if
\begin{equation}
    \label{eq:notation:alg}
    \alg(Z) \approx \E_{\z\in\Z}\f(\z)
    .
\end{equation}
While the terms machine learning algorithm and estimator are interchangeable,
we will typically refer to $\alg$ as a machine learning algorithm when analyzing its computational properties
and as an estimator when analyzing its statistical properties.
Notice that except in trivial models that lack randomness, 
the approximation in \eqref{eq:notation:alg} can never be exact.
The randomness ensures that for any finite data set $Z$,
the left handside can be close to the right hand side with probability,
but they will be non-equal with probability 1.
When the data set $Z$ is understood from context,
we will denote the parameter estimate by $\walg=\alg(Z)$.

\begin{algorithm}[t]
    \caption{\mkprocedure{dist\_learn}(learning algorithm $f$, data sets $Z_i$)}
    \label{alg:dist}
    \vspace{0.1in}
    prerequisite: each machine $i$ has dataset $Z_i$ stored locally
    \begin{algorithmic}[1]
        \State each machine $i$ independently and in parallel:
        \State ~~~~~compute $\walg_i = f(Z_i)$
        \State ~~~~~transmit $\walg_i$ to the master
        \State master machine:
        \State ~~~~~compute $\walg = \merge_f\{\what_i\}_{i=1}^m$
    \end{algorithmic}
\end{algorithm}

We study a special class of estimators called \defn{mergeable estimators}.
%We say an estimator $\alg$ is mergeable if it can be decomposed into two functions $\alglocal : \{\Z\}\to\Wlocal$ and $\merge : \{\Wlocal\} \to \W$ such that
We say an estimator $\alg$ is mergeable if there exists a function $\merge : \{\W\} \to \W$ such that
%there exists a function $\merge : \{\W\} \to \W$ such that
for any collection of data sets $Z_1,...,Z_m$ satisfying $Z_i\subset\Z$,
\begin{equation}
    \label{eq:notation:merge}
    \merge\{\alg(Z_i)\}_{i=1}^m \approx \alg(\cup_{i=1}^m Z_i)
    .
\end{equation}
Many (but not all) existing estimators are mergeable,
but the merging structure has not previously been explicitly studied.%
\footnote{
%The only major work in machine learning is due to Risi Kondor.
    In the language of modern algebra,
    the $\merge$ function defines a \defn{semigroup} structure over $\W$ and the $\alg$ function is an \defn{approximate homomorphism}.
    Abstract algebra is not widely used in machine learning,
    %We will not be using any of the results from algebra,
    so we use the more intuitive terminology of mergeable estimators.
    The only major work I'm familiar with is due to Risi Kondor.
    His phd thesis studied group theoretic methods for designing kernels that obey symmetry invariants \citep{kondor2008group}.
    Abstract algebra is widely studied, however, in both computer science and statistics.

    In theoretical computer science, 
    semigroups are closely related to the study of regular languages, 
    finite automata, 
    parallel computing,
    and the complexity classes NC and AC
    \citep{tesson2004monoids,tesson2006bridges}.
    In functional programming, algebra is widely used to structure code.
    For example, \citet{yorgey2012monoids} uses semigroups to structure a library for drawing graphics.
    %The elegance of this library largely inspired my study of mergeable estimators.

    Category theory is a branch of mathematics closely related to algebra.
    \citet{baez2010physics} provide an elementary introduction to category theory and argue that category theory provides a ``rosetta stone'' that facilitates the easy transferring of domain knowledge from one area (e.g. quantum mechanics) to another (e.g. computer science).
    The emerging field of probabilistic programming languages is based largely on an algebraic/category theoretic structure called the Giry monad \citep[e.g.][]{erwig2006functional,ramsey2002stochastic,pfeffer2009figaro,scibior2015practical}.
    \citet{mccullagh2002statistical} is a unique work in the statistical literature.
    This paper uses category theory to formally describe the definition of a statistical model.
    The advantage of this formalism is that it helps the experiment designer know what assumptions are reasonable to make.
    It is not clear that there are any algorithmic implications, however.


    There is an entire subfield of statistics known as \defn{algebraic statistics}.
    This subfield has its own journal (appropriately titled \emph{Algebraic Statistics}) 
    and several books \citep{pachter2005algebraic,drton2008lectures,sullivant2017algebraic}.
    The focus of this line of work is to study exponential family distributions defined by algebraic varieties (roots of low dimensional polynomials embedded in high dimensional space).
    The tools of algebraic geometry (which describe the properties of algebraic varieties)
    can then be used to describe the properties of statistical models.
    We will briefly mention results from this line of work in our discussion of exponential families in Section \ref{sec:merge:ef},
    but otherwise this line of work is unrelated to this thesis.

    In pure mathematics, a significant body of work studies the properties of approximate homomorphisms.
    Some works that have influenced me are:
    \citet{zelinka1970tolerance} and \citet{zelinka1975tolerance} replace the equality relation with the weaker notion of a tolerance relation (transitivity need not be satisfied).
    This work was used to model fuzzy logic in expert systems. 
    %More recently, quasimorphisms have been studied \citep{kotschick2004quasi}.
    %\citet{green2014approximate} provides a survey of approximate homomorphisms and algebraic structures.
    \citet{kotschick2004quasi} and \citet{green2014approximate} provide surveys on approximate homomorphisms where the approximation has an analytic flavor, 
    as do the approximations for mergeable learning algorithms.
    A primary goal in this work is to show how to construct exact homomorphisms from approximate homomorphisms.
    This work does not consider the computational cost of the constructions,
    and so the work is not applicable to the machine learning setting. 
}
Instead, these estimators have been studied in the context of distributed machine learning.
Algorithm \ref{alg:dist} shows how the $\merge$ function can be easily used to generate a distributed estimator.
Our goal in this chapter is twofold.
First we show that all mergeable estimators have a fast cross validation procedure (Section \ref{sec:cv}).
Second, we show that many existing estimators are mergeable.
For many of these estimators, the distributed learning procedure of Algorithm \ref{alg:dist} is well known.
But for none of these algorithms was a distributed cross validation procedure previously known.
Unlike the approximation in \eqref{eq:notation:alg},
the approximation in \eqref{eq:notation:merge} can be exact (see Section \ref{sec:merge:cf}).
To measure the quality of approximation in non-exact models,
we need a way to measure the quality of an estimator.
%and is exact for the important class of estimators reviewed in Section \ref{sec:merge:exact}.
%For other models, we will be less precise about exactly what is meant by $\approx$.
%Different merge functions will satisfy different guarantees.

%Many previously created estimators are mergeable,
%but this is the first exhaustive study of their properties.

%The approximation \eqref{eq:notation:fhat} cannot be exact unless the distribution over $\Z$ is trivial,
The quality of an estimator is determined by the accuracy of the approximations in \eqref{eq:notation:alg} and \eqref{eq:notation:merge}.
To formally measure this accuracy, 
we require a \defn{loss function} $\Loss : \{\Z\} \times \W \to \R$. 
We define the \defn{true parameter} to be 
\begin{equation}
    \wstar = \argmin_{\w\in\W} \E_{\z\in\Z}\Loss(\z;\w)
\end{equation}
and assume for notational convenience that $\wstar$ is unique.
When $\W$ is a normed space, we define the \defn{statistical error} to be
%\begin{equation}
$
    \ltwo{\wstar-\what}
    .
$
%\end{equation}
It is common to use the triangle inequality to decompose the statistical error as
\begin{equation}
    \ltwo{\wstar-\what}
    \le
    \ltwo{\wstar-\E\what}
    +
    \ltwo{\E\what-\what}
    .
\end{equation}
We call the $\ltwo{\wstar-\E\what}$ the \defn{bias} of $\what$ and the $\ltwo{\E\what-\what}$ term the \defn{variance}%
\footnote{
    It is also common to call the term $\ltwo{\E\what-\what}^2$ the \defn{variance},
    in which case $\ltwo{\E\what-\what}$ could be called the \defn{deviance}.
}
of $\what$.
A well known problem in machine learning is the \defn{bias-variance tradeoff} wherein reducing the bias increases the variance and vice versa.
This tradeoff is a common theme in the development of approximate merging procedures.
We define the \defn{true loss} of a parameter vector $\what$ to be
\begin{equation}
    \riskstar = \E_{\z\in\Z}\Loss(\z;\what)
\end{equation}
and the \defn{empirical loss} of an estimator $\alg$ on dataset $Z$ to be 
\begin{equation}
    \riskemp{} = \Loss(Z;\alg(Z))
    .
\end{equation}
We can measure the quality of an estimator by the difference between the true and empirical losses.
For a good estimator, this difference should be small.
Section \ref{sec:cv} reviews how cross validation can be used to estimate this difference.

%we are given a set $Z\subseteq\Z$ of training data,
%a set $\W$ of model parameters,
%a loss function $\Loss : \Z \times \W \to \R$,
%a regularization function $\reg : \W \to \R$,
%and a regularization strength $\lambda \in \R$.%
%\footnote{
    %Throughout this thesis, we will use the script font to denote sets that may be either finite or infinite,
%and the italic font to denote sets that must be finite.
%}
A particularly important estimator is the \defn{regularized loss minimizer} (RLM).%
\footnote{
    There are many estimators closely related to the RLM that share its useful properties.
    For example: 
    empirical risk minimization,
    structural risk minimization,
    minimum description length estimation,
    maximum likelihood estimation, 
    quasi maximum likelihood estimation,
    and maximum a posteriori estimation.
    For our purposes, all these estimators are essentially the same.
}
The RLM estimator is parameterized by a regularization function $\reg : \W \to \R$ 
and a constant $\lambda : \R$.
The estimator is given by
\begin{equation}
    \label{eq:notation:rlm}
    \wrlm = \argmin_{\w\in\W} \Loss(Z;\w) + \lambda\reg(\w)
    .
\end{equation}
In the common case where the data $Z$ is drawn i.i.d.\ from $\Z$, the loss function can be decomposed as 
\begin{equation}
    \Loss(Z;\w) = \sum_{\z\in Z} \loss(\z;\w)
\end{equation}
where $\loss : \Z \to \W$ is also called the loss function.
The RLM estimator is then
\begin{equation}
    \label{eq:notation:rlm:independent}
    \wrlm = \argmin_{\w\in\W} \sum_{\z\in\Z}\loss(\z;\w) + \lambda\reg(\w)
    .
\end{equation}
Standard results on the RLM estimator show that if $|Z|=mn$,
then $\ltwo{\wstar-\what^f}\le O(1/\sqrt{mn})$ under mild conditions on $f$
\citep[e.g.][]{lehmann1999elements}.
This rate of convergence is known to be optimal.
Exact merge procedures for RLM problems automatically inherit the $O(1/\sqrt{mn})$ convergence rate.
If an approximate merge procedure also has an $O(1/\sqrt{mn})$ convergence rate,
then it is essentially just as good as the standard RLM procedure.
We shall see that existing merge procedures are only able to achieve this rate under special conditions.
In the Chapter 3, we present a novel algorithm called the optimal weighted average that achieves this optimal convergence rate in the most general setting.
%There are two common interpretations of RLM.
%In the Bayesian interpretation, the loss $\loss$ represents the negative log likelihood of a
%Most of the examples we will consider of mergeable estimators are variants of the RLM estimator.
A major challenge of RLM is choosing the optimization strength $\lambda$,
which we discuss in the next subsection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model selection and fast/distributed cross validation}
\label{sec:cv}

Model selection is the process of selecting a good machine learning algorithm for a particular task.
Unfortunately, the most accurate model selection methods have high computational cost and so cannot be used in large scale learning systems.
Most research on large scale learning has focused only on accelerating the learning algorithm,
with the model selection process receiving comparatively little attention.
%Despite the importance of model selection,
%there has been very little work on large scale or distributed model selection procedures.
%The validation set method is the standard method in large scale learning due to attractive computational features,
%but other methods have better statistical properties.
In this section, we review popular model selection procedures with emphasis on their statistical and computational tradeoffs.
A summary is shown in Table \ref{tab:cv}.
We show in particular that cross validation has the best statistical properties,
but that standard cross validation techniques are too slow for large scale data.
We develop two new cross validation techniques that address this shortcoming.
In particular, we show that for mergeable learning algorithms
cross validation can be performed with only a constant computational overhead independent of the size of the problem,
and that this cross validation can be distributed.
Effectively, this results in ``free'' cross validation estimate whenever we train a distributed model.

\begin{table}[t]
    \centering
    \begin{tabular}{lccccc}
        %method & \multicolumn{1}{p{1cm}}{\centering serial \\ runtime} & distributed runtime & bias & variance \\
        method & \makecell{serial \\ runtime} & \makecell{distributed\\ runtime} & bias & variance & prerequisites \\
        \hline
        \vspace{-0.1in}
        \\
        empirical risk & $n$ & ${n}/{p}$ & high & high & --\\
        SRM & $n$ & ${n}/{p}$ & low & high & excess risk bound\\
        validation set & $n$ & ${n}/{p}$ & low & high& -- \\
        bootstrap & $kn$ & ${kn}/{p}$ & low & low & --\\
        $k$-fold cross validation & $kn$ & ${kn}/{p}$ & low & low & --\\
        fast $k$-fold cross validation & $k+n$ & ${n}/{p}$ & low & low & mergable, $k=p$\\
    \end{tabular}
    \caption{Summary of the estimators of a model's true error.
        For simplicity, we have assumed the runtime of learning is $O(n)$,
        the runtime of merging is $O(1)$.
        We have also ignored the communication cost of distributed learning.
    }
    \label{tab:cv}
\end{table}

We begin by formally defining the \defn{model selection} problem.
Given a set of learning algorithms $\set F : \{\Z \to \W\}$ and a dataset $Z$,
the best algorithm $f^*$ is given by
\begin{equation}
    f^*
    = \argmin_{f\in\set F} \riskstar(f(Z))
    = \argmin_{f\in\set F} \E_{\z\in\Z}\Loss(\z,f(Z))
    .
\end{equation}
Because the distribution over $Z$ is unknown, 
the distribution over $f(Z)$ will also be unknown;
so $\riskstar(f(Z))$ cannot be directly computed and must be estimated.
Given an estimator $\riskhat{}^g \approx \riskstar$,
we can define an estimator of $f^*$ as
\begin{equation}
    \label{eq:cv:hatfg}
    \hat f^g = \argmin_{f\in\set F} \riskhat^g(f(Z))
    .
\end{equation}
The minimization \eqref{eq:cv:hatfg} is highly nonconvex,
and a number of algorithms have been proposed to solve it.
In the popular but naive \defn{grid search} algorithm,
the space of functions $\set F$ is discretized into a finite grid $F\subset\set F$,
and the optimization is performed over $F$.
Grid search tends to work well when $\set F$ is one dimensional,
but the performance degrades in higher dimensions.
More advanced techniques involve randomly subsampling $\set F$ \citep{bergstra2012random} and the so-called \defn{bayesian optimization} techniques \citep[e.g.][]{snoek2012practical,feurer2015efficient}.
Each of these techniques requires as a subroutine a good estimate $\riskhat^g$ of the true error.
We focus in the remainder of this section on these estimators.

The most obvious estimator of $\riskstar$ is the empirical risk $\riskemp$.
Unfortunately, the empirical risk $\riskemp$ is heavily biased because it evaluates the model on the same data it was trained on.
\defn{Structural risk minimization} (SRM) is a technique that uses a debiased empirical risk $\riskemp$ for model selection
\citep[Chapter 7 of ][]{shalev2014understanding}.
%The idea is to create an upperbound on the model's \defn{excess risk} $(\riskemp-\riskstar)$.
The idea is to create a function $b : \{\Z\} \to \R$ that upper bounds the \defn{excess risk}.
That is,
\begin{equation}
    \riskstar-    \riskemp\le b(Z)
    .
\end{equation}
These bounds are often given in terms of the learning algorithm's VC-dimension or rademacher complexity.
The SRM risk estimator is then given by
\begin{equation}
    \risksrm 
    = \riskemp + b(Z) 
    %\ge \riskstar
    .
\end{equation}
The main advantage of the SRM estimator is its computational simplicity.
SRM is not often used in practice because deriving the bound function $b$ can be difficult,
and the known bounds are typically not tight enough to give good approximations.

\newcommand{\Ztrain}{Z^\textit{train}}
\newcommand{\Ztest }{Z^\textit{test }}
\newcommand{\ntrain}{n^\textit{train}}
\newcommand{\ntest }{n^\textit{test }}
The most commonly used risk estimator in large scale learning is the \defn{validation set estimator}
\citep[Chapter 11 of ][]{shalev2014understanding}.
This method divides the training data set into two disjoint sets $\Ztrain$ and $\Ztest$.
The risk is then estimated by
\begin{equation}
    \riskvalid = \Loss(\Ztest,f(\Ztrain))
    .
\end{equation}
%Let $\ntrain$ and $\ntest$ denote the number of elements in $\Ztrain$ and $\Ztest$ respectively.
%Asymptotically, we have that as both $\ntrain\to\infty$ and $\ntest\to\infty$,
%$\riskvalid\to\riskstar$.
%Asymptotically as the size of the test set $|\Ztest|$ goes to infinity,
%but its finite sample properties are much more difficult to analyze. 
%This estimator is asymptotically unbiased in the sense that as $|\Ztest|\to\infty$,
%\begin{equation}
    %\riskvalid(f(\Ztrain)) \to \riskstar(f(\Ztrain))
    %%\E_{\z\sim\Ztest}\Loss(\z,f(\Ztrain)) = \riskstar(f(\Ztrain))
    %.
%\end{equation}
%For most natural learning algorithms, the bias converges to zero at a very fast rate.
This estimator has low bias but high variance.
In particular, the variance of $\riskvalid$ is determined by the number of samples in the validation set:
as the number of samples increases, the variance decreases.
Unfortunately, increasing the size of the validation set decreases the size of the test set,
increasing the model's true error.
There is no general procedure for finding the optimal balance between the size of the training and test sets,
although in practice it is common to use about 10\% of the data in the validation set.
The validation set method shares the favorable computational properties of SRM,
but is easier to use because there is no need to derive a bound on the excess risk.
When the bound is loose, as is often the case in practice, the validation set method will also have improved statistical performance.
Whenever the learning algorithm is parallelizable,
then both SRM and the validation set method are parallelizable as well.
%The main advantage of the validation set method is computational,
%as the function $f$ needs to be evaluated only once.
%Many modern machine learning applications use an expensive learning procedure and a large data set,
%so multiple evaluations of $f$ would be computationally impractical.

The \defn{bootstrap} improves the statistical properties of the validation set estimator but is more expensive computationally \citep{efron1979bootstrap}.
The full procedure is shown in Algorithm \ref{alg:boot}.
The idea is to use repeated subsampling to create many overlapping train/validation test splits,
then average the results.
This averaging reduces the variance of the error estimate.
The runtime is linear in the number of resampling iterations $k$,
which is too expensive for large scale problems.
\citet{kleiner2012big} and \citet{kleiner2014scalable} introduce the \defn{bag of little bootstraps} (BLB) as an efficiently computatable alternative to the bootstrap.
The BLB subsamples some small fraction of the data set in each iteration before performing the full bootstrap test.
This increases the bias of the estimator but makes it suitable for large scale problems.
As there are no data dependencies between each of the iterations,
both the standard bootstrap and BLB are easy to parallelize even when the learning algorithm is not parallelizable.

\begin{algorithm}[t]
    \caption{bootstrap(learning algorithm $f$, data set $Z$, number of samples $t$)}
    \label{alg:boot}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \For {$i=1$ \TO $k$}
            \State let $Z^{(i)}$ be a sample of $mn$ data points from $Z$ with replacement
            \State let $\riskhat^{(i)} = \Loss(Z-Z^{(i)},f(Z^{(i)}))$
        \EndFor
        \State \Return $\riskboot = \frac 1 k \sum_{i=1}^k \riskhat^{(i)}$
    \end{algorithmic}
\end{algorithm}

\defn{Cross validation} is a family of techniques that also improves the validation set method
\cite[see][for a survey]{arlot2010survey}.
%There are many variations of cross validation with slightly different theoretical properties \citep{arlot2010survey}.
A standard version called $k$-fold cross validation is shown in Algorithm \ref{alg:cv}.
The data set $Z$ is partitioned into $k$ smaller data sets $Z_1,...,Z_k$.
For each of these data sets, the validation set estimator is used to approximate the true error.
The results are then averaged and returned.

\begin{algorithm}[t]
    \caption{\cv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \label{alg:cv}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State partition $Z$ into $k$ disjoint sets $Z_1,...,Z_k$
        %\vspace{0.1in}
        \For {$i \in \{1,...,k\}$}
            \State let $\Ztrain_i = Z-Z_i$
            \State let $\what_{-i} = f(\Ztrain_i)$ 
            %\State let $\what_{-i} = f(Z - Z_i)$ 
            \State let $\riskemp_i = \Loss(\model_i,Z_i)$
        \EndFor
        %\vspace{0.1in}
        \State \Return $\frac 1 k \sum_{i=1}^k \riskemp_i$
    \end{algorithmic}
\end{algorithm}

Naive $k$-fold cross validation is an expensive procedure taking time linear in the number of folds $k$.
This makes it unsuitable for large scale learning problems.
Algorithm \ref{alg:fastcv} presents a faster method for cross validation that uses the $\merge_f$ function \citep{izbicki2013algebraic}.
The algorithm is divided into four loops.
In the first loop, a ``local model'' $\walg_i$ is trained on each partition $Z_i$.
The second loop calculates ``prefix models,''
which satisfy the property that $\wprefix_{,i}\approx f(\cup_{j=1}^{i-1} Z_j)$.
The third loop calculates  ``suffix models,''
which satisfy the property that $\wprefix_{,i}\approx f(\cup_{j=i+1}^{k} Z_j)$.
The final loop merges the prefix and suffix models so that
$\walg_{-i}\approx f(Z-Z_i)$.
The computed error is then an unbiased estimate of the true error.
When the $\merge_f$ function is exact,
then $\fastcv$ will return exactly the same answer as the standard $\cv$ procedure.
When $\merge_f$ is not exact,
then $\fastcv$ will be an approximate version of cross validation,
and the approximation depends on the accuracy of $\merge_f$.

\begin{algorithm}[t]
    \caption{\fastcv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \label{alg:fastcv}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State partition $Z$ into $k$ equally sized disjoint sets $Z_1,...,Z_k$
        %\vspace{0.1in}
        \State // calculate local models
        \For {$i = 1$ \TO $k$}
            \State $\walg_{i} = f(Z_i)$ 
        \EndFor
        %\vspace{0.1in}
        \State // calculate prefixes
        \For {$i = 1$ \TO $k$}
            %\State $\wprefix_{,i} = \wprefix_{,i-1} \mappend{} \walg_i$ 
            \State $\wprefix_{,i} = \merge_f(\wprefix_{,i-1}, \walg_i)$ 
        \EndFor
        %\vspace{0.1in}
        \State // calculate suffixes
        \For {$i = k$ \TO $1$}
            %\State $\wsuffix_{,i} = \wsuffix_{,i+1} \mappend{} \walg_i$ 
            \State $\wsuffix_{,i} = \merge_f(\wsuffix_{,i+1} , \walg_i)$ 
        \EndFor
        %\vspace{0.1in}
        \State // merge models and calculate estimated risk
        \For {$i = 1$ \TO $k$}
            %\State $\walg_{-i} = \wprefix_i \mappend{} \wsuffix_i$
            \State $\walg_{-i} = \merge_f(\wprefix_{,i} , \wsuffix_{,i})$
            \State $\riskhat^{(i)} = \Loss(\walg_{-i},Z_i)$
        \EndFor
        %\vspace{0.1in}
        \State \Return $\riskcv=\frac 1 k \sum_{i=1}^k \riskhat^{(i)}$
    \end{algorithmic}
\end{algorithm}

Standard cross validation can be easily parallelized as there are no data dependencies between each iteration of the foor loop. 
The $\fastcv$ method is slightly more difficult to parallelize due to the data dependencies in the first and second for loops.%
\footnote{
    The standard parallel prefix-sum algorithm can be used to compute the $\wprefix_{,i}$ and $\wprefix_{,j}$ vectors \citep{ladner1980parallel,blelloch1990prefix},
    which would parallelize the $\fastcv$ method.
    These parallel algorithms are designed for shared memory systems,
    and it is unclear how they generalize to the distributed environment where communication is expensive.
    Thus the $\distcv$ algorithm uses a different approach.
}
Algorithm \ref{alg:distcv} presents a distributed version of fast cross validation.
This algorithm is closely related to the distributed learning algorithm of Algorithm \ref{alg:dist} and has essentially the same runtime.
In other words, when learning an embarrassingly parallel model on a distributed architecture,
we can also perform cross validation with essentially no computational overhead.
The only difference between the distributed cross validation and distributed learning algorithms is that in the second round of communication, 
each machine (rather than only the master) computes the model $\w_{-i}$.
As these machines were sitting idle in the distributed training procedure in Algorithm \ref{alg:dist},
having them perform work does not increase the runtime.
The runtime of the cross validation procedure is increased slightly by the fact that an additional round of communication is needed where the master averages the validation errors calculated on each machine.

\begin{algorithm}[t]
    \caption{\distcv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \label{alg:distcv}
    \vspace{0.1in}
    prerequisite: each machine $i$ has dataset $Z_i$ stored locally
    \begin{algorithmic}[1]
        \State each machine $i$:
        \State ~~~~~calculates $\what_i = f(Z_i)$
        \State ~~~~~broadcasts $\what_i$ to each other machine
        \State each machine $i$:
        \State ~~~~~computes $\w_{-i} = \merge_f\{\w_1,...,\w_{i-1},\w_{i+1},...,\w_k\}$
        \State ~~~~~computes $\riskemp_i = \Loss(\what_{-i},Z_i)$
        \State ~~~~~transmits $\riskemp_i$ to the master
        \State the master machine:
        \State ~~~~~computes $\riskemp = \frac 1 k \sum_{i=1}^k \riskemp_i$
        \State ~~~~~\Return $\riskemp$
    \end{algorithmic}
\end{algorithm}

Many other fast cross validation procedures have been developed for specific models.
\citet{arlot2010survey} provide a survey of results,
describing fast cross validation methods for ridge regression, kernel density estimation, and nearest neighbor classification.
Our fast cross validation framework includes each of these models as a special case,
and many more besides.
\citet{joulani2015fast} develop a cross validation framework that is closely related to our own.
Their framework is suitable for any incremental learning algorithm 
(whereas ours is suitable for mergable algorithms).
All previous work on fast cross validation only considered the non-distributed case,
but we have shown that our fast cross validation framework is suitable for the distributed case.

Both the bootstrap and cross validation are strictly better than the validation set method of error estimation,
but 
standard no-free-lunch results \citep[e.g.\ Chapter 8 of][]{shalev2014understanding} ensure that there is no universally best method for estimating an estimator's true error for all distributions.
On real world datasets, however, $k$-fold cross validation seems to work better and is the method of choice.
\citet{kohavi1995study} empirically compare the bootstrap to $k$-fold cross validation,
and determine that on real world datasets $k$-fold cross validation has the best performance.
When computational requirements are ignored,
it is tempting to make $k$ as large as possible.
%This is not statistically optimal, however.
One of the limitations of the $\distcv$ method is that the number of folds must be the same as the number of processors.
This will be relatively small compared to the number of data points.
Conveniently, this turns out to be a good number of folds to use in practice.
\citet{rao2008dangers} shows that for very small $k$ values,
increasing $k$ improves the estimation of the model's accuracy;
but as $k$ increases beyond a certain threshold, 
the error of cross validation increases as well.
Thus the $\distcv$ method is both computationally cheap and statistically effective for real world data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Online learning}
%
%\begin{algorithm}
    %\caption{\mkprocedure{add1dp}(learning algorithm $f$, model $\what$, data point $z$)}
    %\vspace{0.1in}
    %\begin{algorithmic}[1]
        %\State \Return $\what \mappend{} f(z)$
    %\end{algorithmic}
%\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: estimators with closed form solutions}
\label{sec:merge:cf}

We now show that many popular machine learning algorithms are mergeable.
We begin by looking at models whose $\merge$ function is exact.
That is, given data sets $Z_1,...,Z_m$,
\begin{equation}
    \label{eq:examples:merge}
    \merge(\{\alg(Z_i)\}_{i=1}^m) = \alg(\cup_{i=1}^m Z_i)
    %\merge(\{\alg(Z_i)\}_{i=1}^m) ~~\text{exactly equals}~~ \alg(\cup_{i=1}^m Z_i)
    .
\end{equation}
Notice that \eqref{eq:examples:merge} is the same as \eqref{eq:notation:merge} which defined the invariant that $\merge$ should obey approximate equality,
except the $\approx$ symbol has been replaced by $=$.
In this section, we show that the RLM estimator for linear exponential families, bayesian classifers, and ridge regression all have exact merge functions.
A key feature of each of these models is that the RLM estimator has a closed form solution.
This closed form solution is what allows the development of an exact merge function.
We also show that many other estimators have closed form solutions not based on the RLM,
and these estimators are also exactly mergeable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exponential family distributions}
\label{sec:merge:ef}

%There are three important subfamilies of the EF that we will consider:
%linear exponential families (LEFs),
%curved exponential families (CEFs),
%and stratified/deep exponential families (SEFs/DEFs).

Exponential family distributions have nice computational properties and are used frequently in practice.
Many popular distributions (e.g. Gaussian, Dirichlet, Poisson, exponential, and categorical) are in the exponential family.
In this section we will see that a particularly nice subclass of the exponential family called the linear exponential family is exactly mergeable,
but that other subclasses called curved exponential families and stratified exponential families are not.
Many of the merge procedures in later sections will depend on the mergeability of the linear exponential family.

The \defn{exponential family} (EF) of distributions is defined to be the set $\Pexp$ of all distributions whose density can be written in the form
\begin{equation}
    \p{\z | \w} = h(\z)\exp\left(\trans{\w} T(\z) - \psi(\w)\right)
\end{equation}
where $\z\in\Z$ and $\w\in\W$.
We make no assumption on the set $\Z$, but require $\W$ to be an open subset of a hilbert space $\set H$.
We will ignore the technical details of infinite dimensional hilbert spaces and denote the inner product using $\trans{}$ in analogy with finite dimensional vectors.
The function $h : \Z\to\R$ is called the \defn{base measure},
$T : Z \to \W$ the \defn{sufficient statistic},
and $\psi : \W \to \R$ the \defn{log partition function}.
Many authors require $\psi$ to be strongly convex,
but we will require only the weaker condition that the derivative be invertible.

%Given a prior distribution on the parameter vector $\w$,
%the \defn{maximum a posteriori estimate} (map)
%There are many introductions to exponential families,
%but the presentation in \citet{amari2016information} is especially apropos to our discussion.
%Given a data set $Z$, we can estimate the parameters of the distribution as
%To perform parameter estimation using RLM,
%we set the loss to the negative log likelihood 
The standard method to estimate parameters in the exponential family is with \defn {maximum likelihood estimation} (MLE).
MLE is equivalent to RLM with the regularization strength $\lambda=0$ and loss equal to the negative log likelihood.
That is,
\begin{equation}
    \label{eq:ef:loss}
    \loss(\z;\w) 
    = -\log\p{\z|\w}
    = -\log h(\z) - \trans\w T(\z) + \psi(\w)
    ,
\end{equation}
and the parameter estimate is given by
\newcommand{\deriv}[2]{\frac{\dd {#1}}{\dd {#2}}}
\begin{align}
    \label{eq:ef:erm}
    %\werm = \argmin_{\w\in\W} \sum_{\z\in Z} \loss(\z;\w) + \lambda \reg(\w)
    \wexp
    &= \argmin_{\w\in\W} \sum_{\z\in Z}\left( \psi(\w) - \trans\w T(\z)\right)
    %\\
    %= \left(\deriv{}{\w}\psi\right)^{-1} \left(\frac{1}{mn} \sum_{\z\in Z} T(\z) \right)
    .
\end{align}
The $-\log h(\z)$ term in \eqref{eq:ef:loss} does not depend on the parameter $\w$ and so does not appear in the optimization \eqref{eq:ef:erm}.

%When the parameter space $\W$ equals the containing hilbert space $\set H$ equals $\R^d$,
In the special case when $\W=\set H=\R^d$,
the distribution is said to be in the \defn{linear exponential family} (LEF).%
\footnote{The LEF is sometimes called the \defn{full exponential family} (FEF) when the parameters are identifiable.}
Let $Z$ be a dataset with $mn$ elements.
Then the parameter estimate \eqref{eq:ef:erm} has closed form solution
\begin{equation}
    \label{eq:lef:erm}
    \wexp
    = \psi'^{-1} \left(\frac{1}{mn} \sum_{\z\in Z} T(\z) \right)
    ,
\end{equation}
which follows from setting the derivative of the objective in \eqref{eq:ef:erm} to zero and solving for $\w$.
%Recall that the data set $Z$ has $mn$ elements,
%which is where the $1/mn$ factor comes from in \eqref{eq:lef:erm}.
Because the equation for $\wexp$ can be given in closed form,
an exact merge procedure exists.
Decompose the data set $Z$ into $m$ data sets $Z_1,...,Z_m$ each of size $n$.
On each local data set, the parameter estimate is defined to be
\begin{equation}
    \label{eq:lef:erm:local}
    \wexp_i
    %= \left(\deriv{}{\w}\psi\right)^{-1} \left(\frac{1}{n} \sum_{\z\in Z_i} T(\z) \right)
    = \psi'^{-1} \left(\frac{1}{n} \sum_{\z\in Z_i} T(\z) \right)
\end{equation}
and the merge procedure is then
\begin{equation}
    \label{eq:lef:merge}
    \merge(\{\wexp_i\}_{i=1}^m)
    =
    %\left(\deriv{}{\w}\psi\right)^{-1} 
    \psi'^{-1} 
    \left(
        %\frac{1}{m} \sum_{i=1}^m \deriv{}{\w}\psi \left(\wrlm_i\right)
        \frac{1}{m} \sum_{i=1}^m \psi' \left(\wrlm_i\right)
    \right)
    .
\end{equation}
Substituting \eqref{eq:lef:erm:local} into \eqref{eq:lef:merge} gives the standard MLE \eqref{eq:lef:erm}.
That is, the merge function is exact.
For the linear exponential family, 
we can therefore parallelize training and perform fast cross validation.
%Notice that \eqref{eq:lef:merge} simplifies to \eqref{eq:ef:erm},
%so the learning function for LEF's is an exact semigroup homomorphism.

Other important subfamilies of the exponential family do not have exact merge procedures.
When the parameter space $\W$ is a manifold in the underlying hilbert space $\set H$,
we say the distribution is in the \defn{curved exponential family} (CEF). 
The CEF was introduced by \citet{efron1975defining} and \citet{amari1982differential}.
\citet{amari2016information} provides a modern treatment of the CEF from the perspective of information geometry.
The CEF is used to represent dependency relationships between random variables,
and a particularly important subset of the CEF is the set of undirected graphical models with no hidden variables.
\citet{liu2012distributed} prove that no exact merge procedure exists for CEF distributions.

When the parameter space $\W$ is an algebraic variety,
then the distribution is in the \defn{stratified exponential family} (SEF).
\cite{geiger1998graphical} and \citet{geiger2001stratified} introduced SEFs and show that the SEF is equivalent to the set of directed or undirected graphical models with hidden variables.
They further show that
\begin{equation}
    \label{eq:ef:hierarchy}
    \text{LEF}~ \subset ~\text{CEF}~ \subset ~\text{SEF}~\subset~\text{EF}
    ,
\end{equation}
where the set inequalities in \eqref{eq:ef:hierarchy} are strict.
An immediate consequence of \eqref{eq:ef:hierarchy} is that exact merge procedures do not exist for SEF distributions since they do not exist for CEF distributions.
%\citet{geiger2001stratified} include a number of theorems characterizing the properties of SEFs.
%The most important is that all graphical models with hidden variables are SEFs (but not CEFs).
%SEFs correspond to graphical models with hidden variables.
%The original presentation of SEFs was in terms of algebraic geometry
The \defn{deep exponential family} (DEF) of distributions was proposed by \citet{ranganath2015deep} and is a subset of the SEF.
A distribution is in the DEF if it is the distribution resulting from placing EF priors on the natural parameters of an EF distribution.
%\citet{ranganath2015deep} suggest using variational methods to solve for the parameters of DEFs.
%Section \ref{sec:merge:vi} discusses variational methods in detail.
Although exact merge procedures do not exist for the CEF, SEF, or DEF,
approximate merge procedures can be used.
See Section \ref{sec:merge:approx} for examples of approximate merge procedures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Generative classifiers}

Generative classifiers are some of the oldest and simplest machine learning algorithms.
They model the data as a probability distribution,
and parameter estimation of the classifier is just parameter estimation of the underlying distribution.
Whenever the distribution has a mergeable learning algorithm,
then the generative classifier does as well.
Fast cross validation methods for generative classifiers seem to be ``folklore knowledge'' in the statistical community,
but I know of no publications earlier than \cite{izbicki2013algebraic} that describe a fast cross validation method.

Formally, in \defn{classification} we assume that the data space $\Z=\X\times \Y$,
where $\X$ is the space of features and $\Y$ is a finite set of class labels.
The goal of classification is to learn an unknown function $\hat y : \set X \to \set Y$.
%The goal of a \defn{generative classifier} is to learn a distribution $p(y | \x)$.
%We then classify according to to rule
In a \defn{generative classifier}, the function $\hat y$ has the form
\begin{equation}
    \label{eq:gen:opt1}
    \hat y(\x) 
    =
    \argmax_{y\in\Y} p(y | \x)
    \end{equation}
where $p(y | \x)$ is a distribution from a known family with unknown parameters.
By bayes theorem, we have that
\begin{equation}
    \label{eq:gen:bayes}
    p(y | \x) = \frac{p(y)p(\x | y)}{p(\x)}
    ,
\end{equation}
and so \eqref{eq:gen:opt1} can be rewritten as
\begin{equation}
    \label{eq:gen:opt2}
    \hat y(\x) 
    =
    \argmax_{y\in\Y} p(y)p(\x | y)
    .
\end{equation}
The $1/p(\x)$ factor is dropped as it does not affect the optimization.
To learn a generative classifier,
we need to learn the parameters of the distributions $p(y)$ and $p(\x|y)$.
Whenever these distributions have mergeable learning algorithms,
%(e.g.\ they are in the linear exponential family),
then the generative classifier does as well.

The \defn{naive bayes} model is an important special case of a generative classifier that makes the following simplifying assumptions:
the feature space $\X=\R^d$, 
and each feature $\x^{(i)}$ is independent given $y$.
In notation,
\begin{equation}
    p(\x|y) = \prod_{i=1}^d p(\x^{(i)}|y)
    .
\end{equation}
Substituting into \eqref{eq:gen:opt2} gives the naive bayes classification rule
\begin{equation}
    \label{eq:gen:nb}
    \hat y(\x) 
    =
    \argmax_{y\in\Y} p(y)\prod_{i=1}^dp(\x^{(i)} | y)
    .
\end{equation}
The distribution $p(y)$ is typically assumed to be the categorical distribution,
and the distributions $p(\x^{(i)}|y)$ are typically assumed to be univariate categorical distributions for discrete data or univariate normal distributions for continuous data.
All of these distributions are in the linear exponential family,
so naive bayes has an exact mergeable learning algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ridge regression}
\label{sec:merge:ridge}

Ridge regression is one of the earliest learning algorithms,
but it is still widely studied and used.
For example, \citet{meng2014lsrn}, \citet{wang2016deco}, and \citet{gascón2017privacy} propose distributed learning algorithms.
%\citet{szabo2015twostage,szabo2016learning} uses ridge regression to solve the distribution regression problem (which is a generalization of multiple instance learning).
Fast cross validation procedures for ridge regression have been known at least since \citet{golub1979generalized}.
%No work has created a distributed cross validation algorithm.
In this section, we show another folklore result that ridge regression has an exact merge procedure.
This has the immediate consequence of showing that the fast cross validation procedure can be parallelized,
which was previously unknown.

%\cite{wang2016deco} propose a distributed algorithm for the case when $d > mn$.
%\cite{meng2014lsrn} propose a distributed solver that works in all cases.
%\cite{gascón2017privacy} proposes a distributed solver that maintains differential privacy guarantees.
%\cite{zhang2013divide,zhang2015divide} dnc kernel rr.
%More recent work \citep[e.g.][]{pahikkala2006fast,an2007fast} show fast cross validation methods in the kernelized case.
%Our single machine cross validation method can be seen as a generalization of this work.
%Furthermore, no existing ridge regression fast cross validation method supports the distributed environment.
%Despite the simplicity, ridge regression remains an actively studied problem.
%\cite{wang2016deco} propose a distributed algorithm for the case when $d > mn$.
%\cite{meng2014lsrn} propose a distributed solver that works in all cases.
%\cite{gascón2017privacy} proposes a distributed solver that maintains differential privacy guarantees.
%\cite{zhang2013divide,zhang2015divide} dnc kernel rr.
%\cite{szabo2015twostage,szabo2016learning} uses kernel ridge regression to solve the distribution regression problem (which is a generalization of multiple instance learning).

The \defn{regression problem} is closely related to classification as presented in the previous section.
The data space $\Z=\X\times \Y$, and the goal is to learn an unknown function $\hat y : \set X \to \set Y$.
Unlike the classification problem, however, the set $\Y=\R$ (recall that $\Y$ is finite in classification).
It is traditional to call $\X$ the set of \defn{covariates} and $\Y$ the set of \defn{response variables}.
The function $\hat y$ is defined to be
\begin{equation}
    \hat y(\x) = \trans\w\x
\end{equation}
where $\w\in\W=\R^d$ is a vector of model parameters.
The standard ridge regression estimator uses RLM with the loss and regularization functions 
\begin{equation}
    \loss(\x,y;\w) = \ltwo{y-\trans\w\x}^2
    ~~~~~\text{and}~~~~~
    \reg(\w) = \ltwo{\w}^2
    .
\end{equation}
Substituting into the definition of the RLM estimator from Equation \eqref{eq:notation:rlm:independent} gives 
\begin{equation}
    \label{eq:ridge:ridge}
    \wridge = \argmin_{\w\in\R^d} \sum_{(\x,y)\in Z} (y-\trans \w \x)^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
It is common to use matrix notation to simplify Equation \eqref{eq:ridge:ridge}.
Let $X=(\x_1,...,\x_{mn})$ be the $d\times mn$ matrix of covariates stacked horizontally,
and let $\y=(y_1,...,y_{mn}\trans)$ be the corresponding $d$ dimensional vector of response variables.
Then \eqref{eq:ridge:ridge} can be rewritten as
\begin{equation}
    \wridge = \argmin_{\w\in\R^d} \ltwo{\y - \trans \w X}^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
We can solve this equation by taking the derivative inside the $\argmax$ and setting it to zero.
The result has closed form solution
\begin{equation}
    \wridge = (\trans X X + \lambda I)^{-1}\trans X\y
    .
\end{equation}
The matrix product $\trans X X$ take time $O(mnd^2)$, and the inverse takes time $O(d^3)$.
Whenever $mn \gtgt d$, the product dominate the runtime and should be parallelized.

%Because ridge regression has a closed form solution,
%there is a simple embarrassingly parallel algorithm to solve it.
The closed form solution for $\wridge$ lets us construct a merge procedure.
%Each local machine $i$ calculates the statistics
For each local data set $Z_i=(X_i, Y_i)$, we calculate the local statistics
\begin{equation}
    A_i = \trans X_i X_i
    ~~~~~\text{and}~~~~~
    B_i = \trans X_i \y_i
    .
\end{equation}
The runtime of calculating the local statistics $O(nd^2)$, independent of the number of machines $m$.
The merge procedure is then
\begin{equation}
    \label{eq:ridge:ridgep}
    %\wridgep 
    \merge\{(A_i,B_i)\}_{i=1}^m
    = \left(\sum_{i=1}^m A_i + \lambda I\right)^{-1} \sum_{i=1}^m B_i
    .
\end{equation}
By definition of $A_i$ and $B_i$,
Equations \eqref{eq:ridge:ridge} and \eqref{eq:ridge:ridgep} are the same.
Calculating the summation takes time $O(md^2)$ and the inverse takes time $O(d^3)$.
The total runtime of $\merge$ is $O(md^2+d^3)$,
which is independent of the number of data points per machine $n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Ad-hoc approximations to the RLM}
%
%\fixme{}
%
%The RLM estimator has the advantage that it asymptotically achieves the Cram\'er-Rao lower bound.
%When the loss function $\loss$ and regularizer $\reg$ are not the squared $L_2$ norm,
%however, no closed form solution exists for the RLM.
%Computing the RLM in these cases is relatively computationally expensive and in particular no exact merge procedure is possible.
%To solve the computational difficulty, 
%many alternative estimators with closed form solutions have been proposed for specific problems.
%For most of these procedures, no explicit distributed or fast cross validation algorithms have been developed.
%But because they all have merge procedures,
%these distributed and fast cross validation algorithms can be derived for free.
%
%\citet{chan1994simple} consider the problem of estimating a position given sonar/radar traces.
%They approximate the RLM estimator with a closed form estimator that also manages to achieve the Cram\'er-Rao lower bound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: approximate merging for RLM}
\label{sec:merge:approx}

RLM is by far the most commonly used learning method.
Unfortunately, for most RLM problems no closed form solution exists.
Therefore, no exact merging procedure exists. 
In many settings, however, approximate merge methods are good enough,
and a variety of approximate merging methods have been created.
We divide these methods into three categories:
(i) averaging based merge procedures,
(ii) merge procedures for the principle component analysis (PCA) problem,
and (iii) merge procedures for submodular losses.
%For each of these categories,
%we discuss the quality of approximation possible.
%We divide these methods into five categories.
%The first two categories are averaging methods and one-step estimators.
%These are general methods that work in the common scenario where the parameter space $\W$ is a subset of some vector space.
%A typical application is logistic regression.
%The third category is methods specific to principle component analysis (PCA).
%All the methods from the first two categories can be applied to PCA,
%but PCA has additional structure that allows for further improvement.
%The fourth category is methods for submodular loss functions.
%
%A common feature of the first four categories is that the merge functions do not depend on the data,
%but only on 
%Impossibility results have shown that in this scenario,
%no optimal merge function is possible \citep{liu2012distributed}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Averaging based methods}
\label{sec:merge:ave}
The averaging based merge procedures are the simplest and most general methods.
Most of these procedures require only that the parameter space $\W$ have vector space structure.
This is common in many problems,
the standard example being logistic regression.
This section presents 9 merge methods 
\citep{merugu2003privacy,mcdonald2009efficient,zinkevich2010parallelized,zhang2012communication,zhang2013divide,liu2014distributed,battey2015distributed,han2016bootstrap,jordan2016communication,lee2015communication}.
Each of these methods implies a novel fast cross validation procedure,
and I am not aware of any previous fast cross validation methods that apply in the general setting of these merge operations.
A common theme of these merge procedures is that they do not depend on the data.
In the next chapter, we present a novel improvement to these estimators that does depend on the data 
and so has better approximation guarantees.

%Each method in this section will use a common notation.
%We assume the data set $Z$ has $mn$ data points and has been partitioned into $m$ datasets $Z_1,...,Z_m$ each with $n$ data points.
%The 

The simplest and most popular mergeable estimator is the \defn{naive averaging estimator}
\begin{equation}
\wave = \frac{1}{m}\sum_{i=1}^m \wrlm_i
.
\end{equation}
%This estimator is well studied.
Naive averaging was first studied by \citet{mcdonald2009efficient} for the case $L_2$ regularized maximum entropy models.
They provide concentration inequalities for the estimation error,
showing that the variance $\ltwo{\E\wave-\wave}$ reduces at the optimal rate of $O((mn)^{-1/2})$,
but that the bias $\ltwo{\wstar-\E\wave}$ reduces only as $O(n^{-1/2})$.
%Their analysis uses a martingale technique that requires the radius of the dataset be independent of the size of the dataset.
%This is a particularly limiting assumption as even the simple case of
%normally-distributed data does not satisfy it.
Subsequent work has focused on merge methods that improve both the bias and the variance.

Under more stringent assumptions,
it can be shown that the bias of $\wave$ will reduce at a faster rate.
In particular, when the bias of $\wrlm_i$ shrinks faster than $O(n^{1/2})$,
then naive averaging can be an effective merge procedure.
\citet{zhang2012communication} show that the \defn{mean squared error} (MSE) $\E\ltwo{\wstar-\wave}{}^2$ decays as $O((mn)^{-1} + n^{-2})$.
This matches the optimal MSE of $\wrlm$ whenever $m<n$.
Their analysis also requires limiting assumptions.
For example, they assume the parameter space $\W$ is bounded.
This assumption does not hold under the standard Bayesian interpretation of L2 regularization as a Gaussian prior of the parameter space.
They further make strong convexity and 8\emph{th} order smoothness assumptions which guarantee that $\wrlm_i$ is a ``nearly unbiased estimator'' of $\wstar$.
Most recently, \citet{rosenblatt2016optimality} analyze $\wave$ in the asymptotic regime as the number of data points $n\to\infty$.
This analysis is more general than previous analyses, but it does not hold in the finite sample regime.
%Our analysis of OWA in Section \ref{sec:anal} requires no assumptions of boundedness or convexity, holds in the finite sample regime, and shows OWA reducing both bias and variance.
\citet{zinkevich2010parallelized} show that if the training sets partially overlap each other (instead of being disjoint), then the resulting estimator will have lower bias.

\citet{zhang2013divide} show how to reduce bias in the special case of kernel ridge regression.
By a careful choice of regularization parameter $\lambda$,
they cause $\wrlm_i$ to have lower bias but higher variance,
so that the final estimate of $\wave$ has both reduced bias and variance.
%This suggests that once the proper regularization parameter is known,
%there is no need for a bias reduction at all.
This suggests that a merging procedure that reduces bias is not crucial to good performance if we set the regularization parameter correctly.
Typically there is a narrow range of good regularization parameters,
and finding a $\lambda$ in this range is expensive computationally.
Our $\distcv$ method in Algorithm \ref{alg:distcv} can be used to more quickly tune this parameter.
%We show experimentally in Section \ref{sec:exp} that our method has significantly reduced sensitivity to $\lambda$.
%Therefore, it is computationally cheaper to find a good $\lambda$ for our method than for the other methods discussed in this section.

\citet{battey2015distributed} and \citet{lee2015communication} independently develop methods to reduce the bias for the special case of lasso regression.%
\footnote{
    Both \citet{battey2015distributed} and \citet{lee2015communication} originally appeared on arXiv at the same time in 2015, but only \citet{lee2015communication} has been officially published.
}
In the lasso, the loss function is the squared loss (as in ridge regression),
but the regularization function is the $L_1$ norm (instead of the squared $L_2$ norm).
The idea is to use an estimator with low bias and high variance to train a model on each local data set $Z_i$.
Then, when the results are averaged,
the bias will remain low,
and the high variance will be reduced.
They propose using the following \defn{debiased lasso estimator} (DLE) on each dataset:
\begin{equation}
    \wdle_i = \wrlm_i + \frac 1 n \hat\Theta \trans X (y - X\wrlm_i)
    ,
\end{equation}
where $\hat\Theta$ is an approximate inverse of the empirical covariance $\hat\Sigma=\trans X X$.
The resulting \defn{debiased average} (DAVE) estimator is
\begin{equation}
    \wdave = \frac 1 m \sum_{i=1}^m \wdle
    .
\end{equation}
The bias and variance of this estimator both shrink at the optimal rate of $O((nm)^{-1/2}$.

\citet{zhang2012communication} provide a debiasing technique that works for any estimator.
It works as follows.
Let $r\in(0,1)$, and $Z_i^r$ be a bootstrap sample of $Z_i$ of size $rn$.
Then the bootstrap average estimator is
\begin{equation*}
\wboot = \frac{\wave-r\waver}{1-r},
\text{~~~~~where~~~~~}
\waver = \frac{1}{m}\sum_{i=1}^m \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
.
\end{equation*}
%where
%\begin{equation}
%\begin{aligned}
%\waver = \frac{1}{m}\sum_{i=1}^m \wrlmr_i
%,
%\text{~~~~~and~~~~~}
%%\\
%\wrlmr_i = \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
%.
%\\
%,
%\\
%\wboot & = \frac{\wave-r\waver}{1-r}
%.
%\end{aligned}
%\end{equation}
The intuition behind this estimator is to use the bootstrap sample to directly estimate and correct for the bias.
When the loss function is convex, $\wboot$ enjoys a mean squared error (MSE) that decays as $O((mn)^{-1}+n^{-3})$. %under similar assumptions as their analysis of $\wave$.
Theorem 2 implies that the MSE of $\wowa$ decays as $O((mn)^{-1})$ under more general conditions.
There are two additional limitations to $\wboot$.
First, the optimal value of $r$ is not obvious and setting the parameter requires cross validation on the entire data set.
%Our proposed $\wowa$ estimator has a similar parameter $\lambda_2$ that needs tuning,
%but this tuning happens on a small fraction of the data and always with the L2 regularizer.
So properly tuning $\lambda_2$ is more efficient than $r$.
Second, performing a bootstrap on an unbiased estimator increases the variance.
This means that $\wboot$ could perform worse than $\wave$ on unbiased estimators.
%Our $\wowa$ estimator, in contrast, will perform at least as well as $\wave$ with high probability, as seen in Figure \ref{fig:contour}.
%In Section \ref{sec:exp}, we show that $\wowa$ has better empirical performance than $\wboot$.

\citet{jordan2016communication} propose to reduce bias by incoporating second order information into the average.
In particular, they develop an approach that uses a single approximate Newton step in the merge procedure.
%The optimal newton step would be given by
%\begin{equation}
    %\wave - \nabla_{\wave}^2\Loss(Z;\wave)^{-1} \nabla_{\wave} \Loss(Z;\wave)
    %.
%\end{equation}
%Computing the hessian matrix $\nabla_{\wave}^2\Loss(Z;\wave)^{-1}$ is computationally difficult,
As long as the initial starting point (they suggest using $\wave$) is within $O(\sqrt{1/n})$ of the true parameter vector,
then this approach converges at the optimal rate.
%They suggest using $\wave$ as the starting point.
When implementing Jordan et al.'s approach, we found it suffered from two practical difficulties.
First, Newton steps can diverge if the starting point is not close enough.
We found in our experiments that $\wave$ was not always close enough.
Second, Newton steps require inverting a Hessian matrix.
In the experiments of Chapter 3, we consider a problem with dimension $d\approx7\times10^5$;
the corresponding Hessian is too large to practically invert.
%For these reasons, we do not compare against \citet{jordan2016communication} in our experiments.

\citet{liu2014distributed} propose a more Bayesian approach inspired by \citet{merugu2003privacy}.
Instead of averaging the model's parameters,
they directly ``average the models'' with the following KL-average estimator:
\begin{equation}
    \label{eq:klave}
\wkl = \argmin_{\w\in\W} \sum_{i=1}^m \kl[\bigg]{p(\cdot;\wrlm_i)}{p(\cdot;\w)}
.
\end{equation}
Liu and Ihler show theoretically that this is the best merge function in the class of functions that do not depend on the data.
%Since OWA's merge depends on the data, however, this bound does not apply.
The main disadvantage of KL-averaging is computational.
The minimization in \eqref{eq:klave} is performed via a bootstrap sample from the local models,
which is computationally expensive.
This method has three main advantages.
First, it is robust to reparameterizations of the model.
Second, it is statistically optimal for the class of non-interactive algorithms.
%(We show in the next section that this optimality bound does not apply to our $\wowa$ estimator due to our semi-interactive setting.)
Third, this method is general enough to work for any model,
even if the parameter space $\W$ is not a vector space.
%whereas our proposed OWA method works only for linear models.
The main downside of the KL-average is that the minimization has a prohibitively high computational cost.
Let $n^{kl}$ be the size of the bootstrap sample.
Then Liu and Ihler's method has MSE that shrinks as $O((mn)^{-1}+(nn^{kl})^{-1})$.
This implies that the bootstrap procedure requires as many samples as the original problem to get a MSE that shrinks at the same rate as the averaging estimator.
\citet{han2016bootstrap} provide a method to reduce the MSE to $O((mn)^{-1}+(n^2n^{kl})^{-1})$ using control variates, but the procedure remains prohibitively expensive.
Their experiments show the procedure scaling only to datasets of size $mn\approx10^4$,
whereas our experiments involve a dataset of size $mn\approx10^8$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Principle component analysis}
\label{sec:merge:pca}

\defn{Principle component analysis} (PCA) is a popular technique for dimensionality reduction.
%Given a data set $Z$ of points in $\R^d$,
%the goal of PCA is to find the subspace of dimension $k$ that captures most of the variability of the data.
%The technique was introduced by \cite{pearson1901liii}, 
%and \cite{hotelling1933analysis} first introduced the term principle components.
In this section,
we describe how PCA works on a single machine,
show that two existing techniques for distributed PCA are mergeable
\citep{qu2002principal,liang2013distributed},
and describe the importance of fast cross validation techniques for PCA.

Let $Z$ denote the $mn\times d$ input data matrix.
The goal of PCA is to find a $d\times k$ matrix with $k \ltlt d$ such that the distance between $Z$ and $Z\w\pinv\w$ is minimized.
Formally, 
%the goal of PCA is to find a subspace of dimension $k<d$ that captures most of the variability of the data.
\begin{equation}
    \label{eq:pca:defn}
    \wpca = \argmin_{\w\in\R^{d\times k}} \ltwo{Z - Z\w\pinv\w}^2
    .
\end{equation}
The optimization \eqref{eq:pca:defn} is non-convex, 
but the solution can be calculated efficiently via the \defn{singular value decomposition} (SVD). 
The SVD of $Z$ is
\begin{equation}
    Z = UD\trans V
    ,
\end{equation}
where $U$ is an orthogonal matrix of dimension $mn\times mn$, 
$D$ is a diagonal matrix of dimension $mn\times d$, 
and $V$ is an orthogonal matrix dimension $d\times d$.
The columns of $U$ are called the left singular vectors, 
the columns of $V$ the right singular vectors,
and the entries in $D$ the singular values.
The solution to \eqref{eq:pca:defn} is given by the first $k$ columns of $V$.
\citet{halko2011finding} provides a method for efficiently calculating approximate SVDs on a single machine when only the first $k$ singular values/vectors are needed.

\cite{qu2002principal} and \cite{liang2013distributed} introduce essentially the same algorithm for distributed PCA.
Each machine $i$ locally calculates the SVD of its local dataset $Z_i = U_i D_i \trans {V_i}$.
Let $D_i^{(k)}$ denote the $k\times k$ submatrix of $D_i$ containing the first $k$ singular values,
and $V_i^\{k\}$ denote the $d\times k$ submatrix of $V_i$ containing the first $k$ columns.
The local machines each transmit $D_i^{(k)}$ and $V_i^{(k)}$ to the master machine.
The master calculates 
\begin{equation}
    S = \sum_{i=1}^m V_i^{(k)}D_i^{(k)}\trans{V_i^{(k)}}
\end{equation}
Performing the SVD on $S$ then gives the approximate principle components of the entire data set $Z$.
\cite{qu2002principal} further provide a modification to the merge procedure that approximately centers the data,
but they do not provide any theoretical guarantees on the performance of their algorithm relative to the single machine oracle.
\cite{liang2013distributed} do not consider the possibility of centering the data,
but they do show that their algorithm is a $1+\varepsilon$ approximation of the single machine algorithm, where $\varepsilon$ depends on properties of the data and the choice of $k$.

A major difficulty in PCA (distributed or not) is selecting a good value for $k$.
There are two reasons to choose a small value of $k$.
The most obvious is computational.
When $k$ is small, future stages in the data processing pipeline will be more efficient because they are working in a lower dimensional space.
But there is a more subtle statistical reason.
When there is a large noise component in the data,
using fewer dimensions removes this noise and improves the statistical efficiency of later stages of the data pipeline.
Perhaps the simplest method of determining $k$ is the scree test \citep{cattell1966scree},
where the data's singular values are plotted and the analyst makes a subjective judgement.
More robust methods make distributional assumptions \citep{bartlett1950tests}.
Under these assumptions, the noise in the data can be estimated directly and $k$ determined appropriately.
When these distributional assumption do not hold, however, the resulting $k$ value can be arbitrarily poor.
The most robust solution uses cross validation and the PRESS statistic.
%%In general, the optimal $k$ value will depend on the task at hand.
%Many practitioners use heuristic techniques of ``what looks good enough''.
%A number of more disciplined approaches make distributional assumptions.
%The most general technique is to use cross validation.
Unfortunately, this is also the most expensive technique computationally.
Considerable work has been done to improve both the theoretical guarantees of cross validation and improve its runtime via approximations
\citep{wold1978cross,eastment1982cross,krzanowski1987cross,mertens1995efficient,diana2002cross,engelen2004fast,josse2012selecting,camacho2012cross}.
Notably, none of these fast cross validation techniques work in the distributed setting.
Thus the distributed fast cross validation technique induced by \cite{qu2002principal} and \cite{liang2013distributed} is both novel and useful.

%%Another vein of work improves the robustness of PCA.
%%\cite{collins2002generalization} generalizes PCA to other exponential family distributions.
%%\cite{ding2004k} shows a close relationship between PCA and $k$-means.
%%
%A final vein of work improves the runtime of PCA.
%One of the simplest approaches is to subsamples the data set into a so-called \emph{core set},
%then run PCA on this smaller data set.
%If the core set is constructed appropriately,
%then the result will be a good approximation of the PCA on the entire data set.
%\cite{garber2015fast} provides the state-of-the-art core set generation method
%along with an excellent survey of prior work.
%A number of works deal with the distributed environment.
%\cite{bai2005principal} distributed PCA that doesn't work in this framework.
%\cite{schizas2015distributed} use ADMM for distributed PCA.
%\cite{liu2016decentralized} uses ADMM for distributed clustering.
%\cite{kannan2014princople} provide a method for PCA that requires only $O(1)$ rounds of communication.
%This method is not suitable for our framework, however, because we require exactly 1 round of communication.

%\cite{liang2013distributed} the good stuff.
%
%Let $X$ be a $d\times j$ matrix with orthonormal columns.
%Let $\varepsilon\in(0,1]$ and $k\in\mathbb N$ satisfying 
%\begin{equation}
    %\ltwo{\wpca X - \wlbk X}^2 \le \varepsilon d(P,L(X))^2
%\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Submodular optimization}
\label{sec:merge:submodular}

Submodular functions are a class of set function that share many important properties with convex functions \citep{lovasz1983submodular}.
In particular, they are easy to optimize and have many applications. 
The maximization of submodular functions has become an important technique in the approximation of NP-hard problems \citep{krause14survey}.
%\citet{mirzasoleiman2016distributed} provide a detailed survey of recent applications in machine learning.
Applications in machine learning include clustering, sparse nonparametric regression, image segmentation, document summarization, and social network modeling \citep[see references within][]{mirzasoleiman2016distributed}.
The last five years has seen work focused on scaling up submodular optimization via distributed algorithms.
Six of these methods create mergeable estimators that fit our framework
\citep{mirzasoleiman2013distributed,barbosa2015power,malkomes2015fast,bhaskara2016greedy,barbosa2016new,mirzasoleiman2016distributed}
and so induce fast cross validation procedures.
This is the first work addressing fast cross validation in submodular learning algorithms.
In this section we first define submodularity,
then introduce the distributed optimizers.

For a set function $f : \{\Z\} \to \R$,
a set $Z\subset\Z$ and an element $e\in\Z$,
we define the \defn{discrete derivative} of $f$ at $Z$ with respect to $e$ to be
\begin{equation}
    f'(e; Z) = f (Z\cup\{e\}) - f(Z)
    .
\end{equation}
We call the function $f$ \defn{monotone} if for all $e$ and $Z$, $f'(e;Z) \ge 0$.
We further call $f$ \defn{submodular} if for all $A \subseteq B \subseteq Z$ and $e\in Z-B$,
\begin{equation}
    f'(e;A)\ge f'(e;B)
    .
\end{equation}
We first consider the problem of monotone submodular maximization subject to cardinality constraints.
That is, we want to solve
\begin{equation}
    \label{eq:submodular:opt}
    \hat S =
    \argmax_{S\subseteq Z} f(S)
    ~~~~~\text{s.t.}~~~~~
    |S|\le k
    .
\end{equation}
%Many machine learning problems can be cast as a special case of submodular maximization.
Solving \eqref{eq:submodular:opt} is NP-hard in general,
so it is standard to use the greedy approximation algorithm introduced by \citet{nemhauser1978analysis}.
The procedure (which we will refer to as $\greedy$) is shown in Algorithm \ref{alg:submodular:greedy}.
The $\greedy$ algorithm is known to be a $1-1/e$ approximation algorithm\footnote{
The symbol $e$ is Euler's constant and not an approximation variable.},
and no better approximation algorithm exists unless $P=NP$ \citep{krause14survey}.
\begin{algorithm}[t]
    \caption{\greedy(data set $Z$, constraint size $k$)}
    \label{alg:submodular:greedy}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State $S \leftarrow \{\}$
        \For {$i = 1$ \TO $k$}
            \State $\z_i \leftarrow \argmax_{\z\in Z-S} f'(\z;S)$
            \State $S \leftarrow S \cup \{\z_i\}$
        \EndFor
        \State \Return $S$
    \end{algorithmic}
\end{algorithm}

\cite{mirzasoleiman2013distributed} introduced the first method of merging independently computed solutions in their $\GreeDi$ (GREEdy DIstributed) algorithm.
$\GreeDi$ works by first running the $\greedy$ algorithm locally on each node to compute local solutions $\wgreedy_i$.
These solutions are transmitted to the master machine.
The master combines the local solutions into a set of size $km$ and reruns $\greedy$ on the combined set.
%That is, the merge procedure is given by the equation
In notation, the merge procedure is given by
\begin{equation}
    \GreeDi(\wgreedy_1,...,\wgreedy_m) = \greedy(\cup_{i=1}^m\wgreedy_i,k)
    .
\end{equation}
\cite{mirzasoleiman2013distributed} show that in the worst case,
$\GreeDi$ achieves an approximation guarantee of 
\begin{equation}
    f(\wGreeDi) \ge \frac{(1-1/e)^2}{\min\{m,k\}} f(\wstar)
    .
\end{equation}
%They also show these bounds can be improved when the set $Z$ has either an associated metric or distribution,
%which is often the case in practice.
\citet{barbosa2015power} improve the analysis of $\GreeDi$ to show that in expectation
\begin{equation}
    f(\wGreeDi) \ge \frac {1-1/e}{2} f(\wstar)
    ,
\end{equation}
which matches the guarantee of the optimal $\greedy$ centralized algorithm up to the $1/2$ constant factor.
Notably, the approximation is independent of the number of machines $m$ or the size of the problem $k$.

Subsequent work has extended the $\GreeDi$ framework to apply to more general submodular optimization problems.
\cite{malkomes2015fast} solves the $k$-centers clustering problem;
\cite{bhaskara2016greedy} solves the column subset selection problem;
and both \cite{barbosa2016new} and \citet{mirzasoleiman2016distributed} solve submodular problems with matroid, $p$-system, and knapsack constraints.
The algorithms presented in each of these papers follows the same basic pattern:
the $\greedy$ algorithm used by the local machines and the merge procedure is replaced by an alternative algorithm that is more appropriate for the new problem setting.

As with continuous statistical optimization problems,
the difficulty of submodular optimization is known to depend on the curvature of the problem
\citep{vondrak2010submodularity}.
The \defn{submodular curvature} is defined to be
\begin{equation}
    c = 1 - \min_{e\in Z} \frac{f'(e;Z-e)}{f(e)}
    .
\end{equation}
When the curvature is small, the $\greedy$ algorithm will perform better.
In particular, when the $c=0$ the problem is said to be \defn{modular} and the $\greedy$ algorithm returns an optimal solution.
\citet{vondrak2010submodularity} shows that for all $c>0$, $\greedy$ returns a $(1-e^{-c})/c$ approximate solution,
and that no better approximation is possible.
There is as yet no work discussing the relationship of curvature to the difficulty of merging local solutions.
It seems likely, however, that a bound analogous to the continuous bound provided by \citet{liu2014distributed} will hold.

%\citet{lovasz1983submodular} shows that submodular functions are closely related to convex functions.
%\subsubsection{Applications of submodular optimization}
%
%active set selection in sparse gaussian processes \citep{mirzasoleiman2016distributed},
%inference for determinental point processes \citep{mirzasoleiman2016distributed}
%
%Submodular functions have been used to create summaries of documents
%\citep{lin2004rouge,lin2010multi,lin2011class,lin2012learning}.
%\citet{tschiatschek2014learning} introduces many submodular loss functions for summarizing image data.
%They also point out that \citet{simon2007scene}
%\citet{sinha2011extractive}
%\citet{sinha2011summarization}
%\citet{denton2004selecting}
%use submodular objectives.
%\citet{kulesza2011k} uses a submodular objective to sample from determinental point processes.
%\citet{iyer2015submodular} introduces submodular point processes,
%and \citet{gotovos2015sampling} shows how to sample from these models.
%Can this be combined with the parallel MCMC procedures?
%\cite{qi2016robust} use distributed submodular optimization to partition a dataset into $m$ clusters.
%Each of these clusters then learns a neural network independently and can be thought of as part of the free monoid framework.

%Techniques in submodular optimization are widely used in machine learning.
%\cite{malkomes2015fast} uses the same framework to solve the $k$-centers clustering problem.
%They provide improved approximation guarantees that are stronger than the general submodular case.
%\cite{bhaskara2016greedy} uses the same framework on a specific class of submodular optimization problems called ``column subset selection.''
%They provide approximation guarantees for this problem that are better than the general case.
%
%\cite{barbosa2016new} considers a more general submodular optimization problem than \eqref{eq:submodular:opt}.
%In particular, they do not require the function be monotone, and they support matroid and $p$-system constraints in addition to cardinality constraints.
%\citet{mirzasoleiman2016distributed} show that their original GreeDi algorithm is also able to handle the case of non-monotone functions under these more general constraints.
%
%active set selection in sparse gaussian processes 
%inference for determinental point processes \citep{mirzasoleiman2016distributed}
%
%\subsubsection{Applications of submodular optimization}
%
%\cite{qi2016robust} use distributed submodular optimization to partition a dataset into $m$ clusters.
%Each of these clusters then learns a neural network independently and can be thought of as part of the free monoid framework.
%
%\cite{lucic2016horizontally} provides an interactive distributed algorithm,
%plus a good table summarizing results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: Bayesian Methods}
\label{sec:merge:bayes}

%Bayesian machine learning methods estimate the \defn{posterior distribution} $\p{\theta|Z}$ of a parameter $\theta$ given data set $Z$.
%We assume that our data set is sampled from some distribution $p(\z|\w)$,
%and we have a prior distribution $p(\w)$.
%Then the \defn{posterior distribution} is defined to be $\p{\w|\z}$,
%and by bayes theorem we have
%\begin{equation}
    %p(\w | \z) = \frac{p(\z|\w)p(\w)}{p(\z)}
    %.
%\end{equation}
%In bayesian methods, we make a parametric assumption about the posterior distribution.

In Bayesian inference, 
we treat the data set $Z$ as observed variables and assume there is a hidden variable $\theta$ on which the data depends.
%we are given a model that includes observed variables $\z$ and hidden variables $\theta$.
Our goal is to calculate the \defn{posterior distribution} $p(\theta | Z)$.
%By convention, the notation $p(\theta | Z)$ suppresses the distribution's dependence on parameters $\w\in\W$. 
In this section we discuss three general techniques for learning the posterior.
%The first method uses a simple parametric assumption on the posterior distribution and is the Bayesian equivalent of model averaging.
We warm-up with the Bernstein-von Mises (BvM) method.
BvM uses a simple model based on a parametric approximation and sufficient statistics.
%Distributed learning of the BvM method is the Bayesian analog of parameter averaging.
Next we discuss variational inference (VI).
VI also makes a parametric approximation,
but uses a more complex optimization procedure to choose the parameters.
We shall see that the distributed methods for VI are closely related to those for RLM.
The final method is markov chain monte carlo (MCMC).
MCMC uses sampling to approximate the posterior.
The distributed MCMC methods share little in common with the optimization methods.
%We can use bayes theorem to write the posterior as 
%\begin{equation}
    %p(\theta | \z) = \frac{p(\z|\theta)p(\theta)}{p(\z)}
    %.
%\end{equation}
%The conditional probability of the evidence $p(\z|\theta)$ and the prior distribution $p(\theta)$ are typically easy to compute,
%but the evidence distribution $p(\z)$ is often computationally intractable.

%\begin{align}
    %p(\w | Z) 
    %&= \frac{p(Z|\w)p(\w)}{p(Z)}
    %\\
    %&= \frac{\prod_{i=1}^mp(Z_i|\w)p(\w)}{\prod_{i=1}^mp(Z_i)}
    %\\
    %&= \frac{\prod_{i=1}^m\frac{p(\w|Z_i)p(Z_i)}{p(\w)}p(\w)}{\prod_{i=1}^mp(Z_i)}
%\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bernstein-von Mises}
\label{sec:merge:bvm}

When the posterior distribution $p(\theta|Z_i)$ has a parametric form in the exponential family,
then the local posteriors can easily be combined (Section \ref{sec:merge:ef}).
Under mild conditions, the \defn{Bernstein-von Mises theorem}\footnote{
    The Bernstein-von Mises theorem is also often called the Bayesian central limit theorem.
}
states that as the number of samples $n\to\infty$,
the distribution $p(\theta|Z_i)$ converges to a normal distribution. 
See for example Chapter 10.2 of \cite{vandervaart1998asymptotic} for a formal statement of the theorem with conditions.
\cite{neiswanger2014asymptotically} use this result to create a simple distributed learning procedure that can be thought of as the Bayesian alternative to parameter averaging (Section \ref{sec:merge:ave}). 
%
%That is,
In particular, they assume
\begin{equation}
    \p{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu_i}{\hat\Sigma_i}
\end{equation}
where $\hat\mu_i$ and $\hat\Sigma_i$ are local mean and covariance parameter estimates computed by either variational inference (Section \ref{sec:merge:vi}) or markov chain monte carlo (Section \ref{sec:merge:mcmc}).
The final posterior density is then given by
\begin{equation}
    \phat{\theta | Z}
    =
    \prod_{i=1}^m
    \phat{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu}{\hat\Sigma}
    ,
\end{equation}
where
\begin{equation}
    \hat\Sigma
    =
    \left(
        \sum_{i=1}^m
        \hat\Sigma_i^{-1}
    \right)^{-1}
    ~~~~~\text{and}~~~~~
    %\\
    \hat\mu
    =
    \hat\Sigma \left(\sum_{i=1}^m \hat\Sigma^{-1} \mu_i \right)
    .
\end{equation}
Like averaging (see Section \ref{sec:merge:ave}),
this method reduces the variance but not the bias of the model parameters $\hat\mu$ and $\hat\Sigma$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Variational inference (VI)}
\label{sec:merge:vi}

Variational inference (VI) is a popular method for approximating intractable posterior distributions \citep{jordan1999introduction,blei2017variational}. 
%\cite{jordan1999introduction} and \cite{blei2017variational} provide detailed introductions.
%In this section we present VI as a form of ERM and three methods for distributed VI that fit our framework.
There are three existing methods of distributed VI that fit our framework \citep{broderick2013streaming,campbell2014approximate,neiswanger2015embarrassingly}.
To describe these methods,
we will present VI as a special case of RLM.
This is a non-standard presentation of VI,
but it highlights the similarities between distributed VI methods and the distributed RLM methods already presented.
In particular, the 9 mergeable RLM estimators of Section \ref{sec:merge:ave} can be applied directly to the VI problem.
These estimators have several advantages over the VI-specific merging procedures,
including explicit regret bounds.%
\footnote{
    It seems to be a trend that the optimization techniques used in variational methods lag behind the state-of-the-art non-bayesian learning approaches by several years.
    For example:
    (1) Stochastic gradient descent (SGD) was an established technique in large scale non-bayesian learning at least as early as \citet{bottou2004large},
    but SGD did not get applied to the variational bayes setting until \citet{hoffman2013stochastic}.
    Some papers \citep[e.g.][]{canini2009online,hoffman2010online,wang2011online} used online learning techniques earlier,
    but they only applied in special cases and still lagged behind the non-bayesian optimization research.
    (2) Differentially private empirical risk minimization has been known since \citet{chaudhuri2011differentially}, with privacy preserving models existing in special cases earlier \citep{chaudhuri2009privacy}.
    Differential privacy appears to have only been applied to variational inference so far in three papers \citep{karwa2015private,park2016variational,jalko2016differentially},
    all of which are currently under review and remain published only on arXiv.
    (3) Essentially no papers on variational inference provide learning guarantees (either finite sample or asymptotic) on the quality of the learned parameters,
    whereas hundreds (if not thousands) of these results exist for the ERM case.
}
As far as I know, there is no existing work on fast cross validation procedures for variational methods.
%We will derive three new fast cross validation methods based on the distributed VI methods,
%and \fixme{} new fast cross validation methods based on the distributed ERM methods.

Variational inference uses optimization to create a deterministic approximation to the posterior that is easy to compute.
Given a surrogate family of distributions $Q = \{ q(\theta|\w) : \w\in\W \}$,
the variational parameters are given by
\begin{equation}
    \label{eq:vi:wvi:kl}
    \wvi = \argmin_{\w\in\W} \kl{q(\theta|\w)}{p(\theta|Z)}
    .
\end{equation}
Solving \eqref{eq:vi:wvi:kl} is equivalent to the ERM problem
\begin{equation}
    \label{eq:vi:wvi:erm}
    \wvi = \argmin_{\w\in\W} \sum_{\z\in Z} \loss(\z;\w) + \lambda\reg(\w)
\end{equation}
where
\begin{equation}
    \loss(\z;\w) = -\int_{\theta\in\Theta} q(\theta|\w)\log p(\z|\theta) \dd\theta
    ~~~~~\text{and}~~~~~
    \reg(\w) = \kl{q(\theta|\w)}{p(\theta)}
    .
\end{equation}
The loss $\loss$ above is commonly known as the negative cross entropy.
Practitioners commonly choose the distributions $p$ and $q$ to be conjugate members of the exponential family,
in which case $\loss$ and $\reg$ will have closed form representations.
The ERM formulation has two main advantages.
First, the $\lambda$ hyperparameter directly controls the strength of the prior distribution and can be tuned via cross validation.
Second, we can apply standard tools from ERM to VI,
which seemingly hasn't been done before.
For example, all the previous methods for distributed ERM can now be directly applied to VI.

The equivalence between \eqref{eq:vi:wvi:kl} and \eqref{eq:vi:wvi:erm} follows because 
\begin{align}
    &\kl{q(\theta|\w)}{p(\theta|Z)}
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log\frac{q(\theta|\w)}{p(\theta|Z)} \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log q(\theta|\w)\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(\theta|Z) \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log q(\theta|\w)\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log \frac{p(Z|\theta)p(\theta)}{p(Z)} \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(Z|\theta) \dd\theta
   +\int_{\theta\in\Theta} q(\theta|\w)\log p(Z) \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(Z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log \prod_{\z\in Z}p(\z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\sum_{\z\in Z}\int_{\theta\in\Theta} q(\theta|\w)\log p(\z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \reg(\w)
   +\sum_{\z\in Z} \loss(\z;\w)
   +\log p(Z)
\end{align}
Finally, since the term $\log p(Z)$ does not depend on $\w$ it can be removed from the optimization in \eqref{eq:vi:wvi:erm}. 

%we define the \defn{evidence lower bound} (ELBO) function as
%\begin{align}
    %\ELBO(\w) = \E\log p(\theta,\z) - \E\log q(\theta|\w)
    %%\ELBO(\w) &= \E\log p(\z|\theta) + \E\log p(\theta) - \E\log q(\theta|\w)
    %%\\
    %%&= -\KL{q(\theta|\w)}{p{\theta|Z}} + \log p(\z)
%\end{align}
%where the expectations are taken with respect to $q$.
%The variational parameters are then given by
%\begin{equation}
    %\wvi = \argmax_{\w\in\W} \ELBO(\w)
    %.
%\end{equation}
%The justification for this optimization is that maximizing the $\ELBO$ is equivalent to minimizing the KL-divergence between $q$ and $p$ \citep{blei2017variational}.
%The optimization is traditionally solved in the single machine setting using coordinate ascent.
%is used to approximate the posterior,
%and the variational parameter $\wvi$ is given by minimizing the KL divergence: 
%\begin{align}
    %\label{eq:vi:wvi}
    %\wvi 
    %&= \argmin_{\w\in\W} \kl{\q{\theta | \w}}{\p{\theta | Z}}
    %%\\
    %%&= \argmin_{\w\in\W} \E_q \q{\theta | \w} \log\frac{\q{\theta|\w}}{\p{\theta|Z}}
    %%\wvi = \argmin_{\w\in\W} \int_{\theta\in\Theta}\q{\theta | \w}\log\frac{\q{\theta | \w}}{\p{\theta | Z}}\dd\theta
    %.
%\end{align}
%To make the optimization in \eqref{eq:vi:wvi} tractable, we define the \defn{evidence lower bound} ($\ELBO$) function
%The most common form of VI is the mean field approximation.
%Here, the $q$ distribution factorizes as
%\begin{equation}
    %\q{\theta|\w} = \prod_{i=1}^d \qi{\theta^{(i)}|\w^{(i)}}
%\end{equation}
%where the superscript ${}^{(i)}$ denotes the $i$th component in the vector and
%the $q^{(i)}$ are univariate distributions.
%It is further common to assume that the $q^{(i)}$ are in the exponential family.

We now present the three methods for distributed VI that fit our framework.
\citet{broderick2013streaming} proposed the first method, 
\defn{streaming distributed asynchronous Bayes} (SDA-Bayes). 
SDA-Bayes is effectively a form of parameter averaging (see Section \ref{sec:merge:ave}) that also updates the regularization. 
Each local machine calculates the variational approximation $q(\theta|\wvi_i)$ locally,
then the merged parameters are given by
\begin{equation}
    \wsda = (1-m)\wvi_0+\sum_{i=1}^m\wvi_i
\end{equation}
where $\wvi_0$ denotes the hyper parameters of the prior distribution $p(\theta)$,
which is assumed to be in the same exponential family as the variational approximation $q(\theta|\wvi_i)$.

The merge formula for SDA-Bayes has the following probabilistic justification.
We can factor the global posterior into local posteriors and apply the exponential family assumption to get
\begin{align}
    p(\theta|Z) 
    &= p(\theta|Z_1,...,Z_m)
    \\
    &\propto \left( \prod_{i=1}^m p(Z_i|\theta) \right) p(\theta)
    \\
    &\propto \left( \prod_{i=1}^m \frac{p(\theta|Z_i)}{p(\theta)} \right) p(\theta)
    \\
    &= p(\theta)^{1-m} \prod_{i=1}^m p(\theta|Z_i)
    \\
    &\approx p(\theta)^{1-m} \prod_{i=1}^m q(\theta|\wvi_i)
    \\&=
    h(\theta)\exp\left(\trans{\left((1-m)\wvi_0+\sum_{i=1}^m\wvi_i\right)} T(\theta) - (1-m)\psi(\wvi_0)-\sum_{i=1}^m\psi(\wvi_i)\right)
    \\&\approx
    h(\theta)\exp\left(\trans{\left((1-m)\wvi_0+\sum_{i=1}^m\wvi_i\right)} T(\theta) - \psi\left((1-m)\wvi_0-\sum_{i=1}^m\wvi_i\right)\right)
    \\&=q(\theta|\wsda)
\end{align}
SDA-Bayes unfortunately provides no insight into how $\wsda$ compares to the single machine oracle VI parameters $\wvi$.
With parameter averaging, at least we know that the variance shrinks at the optimal rate even if the bias does not improve,
and we can use this result to determine how many machines we can realistically use for distributed computing.
For SDA, there is no similar result.

%The remaining two methods for distributed VI are improvements to SDA-Bayes.
%Parameter averaging (and hence SDA-Bayes) fails spectacularly when the model is non-identifiable.
%This rarely occurs in the models of Section \ref{sec:merge:ave}, 
%but nonidentifiability is common in the sort of models that require variational methods.

One limitation of SDA-Bayes is that it only works when the parameters of the variational family are identifiable.
We say that the variational family is \defn{identifiable} if for all $\w_1 \ne \w_2 \in W$, $q(\theta|\w_1)\ne q(\theta|\w_2)$.
One important class of nonidentifiability is parameter symmetry.
Let $S_d$ denote the group of permutation matrices of dimension $d$ 
(called the symmetric group of order $d$),
and recall that the variational parameter space $\W=\R^d$.
We say that the parameter space exhibits \defn{symmetry} if for any $P\in S_d$, $q(\theta|\w) = q(\theta|P\w)$.
Symmetry is commonly found in mixture models, 
and it complicates their learning.
\citet{campbell2014approximate} propose an extension to SDA-Bayes called \defn{approximate merging of posteriors with symmetries} (AMPS).
The AMPS estimator is given by
\begin{equation}
\wamps = (1-m)\wvi_0+\sum_{i=1}^mP_i\wvi_i
\end{equation}
where the $P_i$ are given by
\begin{equation}
    \{P_i\} = \argmax_{P_i\in S_d} \psi\left((1-m)\wvi_0+\sum_{i=1}^mP_i\wvi_i\right)
\end{equation}
The intuition behind AMPS is that we should permute each machine's parameters in such a way that they have the highest probability of being aligned correctly.
Maximizing the log partition function $\psi$ does this.
The idea of optimizing over permutation matrices to handle symmetry non-identifiabilities is useful in general ERM, not only variational methods.
No similar result exists for merging procedures for general ERM.

A second limitation of SDA-Bayes is that it requires the prior and variational families be conjugate.
\citet{gershman2012nonparametric} introduce \defn{nonparametric variational inference} (NVI), 
which is a single machine method that does not require conjugacy.
\citet{neiswanger2015embarrassingly} propose a distributed extension to NVI that fits our framework.
The central idea is that the variational family is the mixture 
\begin{equation}
    Q^\textit{nvi}= \left\{\frac{1}{k}\prod_{i=1}^k \normal{\theta}{\mu}{\Sigma} : \mu\in\R^d, \Sigma\text{ is a $d$ dimensional covariance matrix}\right\}
\end{equation}
and $k$ is a hyperparameter determining the number of mixture components.
Since $Q^\textit{nvi}$ is a family of mixture models, 
it exhibits parameter symmetry.
Rather than optimizing over rotation matrices like AMPS,
\citet{neiswanger2015embarrassingly} choose to use a merge procedure that uses sampling to align the components.
As the details are rather complicated,
we do not describe them here.
We do note, however, that the sampling is done locally on the master machine with no communication.
The method therefore fits our framework.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Markov chain monte carlo (MCMC)}
\label{sec:merge:mcmc}

\defn{Markov chain monte carlo} (MCMC) is a stochastic method for approximating the posterior of a distribution \citep[e.g.][]{andrieu2003introduction}.
MCMC provides a way to generate samples from the posterior distribution,
and these samples can be used to calculate the expectation or mode of the distribution.
MCMC is typically slower than variational methods but more accurate.
As the number of MCMC samples approaches infinity,
the approximation error of MCMC shrinks to zero;
whereas the approximation error for variational methods is always at least some nonzero constant determined by the approximating distribution.
Because MCMC is computationally expensive,
many techniques have been developed to scale MCMC to large datasets. 
In this section, we will see that seven of these methods involve a merge procedure
\citep{
wang2013parallelizing,
minsker2014scalable,
neiswanger2014asymptotically,
wang2015parallelizing, 
white2015piecewise,
srivastava2015wasp,
nemeth2016merging,
scott2016bayes}.
Fast cross validation for MCMC samplers is an important problem with a growing body of work
\citep{marshall2003approximate,
bhattacharya2007importance,
bornn2010efficient,
held2010posterior,
vehtari2012survey,
li2016approximating}.
None of the existing fast cross validation algorithms is suitable for the distributed environment,
unlike the methods induced by our framework.

%The output of a MCMC sampler is a stream of samples $\theta_1,...,\theta_t$. 
We now formally describe the problem of merging MCMC samples.
As before, we are given a data set $Z$ that has been partitioned into $m$ equal sized sub-data sets $Z_1,...,Z_m$ each of size $n$.
We assume that the $Z_i$ are conditionally independent given the parameters $\theta$.
Then the posterior distribution can be written as
\begin{equation}
    \label{eq:mcmc:ptz}
    p(\theta|Z) 
    \propto p(\theta)p(Z|\theta)
    = p(\theta)\prod_{i=1}^m p(Z_i|\theta)
    \propto p(\theta)\prod_{i=1}^m p(\theta|Z_i)p(\theta)^{1/m}
    = \prod_{i=1}^m p(\theta|Z_i)
    %\propto p(\theta)\prod_{i=1}^m p(\theta|Z_i)p_i(\theta)
    %= \prod_{i=1}^m p(Z_i|\theta)p(\theta)^{1/m}
    ,
\end{equation}
The distribution $p(\theta|Z_i)$ is called the $i$th \defn{subposterior distribution}.
We use an MCMC sampler to create a series of $t$ samples $\sample{i}{1},...,\sample{i}{t}$ from each subposterior $i$.
The number of samples generated by each machine ($t$) need not have any relationship to the number of data points stored on each machine ($n$).
Generating these samples can be done independently on $m$ separate machines without communication.
Furthermore, any sequential MCMC sampler can be used.
The optimal choice of sampler will depend on the particular problem.
The $\merge$ procedure takes as inputs samples from the $m$ subposteriors and generates samples from the full posterior.
%Note that $\merge$ cannot simply take the union of the samples of the subposteriors,
%as this would generate samples from $\sum_{i=1}^mp(\theta|Z_i)$ rather than $\prod_{i=1}^m p(\theta|Z_i)$.
%In general, the sum and product of distributions can be arbitrarily different from each other.

There are three families of merge procedures for MCMC.
The simplest family directly merges the samples from the subposterior distributions.
The \defn{consensus monte carlo} (CMC) algorithm introduced by \citet{scott2016bayes}% 
\footnote{
    The CMC method appears to be the earliest mergeable MCMC method despite the 2016 official publication date.
    An early version of the CMC paper was first released online in 2013 and is cited by many other mergeable MCMC papers. 
    Since the earlier papers are not on arXiv or otherwise available online,
    I have chosen to cite the official journal publication.
}
is the primary example.
CMC generates a sample by a weighted average of the subposteriors' samples.
Formally, the $j$th sample from the posterior is given by
\begin{equation}
    \theta_j^{cmc} = \left(\sum_{i=1}^m \Sigma_i\right)^{-1} \sum_{i=1}^m \Sigma_i \theta_{i,j}^{mcmc}
    .
\end{equation}
Where $\Sigma_i$ is the sample covariance matrix of $Z_i$.
Like the naive averaging estimator for optimization problems,
CMC reduces the variance but not the bias of the samples.
If we construct a pseudo-parameter vector $\wmcmc_i=(\sample{i}{1},...,\sample{i}{r})$ by concatenating the samples from the subposteriors,
then we can use all the techniques of merging RLM parameters to merge samples as well.
None of the RLM merge procedures (other than averaging) have been applied to MCMC, 
so it is unclear if they would offer any benefit.

A second category of merge functions uses a nonparametric estimate of the subposterior density.
This results in significantly more complicated merge functions.
These merge functions trade better statistical performance for worse computational performance.
Instead of merging the samples directly,
an approximate posterior distribution is created by multiplying the nonparametric estimates of the subposteriors.
MCMC samples are then generated by sampling from the approximate posterior.
The first method in this catgory was given by \citet{neiswanger2014asymptotically} 
and called the \defn{nonparametric density product estimator} (NDPE).
%\cite{neiswanger2014asymptotically} also present asymptotically efficient estimators.
NDPE uses the samples from subposterior $i$ to approximate the subposterior density using the \defn{kernel density estimator} (KDE) with a Gaussian kernel:
\begin{equation}
    \label{eq:mcmc:pkde}
    \pkde{\theta|Z_i}
    %=
    %\frac{1}{n}\sum_{\z\in Z_i}
    %\frac{1}{h}k(\theta,\z)
    =
    \frac{1}{n}\sum_{j=1}^t
    \gaussian{\theta}{\sample{i}{j}}{h^2 \eye d}
    ,
\end{equation}
where $\eye d$ is the $d$ dimensional identity matrix and $h$ is a global bandwidth parameter for the gaussian kernel.
The full posterior is then approximated by substituting \eqref{eq:mcmc:pkde} into \eqref{eq:mcmc:ptz}:
\begin{align}
    \label{eq:mcmc:pndpe}
    %\pnp{\theta|Z}
    p^{ndpe}(\theta|Z)
    &\propto
    \prod_{i=1}^m \pkde{\theta|Z_i}
    %\\&
    =
    \frac{1}{n^m}
    \prod_{i=1}^m 
    \sum_{j=1}^t
    \gaussian{\theta}{\sample{i}{j}}{h^2 \eye d}
    %\\
    %&\propto
    %\sum_{z_1\in Z_1}
    %\dots
    %\sum_{z_m\in Z_m}
    %\gaussian{\theta}{z_{\{z_1,...,z_m\}}}{h^2 \eye d}
%\end{align}
%where
%\begin{align}
    %\bar\theta_{\{z_1,...,z_m\}} 
    %&= 
    %\frac{1}{m}\sum_{z_i\in\{z_1,...,z_m\}} z_i
    %,
    %\\
    %w_{\{z_1,...,z_m\}} 
    %&=
    %\prod_{i=1}^m\gaussian{z_i}{\bar\theta_{\{z_1,...,z_m\}}}{h^2\eye d}
\end{align}
Naively sampling from \eqref{eq:mcmc:pndpe} is computationally expensive as there $n^m$ mixture components.
Many of these components turn are redundant, however, 
and the NDPE samples from a mixture of only $mt$ components,
which is tractable.
The advantage of nonparametric estimators is that they are unbiased when $n$ is fixed and $m\to\infty$,
but the disadvantage is that they take a long time to converge.
\citet{neiswanger2014asymptotically} also propose the \defn{semiparametric density product estimator} (SDPE). 
SDPE approximates the subposterior distributions as the product of a gaussian distribution and the KDE of \eqref{eq:mcmc:pkde}.
SDPE exhibits both faster convergence and asymptotic efficiency.
\citet{wang2013parallelizing} propose an improvement to the SDPE called the \defn{weierstrass sampler} (WS).%
\footnote{
    \citet{wang2013parallelizing} has an earlier citation date than \citet{neiswanger2014asymptotically}, but was actually created later.
    \citet{neiswanger2014asymptotically} first appeared on arXiv in 2013 and later was published in a journal in 2014.
    The improvements of \citet{wang2013parallelizing} were released on arXiv between these two dates.
}
The WS method approximates the subposteriors using the weierstrass transform of the KDE,
which is the convolution (instead of the product) of a gausian distribution and KDE.
\citet{nemeth2016merging} report that using a gaussian process to approximate the subposteriors has improved empirical performance compared to both the SDPE and WS methods,
although they provide little theoretical justification for this claim.
\citet{wang2015parallelizing} introduce the \defn{parallel aggregation random trees} (PART) merging method.
The PART algorithm uses a $k$d-tree to nonparametrically represent the subposterior distributions. 
This has the advantage are that there is no kernel hyperparameter that needs tuning.
The PART algorithm is the only method that provides finite sample guarantees on the quality of the emitted samples.
Finally, \citet{white2015piecewise} study the NPDE method in the more general setting of \defn{approximate baysian computation} (ABC) where the likelihood function $p(\z|\theta)$ is either unknown or too expensive to compute.

The final category of samplers is based on taking the geometric median or mean of the subposteriors.
The earliest example in this category is due to \citet{minsker2014scalable} 
and called the \defn{median of subposterior measures} (MSM).
The MSM embeds the subposteriors into a reproducing kernel hilbert space.
The approximated posterior is the median of the subposteriors with respect to the distance function in the hilbert space.
\citet{srivastava2015wasp} propose the \defn{waserstein posterior} (WASP) method.
WASP returns the barycenter of the subposterior distributions with respect to the wasserstein distance (the continuous analog of the earth mover distance).
WASP shows that this problem can be formulated as a sparse linear program and so solved efficiently in practice.

%MSM selects the median of the subposteriors based on the hellinger distance.
%The hellinger distance is a standard distance metric between probability distributions.
%Given two distributions over $\R^d$ with density $p$ and $q$,
%the hellinger distance is defined to be
%\begin{equation}
    %h(p,q) = \sqrt{\frac 1 2 \int_{\R^d} \left(\sqrt{p(\x)} + \sqrt{q(\x)}\right)^2 \dd \x}
    %.
%\end{equation}
%The wasserstein distance is defined to be
%\begin{equation}
%\end{equation}
%and is better known as the \defn{earth mover distance} (EMD) which the probability distributions are discrete.
%The WASP method 
%\begin{equation}
    %p^{wasp}(\theta|Z) = \argmin_p w
%\end{equation}
%They show how to formulate \eqref{} as a sparse linear program,
%which makes WASP fast to calculate in practice.


%Recent work has focused on improving non-asymptotic bounds of MCMC 
%\citep[e.g.][]{latuszynski2013nonasymptotic,andrieu2015convergence,adamczak2015exponential,andrieu2016establishing}.

%\begin{theorem}
    %If $h \asymp T^{-1/d}$,
    %then
    %\begin{equation}
        %\mse{}
        %\E((\phat{\theta} - \p{\theta})^2)
    %\end{equation}
%\end{theorem}

%\citet{xu2014distributed} proposes to share a small number of moments between each machine.
%This results in the machines not being truely independent and so the CV estimates will be biased.

%\citet{sisson2011likelihood} provides a tutorial on MCMC methods in likelihood free \defn{approximate bayesian optimization} (BAC).
%
%\citet{welling2009herding} introduced \defn{herding},
%which is a method of generating samples without using randomness.
%
%\citet{meeds2015optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\fixme{}
%\section{Examples}
%
%\subsection{The Free Monoid}
%
%\subsection{Moment Estimators}
%
%\subsection{Cupulae}
%
%\subsection{Empirical Characteristic function}
%\cite{yu2004empirical}
%
%
%\subsection{Information Theoretic Metric Learning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliography{bibfile}

\end{document}
