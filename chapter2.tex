\documentclass[thesis.tex]{subfiles}

\newcommand{\set}[1]{\mathcal {#1}}
\newcommand{\sized}[1]{\tilde \set {#1}}
\newcommand{\mappend}[1]{\oplus_{\set {#1}}}
\newcommand{\mempty}[1]{\epsilon_{\set {#1}}}
\newcommand{\monoid}[1]{(\set {#1}, \mappend {#1}, \mempty {#1})}

\newcommand{\mse}[1]{}

\newcommand{\eye}[1]{I_{#1}}
\newcommand{\p}[1]{p({#1})}
\newcommand{\psup}[2]{\hat p^{#1}({#2})}
\newcommand{\phat}[1]{\psup{}{#1}}
\newcommand{\pkde}[1]{\psup{\text{kde}}{#1}}
\newcommand{\pnp}[1]{\psup{\text{NWX,nonparametric}}{#1}}
\newcommand{\psp}[1]{\psup{\text{NWX,semiparametric}}{#1}}
\newcommand{\gaussian}[3]{\mathcal N({#1};{#2},{#3})}

\newcommand{\wridge}{\hat\w^\textit{ridge}}
\newcommand{\wridgep}{\hat\w^\textit{ridge,par}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\chapter{Chapter 2}

From a certain perspective, everything in this dissertation could be described as ``obvious.''
One of my main contributions is to formalize the obvious.
This gives the practitioner a way to easily and systematically derive new algorithms.
Despite all of these techniques being obvious,
as we shall see,
they are not widely used in practice or research.

\begin{definition}
    A \emph{monoid} is a tuple $(\set A, \mappend A, \mempty A)$,
    where $\set A$ is a set,
    $\mappend A : \set A \times \set A \to \set A$ is a binary operation,
    and $\mempty A \in \set A$ if it obeys the following two laws:
    \begin{align}
    \tag{associativity}
    \forall a_1, a_2, a_3 \in \set A. & (a_1 \mappend A a_2) \mappend A a_3 &= a_1 \mappend A (a_2 \mappend A a_3)
    \\
    \tag{identity}
    \end{align}
\end{definition}

\begin{definition}
    A \emph{homomorphism} between two monoids $\monoid A$ and $\monoid B$ is a function  $f: \set A \to \set B$ satisfying
    \begin{equation}
        f(a_1 \mappend A a_2) = f(a_1) \mappend B f(a_2)
    \end{equation}
\end{definition}

\begin{definition}
    A \emph{learning algorithm} is a function from $\{\set Z\} \to \set W$.
    We call a learning algorithm \emph{homomorphic} if there exists an operation $\mappend W$ such that $\monoid W$ is a monoid and $A$ is a homomorphism.
\end{definition}

%\begin{remark}
    %For any monoidal parameter space $\monoid W$, we can define the sized monoid %$\monoid {\sized W}$ as follows.
%\end{remark}

\begin{example}
    Sized monoid
\end{example}

\begin{example}
    product monoid
\end{example}

\begin{example}
    trivial learner, bad runtime properties
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: sufficient statistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ridge regression}
\label{sec:ridge}

%Recall that in empirical risk minimization (ERM) we solve the following equation
%\begin{equation}
    %\wmle = \argmin_{\w\in\W} \sum_{\z\in\Z} \loss(\z;\w) + \reg(\w)
    %,
%\end{equation}
%where $\W$ is the space of parameters, 
%$\Z$ is the space of data points

%We assume that data is generated by the model
%\begin{equation}
    %y \sim \trans\w\x + \epsilon,
%\end{equation}
%where $\x$
%We are given a set of data points $Z\subset \X\times\Y$,
%and our goal is to find a linear function $f : \X \to \Y$ that best fits the data.

Ridge regression is a special case of ERM using the squared loss and squared $L_2$ norm.
This combination of loss and norm have good computational and theoretical properties,
which makes ridge regression one of the most widely used and studied statistical algorithms.
The important property of ridge regression for our purposes is that it can be solved for exactly in an embarrassingly parallel fashion.

Ridge regression solves the optimization
\begin{equation}
    \label{eq:ridge:ridge}
    \wridge = \argmin_{\w\in\R^d} \sum_{(\x,y)\in Z} (y-\trans \w \x)^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
It is common to use matrix notation to simplify Equation \eqref{eq:ridge:ridge}.
Let $X=(\x_1,...,\x_{mn})$ be the $d\times mn$ matrix of covariates stacked horizontally,
and let $\y=(y_1,...,y_{mn}\trans)$ be the corresponding $d$ dimensional vector of response variables.
Then \eqref{eq:ridge:ridge} can be rewritten as
\begin{equation}
    \wridge = \argmin_{\w\in\R^d} \ltwo{\y - \trans \w X}^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
We can solve this equation by taking the derivative inside the $\argmax$ and setting it to zero.
The result has closed form solution
\begin{equation}
    \wridge = (\trans X X + \lambda I)^{-1}\trans X\y
    .
\end{equation}
The matrix products take time $O(mnd^2)$ and the inverse takes time $O(d^3)$.
Whenever $mn \gtgt d$, the products dominate the runtime and should be parallelized.

Because ridge regression has a closed form solution,
there is a simple embarrassingly parallel algorithm to solve it.
Each local machine $i$ calculates the statistics
\begin{equation}
    A_i = \trans X_i X_i
    ,~~~~~\text{and}~~~~~
    B_i = \trans X_i \y_i
    .
\end{equation}
Then the merge procedure calculates
\begin{equation}
    \label{eq:ridge:ridgep}
    \wridgep = \left(\sum_{i=1}^m A_i + \lambda I\right)^{-1} \sum_{i=1}^m B_i
    .
\end{equation}
By definition of $A_i$ and $B_i$,
Equations \eqref{eq:ridge:ridge} and \eqref{eq:ridge:ridgep} are the same.

Despite the simplicity, ridge regression remains an actively studied problem.
\cite{wang2016deco} propose a distributed algorithm for the case when $d > mn$.
\cite{meng2014lsrn} propose a distributed solver that works in all cases.
\cite{gascoÃÅn2017privacy} proposes a distributed solver that maintains differential privacy guarantees.
\cite{zhang2013divide,zhang2015divide} dnc kernel rr.
\cite{szabo2015twostage,szabo2016learning} uses kernel ridge regression to solve the distribution regression problem (which is a generalization of multiple instance learning).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Principle component analysis}

Principle component analysis (PCA) is a popular technique for dimensionality reduction.
The main idea was introduced by \cite{pearson1901liii}, 
and \cite{hotelling1933analysis} first introduced the term principle components.

\begin{equation}
    \w = \argmin_\w \ltwo{\x - \w\pinv\w\x}^2
\end{equation}

Modern research into PCA can be divided into two categories.
The first category of research proposes methods for determining the target dimension.
Many practitioners use heuristic techniques of ``what looks good enough''.
A number of more disciplined approaches make distributional assumptions.
The most general technique is to use cross validation.
Unfortunately, this is also the most expensive technique computationally.
Considerable work has been done to improve both the theoretical guarantees of cross validation and improve its runtime
\citep{wold1978cross,eastment1982cross,krzanowski1987cross,mertens1995efficient,diana2002cross,engelen2004fast,josse2012selecting,camacho2012cross}.

Another vein of work improves the robustness of PCA.
\cite{collins2002generalization} generalizes PCA to other exponential family distributions.
\cite{ding2004k} shows a close relationship between PCA and $k$-means.

A final vein of work improves the runtime of PCA.
One of the simplest approaches is to subsamples the data set into a so-called \emph{core set},
then run PCA on this smaller data set.
If the core set is constructed appropriately,
then the result will be a good approximation of the PCA on the entire data set.
\cite{garber2015fast} provides the state-of-the-art core set generation method
along with an excellent survey of prior work.
A number of works deal with the distributed environment.
\cite{bai2005principal} distributed PCA that doesn't work in this framework.
\cite{schizas2015distributed} use ADMM for distributed PCA.
\cite{liu2016decentralized} uses ADMM for distributed clustering.
\cite{kannan2014princople} provide a method for PCA that requires only $O(1)$ rounds of communication.
This method is not suitable for our framework, however, because we require exactly 1 round of communication.

The data points are assumed to be in $\R^d$,
and we are given a target dimension $k<\!\!<d$ to project the data onto.
Let $X$ denote the $mn\times d$ input data matrix.
We will assume without loss of generality that the rows of $X$ have zero mean.
Then calculate the singular value decomposition 
\begin{equation}
    P = UD\trans V
\end{equation}


\cite{liang2013distributed} the good stuff.
\cite{qu2002principal} works in this framework, but not approximation guarantees.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exponential family}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Naive Bayes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{High dimensional logistic regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: non-quadratic empirical risk minimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Averaging}

The simplest and most popular non-interactive estimator is the averaging estimator:
\begin{equation}
\wave = \frac{1}{m}\sum_{i=1}^m \wmle_i
.
\end{equation}
Previous analysis of $\wave$ makes a number of limiting assumptions.
\citet{mcdonald2009efficient} analyze $\wave$ in the special case of L2 regularized maximum entropy models.
They provide concentration inequalities for the estimation error,
showing that the variance $\ltwo{\E\wave-\wave}$ reduces as $O((mn)^{-1/2})$,
but that the bias $\ltwo{\wstar-\E\wave}$ reduces only as $O(n^{-1/2})$.
Their analysis uses a martingale technique that requires the radius of the dataset be independent of the size of the dataset.
This is a particularly limiting assumption as even the simple case of
normally-distributed data does not satisfy it.
\citet{zhang2012communication} provide a more general analysis showing that the mean squared error (MSE) $\E\ltwo{\wstar-\wave}{}^2$ decays as $O((mn)^{-1} + n^{-2})$.
This matches the optimal MSE of $\wmle$ whenever $m<n$.
Their analysis also requires limiting assumptions.
For example, they assume the parameter space $\W$ is bounded.
This assumption does not hold under the standard Bayesian interpretation of L2 regularization as a Gaussian prior of the parameter space.
They further make strong convexity and 8\emph{th} order smoothness assumptions which guarantee that $\wmle_i$ is a ``nearly unbiased estimator'' of $\wstar$.
Most recently, \citet{rosenblatt2016optimality} analyze $\wave$ in the asymptotic regime as the number of data points $n\to\infty$.
This analysis is more general than previous analyses, but it does not hold in the finite sample regime.
Our analysis of OWA in Section \ref{sec:anal} requires no assumptions of boundedness or convexity, holds in the finite sample regime, and shows OWA reducing both bias and variance.

\citet{zinkevich2010parallelized} show that if the training sets partially overlap each other (instead of being disjoint), then the resulting estimator will have lower bias.

\citet{zhang2012communication} provide a debiasing technique that works for any estimator.
It works as follows.
Let $r\in(0,1)$, and $Z_i^r$ be a bootstrap sample of $Z_i$ of size $rn$.
Then the bootstrap average estimator is
\begin{equation*}
\wboot = \frac{\wave-r\waver}{1-r},
\text{~~~~~where~~~~~}
\waver = \frac{1}{m}\sum_{i=1}^m \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
.
\end{equation*}
%where
%\begin{equation}
%\begin{aligned}
%\waver = \frac{1}{m}\sum_{i=1}^m \wmler_i
%,
%\text{~~~~~and~~~~~}
%%\\
%\wmler_i = \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
%.
%\\
%,
%\\
%\wboot & = \frac{\wave-r\waver}{1-r}
%.
%\end{aligned}
%\end{equation}
The intuition behind this estimator is to use the bootstrap sample to directly estimate and correct for the bias.
When the loss function is convex, $\wboot$ enjoys a mean squared error (MSE) that decays as $O((mn)^{-1}+n^{-3})$. %under similar assumptions as their analysis of $\wave$.
Theorem 2 implies that the MSE of $\wowa$ decays as $O((mn)^{-1})$ under more general conditions.
There are two additional limitations to $\wboot$.
First, the optimal value of $r$ is not obvious and setting the parameter requires cross validation on the entire data set.
Our proposed $\wowa$ estimator has a similar parameter $\lambda_2$ that needs tuning,
but this tuning happens on a small fraction of the data and always with the L2 regularizer.
So properly tuning $\lambda_2$ is more efficient than $r$.
Second, performing a bootstrap on an unbiased estimator increases the variance.
This means that $\wboot$ could perform worse than $\wave$ on unbiased estimators.
Our $\wowa$ estimator, in contrast, will perform at least as well as $\wave$ with high probability, as seen in Figure \ref{fig:contour}.
In Section \ref{sec:exp}, we show that $\wowa$ has better empirical performance than $\wboot$.

\citet{liu2014distributed} propose a more Bayesian approach inspired by \citet{merugu2003privacy}.
Instead of averaging the model's parameters,
they directly ``average the models'' with the following KL-average estimator:
\begin{equation}
    \label{eq:klave}
\wkl = \argmin_{\w\in\W} \sum_{i=1}^m \kl[\bigg]{p(\cdot;\wmle_i)}{p(\cdot;\w)}
.
\end{equation}
Liu and Ihler show theoretically that this is the best merge function in the class of functions that do not depend on the data.
Since OWA's merge depends on the data, however, this bound does not apply.
The main disadvantage of KL-averaging is computational.
The minimization in \eqref{eq:klave} is performed via a bootstrap sample from the local models,
which is computationally expensive.
%This method has three main advantages.
%First, it is robust to reparameterizations of the model.
%Second, it is statistically optimal for the class of non-interactive algorithms.
%(We show in the next section that this optimality bound does not apply to our $\wowa$ estimator due to our semi-interactive setting.)
%Third, this method is general enough to work for any model,
%whereas our proposed OWA method works only for linear models.
%The main downside of the KL-average is that the minimization has a prohibitively high computational cost.
Let $n^{kl}$ be the size of the bootstrap sample.
Then Liu and Ihler's method has MSE that shrinks as $O((mn)^{-1}+(nn^{kl})^{-1})$.
This implies that the bootstrap procedure requires as many samples as the original problem to get a MSE that shrinks at the same rate as the averaging estimator.
\citet{han2016bootstrap} provide a method to reduce the MSE to $O((mn)^{-1}+(n^2n^{kl})^{-1})$ using control variates, but the procedure remains prohibitively expensive.
Their experiments show the procedure scaling only to datasets of size $mn\approx10^4$,
whereas our experiments involve a dataset of size $mn\approx10^8$.

%An alternative definition of the $\wave$ estimator is
%\begin{equation}
%\wave = \argmin_\w \frac{1}{m}\sum_{i=1}^m \ltwo{\wmle_i-\w}^2
%\end{equation}
%It is easy to show that the two definitions are equivalent with standard calculus.

Surprisingly, \citet{zhang2013divide} show that in the special case of kernel ridge regression,
a reduction in bias is not needed to have the MSE of $\wave$ decay at the optimal sequential rate.
By a careful choice of regularization parameter $\lambda$,
they cause $\wmle_i$ to have lower bias but higher variance,
so that the final estimate of $\wave$ has both reduced bias and variance.
%This suggests that once the proper regularization parameter is known,
%there is no need for a bias reduction at all.
This suggests that a merging procedure that reduces bias is not crucial to good performance if we set the regularization parameter correctly.
Typically there is a narrow range of good regularization parameters,
and finding a $\lambda$ in this range is expensive computationally.
We show experimentally in Section \ref{sec:exp} that our method has significantly reduced sensitivity to $\lambda$.
Therefore, it is computationally cheaper to find a good $\lambda$ for our method than for the other methods discussed in this section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{One step estimators}

\citet{lee2015communication} and \citet{battey2015distributed} independently develop closed form formulas for debiasing L1 regularized least squares regressions.
They combine these debiased estimators with the averaging estimator to create a non-interactive estimator that reduces both bias and variance at the optimal rate.
OWA's advantage over these methods is that it is that it can be applied to a much larger class of problems.

\citet{jordan2016communication} develop an approach that uses a single approximate Newton step in the merge procedure.
As long as the initial starting point (they suggest using $\wave$) is within $O(\sqrt{1/n})$ of the true parameter vector,
then this approach converges at the optimal rate.
%They suggest using $\wave$ as the starting point.
When implementing Jordan et al.'s approach, we found it suffered from two practical difficulties.
First, Newton steps can diverge if the starting point is not close enough.
We found in our experiments that $\wave$ was not always close enough.
Second, Newton steps require inverting a Hessian matrix.
In Section 6, we consider a problem with dimension $d\approx7\times10^5$;
the corresponding Hessian is too large to practically invert.
%For these reasons, we do not compare against \citet{jordan2016communication} in our experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Submodular optimization}

\cite{mirzasoleiman2015distributed} 
Lots of good references.

\cite{mirzasoleiman2016distributed} 2 round GreeDi algorithm.

\citep{barbosa2015power} gives a 2 round method similar to OWA.

\cite{malkomes2015fast} gives 2 round method for $k$-centers based on submodular optimization.

\cite{barbosa2016new}

\cite{lucic2016horizontally} plus good table summarizing results.

\cite{bhaskara2016greedy}

\cite{qi2016robust} use distributed submodular optimization to partition a dataset into $m$ clusters.
Each of these clusters then learns a neural network independently and can be thought of as part of the free monoid framework.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: Bayesian Methods}

Bayesian machine learning methods estimate the \emph{posterior distribution} $\p{\theta|Z}$ of a parameter $\theta$ given data set $Z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gaussian posteriors}

The simplest method of merging posterior distributions was proposed by \cite{neiswanger2013asymptotically}.
They assume that the posterior distribution is approximately normal.
That is,
\begin{equation}
    \p{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu_i}{\hat\Sigma_i}
    .
\end{equation}
This assumption is justified by the Bernstein-von Mises theorem.%
\footnote{
    The Bernstein-von Mises theorem is also often called the Bayesian central limit theorem.
}

The final posterior density is then given by
\begin{equation}
    \phat{\theta | Z}
    =
    \prod_{i=1}^m
    \phat{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu}{\hat\Sigma}
    ,
\end{equation}
where
\begin{align}
    \hat\Sigma
    &=
    \left(
        \sum_{i=1}^m
        \hat\Sigma_i^{-1}
    \right)^{-1}
    ,
    \\
    \hat\mu
    &=
    \hat\Sigma \left(\sum_{i=1}^m \hat\Sigma^{-1} \mu_i \right)
    .
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nonparametric and semiparametric merging}

Recent work has focused on improving non-asymptotic bounds of MCMC 
\citep[e.g.][]{latuszynski2013nonasymptotic,andrieu2015convergence,adamczak2015exponential,andrieu2016establishing}.

\cite{neiswanger2014asymptotically} also present asymptotically efficient estimators.
We can write the kernel density estimate of the posterior as
\begin{equation}
    \pkde{\theta|Z}
    =
    \frac{1}{Z}\sum_{z\in Z}
    \frac{1}{h}k(\theta,z)
    =
    \frac{1}{Z}\sum_{z\in Z}
    \gaussian{\theta}{z}{h^2 \eye d}
    ,
\end{equation}
where $\eye d$ is the $d$ dimensional identity matrix.
\begin{align}
    \pnp{\theta|Z}
    &=
    \prod_{i=1}^m \pkde{\theta|Z_i}
    \\
    &=
    \frac{1}{n^m}
    \prod_{i=1}^m 
    \sum_{z\in Z_i}
    \gaussian{\theta}{z}{h^2 \eye d}
    \\
    &\propto
    \sum_{z_1\in Z_1}
    \dots
    \sum_{z_m\in Z_m}
    \gaussian{\theta}{z_{\{z_1,...,z_m\}}}{h^2 \eye d}
\end{align}
where
\begin{align}
    \bar\theta_{\{z_1,...,z_m\}} 
    &= 
    \frac{1}{m}\sum_{z_i\in\{z_1,...,z_m\}} z_i
    ,
    \\
    w_{\{z_1,...,z_m\}} 
    &=
    \prod_{i=1}^m\gaussian{z_i}{\bar\theta_{\{z_1,...,z_m\}}}{h^2\eye d}
\end{align}

\begin{theorem}
    If $h \asymp T^{-1/d}$,
    then
    \begin{equation}
        \mse{}
        \E((\phat{\theta} - \p{\theta})^2)
    \end{equation}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\fixme{}
%\section{Examples}
%
%\subsection{The Free Monoid}
%
%\subsection{Moment Estimators}
%
%\subsection{Cupulae}
%
%\subsection{Empirical Characteristic function}
%\cite{yu2004empirical}
%
%
%\subsection{Information Theoretic Metric Learning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibfile}

\end{document}
