\documentclass[thesis.tex]{subfiles}

\newcommand{\TO}{{\bfseries to}~}
\newcommand{\set}[1]{\mathcal {#1}}
\newcommand{\sized}[1]{\tilde \set {#1}}
\newcommand{\mappend}[1]{\oplus_{\set {#1}}}
\newcommand{\mempty}[1]{\epsilon_{\set {#1}}}
\newcommand{\monoid}[1]{(\set {#1}, \mappend {#1}, \mempty {#1})}
\newcommand{\semigroup}[1]{(\set {#1}, \mappend {#1})}
\newcommand{\model}{\hat{\text{model}}}

\newcommand{\riskstar}{{\textrm{err}^*}}
\newcommand{\risktilde}{\widetilde{\textrm{err}}}
\newcommand{\riskhat}{\widehat{\textrm{err}}}

\newcommand{\cv}{\mkprocedure{cv}}
\newcommand{\fastcv}{\mkprocedure{fast\_cv}}
\newcommand{\distcv}{\mkprocedure{dist\_cv}}
\newcommand{\greedy}{\mkprocedure{greedy}}
\newcommand{\GreeDi}{\mkprocedure{GreeDi}}

\newcommand{\mse}[1]{}

\newcommand{\eye}[1]{I_{#1}}
\newcommand{\q}[1]{q\left({#1}\right)}
\newcommand{\qi}[1]{q^{(i)}\left({#1}\right)}
\newcommand{\p}[1]{p\left({#1}\right)}
\newcommand{\psup}[2]{\hat p^{#1}({#2})}
\newcommand{\phat}[1]{\psup{}{#1}}
\newcommand{\pkde}[1]{\psup{\text{kde}}{#1}}
\newcommand{\pnp}[1]{\psup{\text{NWX,nonparametric}}{#1}}
\newcommand{\psp}[1]{\psup{\text{NWX,semiparametric}}{#1}}
\newcommand{\prior}[1]{p_0\left({#1}\right)}
\newcommand{\gaussian}[3]{\mathcal N({#1};{#2},{#3})}

\newcommand{\wprefix}{{\hat\w_\textit{prefix}}{}}
\newcommand{\wsuffix}{{\hat\w_\textit{suffix}}{}}
\newcommand{\wridge}{\hat\w^\textit{ridge}}
\newcommand{\wridgep}{\hat\w^\textit{ridge,par}}
\newcommand{\wvi}{\hat\w^\textit{vi}}
\newcommand{\wsda}{\hat\w^\textit{sda}}
\newcommand{\wamps}{\hat\w^\textit{amps}}
\newcommand{\wgreedy}{\hat\w^\textit{greedy}}
\newcommand{\wGreeDi}{\hat\w^\textit{GreeDi}}
\newcommand{\wpca}{\hat\w^\textit{pca}}
\newcommand{\wlbk}{\hat\w^\textit{lbk}}

\newcommand{\ELBO}{\text{ELBO}}
\newcommand{\Pdist}{\mathcal P}
\newcommand{\Pexp}{\Pdist^\textit{exp}}
\newcommand{\GO}[2]{\text{GO}_{{#1},{#2}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\chapter{Chapter 2}

%Our model of computation is closely related to the \emph{map reduce class} (MRC) introduced by \cite{karloff2010model}.
%They define the class $MRC^i$
\cite{karloff2010model} introduce the \emph{map reduce class} model of computation as an alternative to the PRAM model for analyzing algorithms.
Specifically, they define $MRC^i$ to be the class of functions computable on a MapReduce architecture with $O(\log^i n)$ rounds of computation.
Our model of computation is a subset of $MRC^0$,
however we require exactly one round of computation rather than a constant number of rounds.

\noindent
%A machine learning model consists of a set of data points $\Z$,
%a set of parameters $\W$,
%and a function $A : \{\Z\} \to \W$ that attempts to find the ``best'' set of parameters for the input data set.
In machine learning, we are given a set of data points $Z$

%From a certain perspective, everything in this dissertation could be described as ``obvious.''
%One of my main contributions is to formalize the obvious.
%This gives the practitioner a way to easily and systematically derive new algorithms.
%Despite all of these techniques being obvious,
%as we shall see,
%they are not widely used in practice or research.

\ignore{
\subsubsection{Algebra definitions and examples}

%\begin{definition}
    A \defn{semigroup} is a tuple $\semigroup A$
    where $\set A$ is a set and
    $\mappend A : \set A \times \set A \to \set A$ is an associative binary operation.
    That is, for all $a_1, a_2, a_3 \in \set A$, 
    \begin{equation}
    (a_1 \mappend A a_2) \mappend A a_3 = a_1 \mappend A (a_2 \mappend A a_3)
    .
    \end{equation}
    When clear from context, we will often refer to the set $\set A$ by itself as a semigroup and drop the subscript from $\mappend{}$.
%\end{definition}
%
%\begin{definition}
    A \defn{homomorphism} between two semigroups $\semigroup A$ and $\semigroup B$ is a function  $f: \set A \to \set B$ satisfying
    \begin{equation}
        f(a_1 \mappend A a_2) = f(a_1) \mappend B f(a_2)
        .
    \end{equation}
%\end{definition}

\begin{example}
\end{example}

\begin{example}
    Every vector space is a semigroup with vector addition as the binary operation.
    Linear functions are homomorphisms. 
\end{example}

\subsubsection{Learning algorithms}

%\begin{definition}
    A \defn{learning algorithm} is a function from $\{\set Z\} \to \set W$.
    We call a learning algorithm \emph{homomorphic} if there exists an operation $\mappend W$ such that $\monoid W$ is a monoid and $A$ is a homomorphism.
%\end{definition}

%\begin{remark}
    %For any monoidal parameter space $\monoid W$, we can define the sized monoid %$\monoid {\sized W}$ as follows.
%\end{remark}

%\begin{example}
    %Sized monoid
%\end{example}
%
%\begin{example}
    %product monoid
%\end{example}
%
%\begin{example}
    %trivial learner, bad runtime properties
%\end{example}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We typically measure the quality of an estimator by its \defn{risk},
which is defined to be
\begin{equation}
    \riskstar = \E \Loss(\w,Z)
\end{equation}
where the expectation is taken with respect to the data $Z$.
In most situations, the distribution of $Z$ is unknown and so the true risk cannot be computed.
The \defn{empirical risk} is an approximation to the true risk given by
\begin{equation}
    \risktilde = \Loss(\what,Z)
\end{equation}
where $\what$ now depends on $Z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Parallel and distributed learning}

There are many frameworks for distributed machine learning.

\begin{algorithm}
    \caption{\mkprocedure{dist\_learn}(learning algorithm $f$, data sets $Z_i$)}
    \vspace{0.1in}
    prerequisite: each machine $i$ has dataset $Z_i$ stored locally
    \begin{algorithmic}[1]
        \State each machine:
        \State ~~~~~computes $\what_i = f(Z_i)$
        \State ~~~~~transmits $\what_i$ to the master
        \State master machine:
        \State ~~~~~$\what = merge\{\what_i\}_{i=1}^m$
    \end{algorithmic}
\end{algorithm}

\subsubsection{Differential privacy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Cross validation}

Cross validation is a method for estimating the quality of an estimator.

\citet{joulani2015fast} provides a tree based fast cross validation algorithm for incremental learning algorithms.

\begin{algorithm}
    \caption{\cv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State partition $Z$ into $k$ disjoint sets $Z_1,...,Z_k$
        \vspace{0.1in}
        \For {$i \in \{1,...,k\}$}
            \State $\what_{-i} = f(Z - Z_i)$ 
            \State $\riskhat_i = \Loss(\model_i,Z_i)$
        \EndFor
        \vspace{0.1in}
        \State \Return $\frac 1 k \sum_{i=1}^k \riskhat_i$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\fastcv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State partition $Z$ into $k$ equally sized disjoint sets $Z_1,...,Z_k$
        %\State
        \vspace{0.1in}
        %\State // calculate local models
        \For {$i = 1$ \TO $k$}
            \State $\what_{i} = f(Z_i)$ 
        \EndFor
        \vspace{0.1in}
        %\State // calculate prefixes
        \For {$i = 1$ \TO $k$}
            \State $\wprefix_{,i} = \wprefix_{,i-1} \mappend{} \what_i$ 
        \EndFor
        \vspace{0.1in}
        %\State // calculate suffixes
        \For {$i = k$ \TO $1$}
            \State $\wsuffix_{,i} = \wsuffix_{,i+1} \mappend{} \what_i$ 
        \EndFor
        \vspace{0.1in}
        %\State // merge models and calculate estimated risk
        \For {$i = 1$ \TO $k$}
            \State $\what_{-i} = \wprefix_i \mappend{} \wsuffix_i$
            \State $\riskhat_i = \Loss(\what_{-i},Z_i)$
        \EndFor
        \vspace{0.1in}
        \State \Return $\frac 1 k \sum_{i=1}^k \riskhat_i$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{\distcv(learning algorithm $f$, data set $Z$, number of folds $k$)}
    \vspace{0.1in}
    prerequisite: each machine $i$ has dataset $Z_i$ stored locally
    \begin{algorithmic}[1]
        \State each machine $i$:
        \State ~~~~~calculates $\what_i = f(Z_i)$
        \State ~~~~~broadcasts $\what_i$ to each other machine
        \State each machine $i$:
        \State ~~~~~computes $\w_{-i} = merge\{\w_1,...,\w_{i-1},\w_{i+1},...,\w_k\}$
        \State ~~~~~computes $\riskhat_i = \Loss(\what_{-i},Z_i)$
        \State ~~~~~transmits $\riskhat_i$ to the master
        \State the master machine:
        \State ~~~~~computes $\riskhat = \frac 1 k \sum_{i=1}^k \riskhat_i$
        \State ~~~~~\Return $\riskhat$
    \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Online learning}

\begin{algorithm}
    \caption{\mkprocedure{add1dp}(learning algorithm $f$, model $\what$, data point $z$)}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State \Return $\what \mappend{} f(z)$
    \end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: estimators with closed form solutions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ridge regression}
\label{sec:ridge}

%Recall that in empirical risk minimization (ERM) we solve the following equation
%\begin{equation}
    %\wmle = \argmin_{\w\in\W} \sum_{\z\in\Z} \loss(\z;\w) + \reg(\w)
    %,
%\end{equation}
%where $\W$ is the space of parameters, 
%$\Z$ is the space of data points

%We assume that data is generated by the model
%\begin{equation}
    %y \sim \trans\w\x + \epsilon,
%\end{equation}
%where $\x$
%We are given a set of data points $Z\subset \X\times\Y$,
%and our goal is to find a linear function $f : \X \to \Y$ that best fits the data.

Ridge regression is a special case of ERM using the squared loss and squared $L_2$ norm.
This combination of loss and norm have good computational and theoretical properties,
which makes ridge regression one of the most widely used and studied statistical algorithms.
The important property of ridge regression for our purposes is that it can be solved for exactly in an embarrassingly parallel fashion.

Ridge regression solves the optimization
\begin{equation}
    \label{eq:ridge:ridge}
    \wridge = \argmin_{\w\in\R^d} \sum_{(\x,y)\in Z} (y-\trans \w \x)^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
It is common to use matrix notation to simplify Equation \eqref{eq:ridge:ridge}.
Let $X=(\x_1,...,\x_{mn})$ be the $d\times mn$ matrix of covariates stacked horizontally,
and let $\y=(y_1,...,y_{mn}\trans)$ be the corresponding $d$ dimensional vector of response variables.
Then \eqref{eq:ridge:ridge} can be rewritten as
\begin{equation}
    \wridge = \argmin_{\w\in\R^d} \ltwo{\y - \trans \w X}^2 + \lambda\ltwo{\w}^2
    .
\end{equation}
We can solve this equation by taking the derivative inside the $\argmax$ and setting it to zero.
The result has closed form solution
\begin{equation}
    \wridge = (\trans X X + \lambda I)^{-1}\trans X\y
    .
\end{equation}
The matrix products take time $O(mnd^2)$ and the inverse takes time $O(d^3)$.
Whenever $mn \gtgt d$, the products dominate the runtime and should be parallelized.

Because ridge regression has a closed form solution,
there is a simple embarrassingly parallel algorithm to solve it.
Each local machine $i$ calculates the statistics
\begin{equation}
    A_i = \trans X_i X_i
    ,~~~~~\text{and}~~~~~
    B_i = \trans X_i \y_i
    .
\end{equation}
Then the merge procedure calculates
\begin{equation}
    \label{eq:ridge:ridgep}
    \wridgep = \left(\sum_{i=1}^m A_i + \lambda I\right)^{-1} \sum_{i=1}^m B_i
    .
\end{equation}
By definition of $A_i$ and $B_i$,
Equations \eqref{eq:ridge:ridge} and \eqref{eq:ridge:ridgep} are the same.

Despite the simplicity, ridge regression remains an actively studied problem.
\cite{wang2016deco} propose a distributed algorithm for the case when $d > mn$.
\cite{meng2014lsrn} propose a distributed solver that works in all cases.
\cite{gascoÌn2017privacy} proposes a distributed solver that maintains differential privacy guarantees.
\cite{zhang2013divide,zhang2015divide} dnc kernel rr.
\cite{szabo2015twostage,szabo2016learning} uses kernel ridge regression to solve the distribution regression problem (which is a generalization of multiple instance learning).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exponential family distributions}
\label{sec:merge:ef}

%There are three important subfamilies of the EF that we will consider:
%linear exponential families (LEFs),
%curved exponential families (CEFs),
%and stratified/deep exponential families (SEFs/DEFs).

The \defn{exponential family} (EF) of distributions is defined to be the set $\Pexp$ of all distributions whose density can be written in the form
\begin{equation}
    \p{\z | \w} = h(\z)\exp\left(\trans{\w} T(\z) - \psi(\w)\right)
\end{equation}
where $\z\in\Z$ and $\w\in\W$.
We make no assumption on the set $\Z$, but require $\W$ to be an open subset of a hilbert space $\set H$.
We will ignore the technical details of infinite dimensional hilbert spaces and denote the inner product using $\trans{}$ in analogy with finite dimensional vectors.
The function $h : \Z\to\R$ is called the base measure,
$T : Z \to \W$ the sufficient statistic,
and $\psi : \W \to \R$ the log partition function.
Many authors require $\psi$ to be strongly convex,
but we will require only that derivative be invertible.
%Many popular distributions (e.g. Gaussian, Dirichlet, Poisson, exponential, and categorical) are in the exponential family.

%Given a prior distribution on the parameter vector $\w$,
%the \defn{maximum a posteriori estimate} (map)
%There are many introductions to exponential families,
%but the presentation in \citet{amari2016information} is especially apropos to our discussion.
%Given a data set $Z$, we can estimate the parameters of the distribution as
To perform parameter estimation using ERM,
we set the loss to the negative log likelihood 
\begin{equation}
    \loss(\z;\w) 
    = -\log\p{\z|\w}
    = -\log h(\z) - \trans\w T(\z) + \psi(\w)
    .
\end{equation}

\begin{equation}
    \reg(\w) = \log\prior\w
\end{equation}
where $\prior\w$ is the prior distribution over $\w$.
The ERM is then
\newcommand{\deriv}[2]{\frac{\dd {#1}}{\dd {#2}}}
\begin{align}
    \label{eq:ef:erm}
    %\werm = \argmin_{\w\in\W} \sum_{\z\in Z} \loss(\z;\w) + \lambda \reg(\w)
    \werm 
    &= \argmin_{\w\in\W} \sum_{\z\in Z}\left( \psi(\w) - \trans\w T(\z)\right)
    %\\
    %= \left(\deriv{}{\w}\psi\right)^{-1} \left(\frac{1}{mn} \sum_{\z\in Z} T(\z) \right)
    = \psi'^{-1} \left(\frac{1}{mn} \sum_{\z\in Z} T(\z) \right)
    .
\end{align}


\begin{method}
    When the parameter space $\W$ equals $\R^d$,
    the distribution is in the \defn{linear exponential family} (LEF).
    Local machines calculate 
    \begin{equation}
        \wmle_i
        %= \left(\deriv{}{\w}\psi\right)^{-1} \left(\frac{1}{n} \sum_{\z\in Z_i} T(\z) \right)
        = \psi'^{-1} \left(\frac{1}{n} \sum_{\z\in Z_i} T(\z) \right)
    \end{equation}
    and the merge procedure is
    \begin{equation}
        \label{eq:lef:merge}
        %\left(\deriv{}{\w}\psi\right)^{-1} 
        \psi'^{-1} 
        \left(
            %\frac{1}{m} \sum_{i=1}^m \deriv{}{\w}\psi \left(\wmle_i\right)
            \frac{1}{m} \sum_{i=1}^m \psi' \left(\wmle_i\right)
        \right)
        .
    \end{equation}
    Notice that \eqref{eq:lef:merge} simplifies to \eqref{eq:ef:erm},
    so the learning function for LEF's is an exact semigroup homomorphism.
\end{method}

When the parameter space $\W$ is a manifold in the underlying hilbert space $\set H$,
we say the distribution is in the \defn{curved exponential family} (CEF). 
The CEF was introduced by \citet{efron1975defining} and \citet{amari1982differential} and is a central topic in the field of information geometry.
The book \citet{amari2016information} provides a modern treatment of CEFs from this geometric perspective.
%When $\W$ is a $d$ dimensional manifold embedded in a $d' > d$ dimensional hilbert space.

%\begin{method}
    %When $d'$ is finite,
%\end{method}
%
%When $d'$ is infinite, other methods of estimation are required.

%Then the ERM estimate is given by
%\begin{align}
    %\wmle 
    %&= \argmax_{\w\in\W} \prod_{\z\in Z} \p{\z | \w} \p{\w}
    %\\
    %&= \argmin_{\w\in\W} \sum_{\z\in Z} \loss(\z ; \w) + \reg(\w)
%\end{align}

\cite{geiger1998graphical} and \citet{geiger2001stratified} introduced the class of \defn{stratified exponential families} (SEFs),
which is defined to be the set of all algebraic subsets of $\Pexp$.
The definition of an algebraic subset comes from algebraic geometry.
It is rather technical, so we do not review it here.
\citet{geiger2001stratified} include a number of theorems characterizing the properties of SEFs.
The most important is that all graphical models with hidden variables are SEFs (but not CEFs).
%SEFs correspond to graphical models with hidden variables.
%The original presentation of SEFs was in terms of algebraic geometry
\cite{ranganath2015deep} independently reinvented essentially the same concept in the form of \defn{deep exponential families} (DEFs).
A DEF is the distribution resulting from placing EF priors on the natural parameters of an EF distribution.
\citet{ranganath2015deep} suggest using variational methods to solve for the parameters of DEFs.
Section \ref{sec:merge:vi} discusses variational methods in detail.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bayesian classifiers}

Assume that the data space $\Z$ is decomposed into $\X\times \Y$,
where $\X$ is the space of features and $\Y$ the space of class labels.
Our goal is to learn a distribution $\p{\y ; \x}$.
We then classify according to rule
\begin{equation}
    \argmax_{\y\in\Y} \p{\y | \x}
    \end{equation}
By bayes theorem, we have that
\begin{equation}
    \p{\y | \x} = \frac{\p{\x | \y}\p{\y}}{\p{\x}}
    .
\end{equation}

In naive bayes, we assume that the data can be decomposed into $d$ dimensions,
and these dimensions are independent.
That is,
\begin{equation}
    \p{\w | \z} = \prod_{i=1}^d \psup{(i)}{\w^{(i)} | \z^{(i)}}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ad-hoc approximations to the maximum likelihood}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: non-quadratic empirical risk minimization}

When either the loss or regularization functions for ERM are non-quadratic,
no closed form solution exists.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Averaging}
\label{sec:merge:ave}

The simplest and most popular non-interactive estimator is the averaging estimator:
\begin{equation}
\wave = \frac{1}{m}\sum_{i=1}^m \wmle_i
.
\end{equation}
Previous analysis of $\wave$ makes a number of limiting assumptions.
\citet{mcdonald2009efficient} analyze $\wave$ in the special case of L2 regularized maximum entropy models.
They provide concentration inequalities for the estimation error,
showing that the variance $\ltwo{\E\wave-\wave}$ reduces as $O((mn)^{-1/2})$,
but that the bias $\ltwo{\wstar-\E\wave}$ reduces only as $O(n^{-1/2})$.
Their analysis uses a martingale technique that requires the radius of the dataset be independent of the size of the dataset.
This is a particularly limiting assumption as even the simple case of
normally-distributed data does not satisfy it.
\citet{zhang2012communication} provide a more general analysis showing that the mean squared error (MSE) $\E\ltwo{\wstar-\wave}{}^2$ decays as $O((mn)^{-1} + n^{-2})$.
This matches the optimal MSE of $\wmle$ whenever $m<n$.
Their analysis also requires limiting assumptions.
For example, they assume the parameter space $\W$ is bounded.
This assumption does not hold under the standard Bayesian interpretation of L2 regularization as a Gaussian prior of the parameter space.
They further make strong convexity and 8\emph{th} order smoothness assumptions which guarantee that $\wmle_i$ is a ``nearly unbiased estimator'' of $\wstar$.
Most recently, \citet{rosenblatt2016optimality} analyze $\wave$ in the asymptotic regime as the number of data points $n\to\infty$.
This analysis is more general than previous analyses, but it does not hold in the finite sample regime.
Our analysis of OWA in Section \ref{sec:anal} requires no assumptions of boundedness or convexity, holds in the finite sample regime, and shows OWA reducing both bias and variance.

\citet{zinkevich2010parallelized} show that if the training sets partially overlap each other (instead of being disjoint), then the resulting estimator will have lower bias.

\citet{zhang2012communication} provide a debiasing technique that works for any estimator.
It works as follows.
Let $r\in(0,1)$, and $Z_i^r$ be a bootstrap sample of $Z_i$ of size $rn$.
Then the bootstrap average estimator is
\begin{equation*}
\wboot = \frac{\wave-r\waver}{1-r},
\text{~~~~~where~~~~~}
\waver = \frac{1}{m}\sum_{i=1}^m \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
.
\end{equation*}
%where
%\begin{equation}
%\begin{aligned}
%\waver = \frac{1}{m}\sum_{i=1}^m \wmler_i
%,
%\text{~~~~~and~~~~~}
%%\\
%\wmler_i = \argmax_\w \sum_{(\x,y)\in Z_i^r} \loss(y,\trans\x\w) + \lambda \reg(\w)
%.
%\\
%,
%\\
%\wboot & = \frac{\wave-r\waver}{1-r}
%.
%\end{aligned}
%\end{equation}
The intuition behind this estimator is to use the bootstrap sample to directly estimate and correct for the bias.
When the loss function is convex, $\wboot$ enjoys a mean squared error (MSE) that decays as $O((mn)^{-1}+n^{-3})$. %under similar assumptions as their analysis of $\wave$.
Theorem 2 implies that the MSE of $\wowa$ decays as $O((mn)^{-1})$ under more general conditions.
There are two additional limitations to $\wboot$.
First, the optimal value of $r$ is not obvious and setting the parameter requires cross validation on the entire data set.
Our proposed $\wowa$ estimator has a similar parameter $\lambda_2$ that needs tuning,
but this tuning happens on a small fraction of the data and always with the L2 regularizer.
So properly tuning $\lambda_2$ is more efficient than $r$.
Second, performing a bootstrap on an unbiased estimator increases the variance.
This means that $\wboot$ could perform worse than $\wave$ on unbiased estimators.
Our $\wowa$ estimator, in contrast, will perform at least as well as $\wave$ with high probability, as seen in Figure \ref{fig:contour}.
In Section \ref{sec:exp}, we show that $\wowa$ has better empirical performance than $\wboot$.

\citet{liu2014distributed} propose a more Bayesian approach inspired by \citet{merugu2003privacy}.
Instead of averaging the model's parameters,
they directly ``average the models'' with the following KL-average estimator:
\begin{equation}
    \label{eq:klave}
\wkl = \argmin_{\w\in\W} \sum_{i=1}^m \kl[\bigg]{p(\cdot;\wmle_i)}{p(\cdot;\w)}
.
\end{equation}
Liu and Ihler show theoretically that this is the best merge function in the class of functions that do not depend on the data.
Since OWA's merge depends on the data, however, this bound does not apply.
The main disadvantage of KL-averaging is computational.
The minimization in \eqref{eq:klave} is performed via a bootstrap sample from the local models,
which is computationally expensive.
%This method has three main advantages.
%First, it is robust to reparameterizations of the model.
%Second, it is statistically optimal for the class of non-interactive algorithms.
%(We show in the next section that this optimality bound does not apply to our $\wowa$ estimator due to our semi-interactive setting.)
%Third, this method is general enough to work for any model,
%whereas our proposed OWA method works only for linear models.
%The main downside of the KL-average is that the minimization has a prohibitively high computational cost.
Let $n^{kl}$ be the size of the bootstrap sample.
Then Liu and Ihler's method has MSE that shrinks as $O((mn)^{-1}+(nn^{kl})^{-1})$.
This implies that the bootstrap procedure requires as many samples as the original problem to get a MSE that shrinks at the same rate as the averaging estimator.
\citet{han2016bootstrap} provide a method to reduce the MSE to $O((mn)^{-1}+(n^2n^{kl})^{-1})$ using control variates, but the procedure remains prohibitively expensive.
Their experiments show the procedure scaling only to datasets of size $mn\approx10^4$,
whereas our experiments involve a dataset of size $mn\approx10^8$.

%An alternative definition of the $\wave$ estimator is
%\begin{equation}
%\wave = \argmin_\w \frac{1}{m}\sum_{i=1}^m \ltwo{\wmle_i-\w}^2
%\end{equation}
%It is easy to show that the two definitions are equivalent with standard calculus.

Surprisingly, \citet{zhang2013divide} show that in the special case of kernel ridge regression,
a reduction in bias is not needed to have the MSE of $\wave$ decay at the optimal sequential rate.
By a careful choice of regularization parameter $\lambda$,
they cause $\wmle_i$ to have lower bias but higher variance,
so that the final estimate of $\wave$ has both reduced bias and variance.
%This suggests that once the proper regularization parameter is known,
%there is no need for a bias reduction at all.
This suggests that a merging procedure that reduces bias is not crucial to good performance if we set the regularization parameter correctly.
Typically there is a narrow range of good regularization parameters,
and finding a $\lambda$ in this range is expensive computationally.
We show experimentally in Section \ref{sec:exp} that our method has significantly reduced sensitivity to $\lambda$.
Therefore, it is computationally cheaper to find a good $\lambda$ for our method than for the other methods discussed in this section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{One step estimators}

\citet{lee2015communication} and \citet{battey2015distributed} independently develop closed form formulas for debiasing L1 regularized least squares regressions.
They combine these debiased estimators with the averaging estimator to create a non-interactive estimator that reduces both bias and variance at the optimal rate.
OWA's advantage over these methods is that it is that it can be applied to a much larger class of problems.

\citet{jordan2016communication} develop an approach that uses a single approximate Newton step in the merge procedure.
As long as the initial starting point (they suggest using $\wave$) is within $O(\sqrt{1/n})$ of the true parameter vector,
then this approach converges at the optimal rate.
%They suggest using $\wave$ as the starting point.
When implementing Jordan et al.'s approach, we found it suffered from two practical difficulties.
First, Newton steps can diverge if the starting point is not close enough.
We found in our experiments that $\wave$ was not always close enough.
Second, Newton steps require inverting a Hessian matrix.
In Section 6, we consider a problem with dimension $d\approx7\times10^5$;
the corresponding Hessian is too large to practically invert.
%For these reasons, we do not compare against \citet{jordan2016communication} in our experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Principle component analysis}
\label{sec:merge:pca}

\defn{Principle component analysis} (PCA) is a popular technique for dimensionality reduction.
%Given a data set $Z$ of points in $\R^d$,
%the goal of PCA is to find the subspace of dimension $k$ that captures most of the variability of the data.
%The technique was introduced by \cite{pearson1901liii}, 
%and \cite{hotelling1933analysis} first introduced the term principle components.
In this section,
we describe how PCA works on a single machine,
show that two existing techniques for distributed PCA fit our framework,
then describe the importance of fast cross validation techniques for PCA.

Let $Z$ denote the $mn\times d$ input data matrix.
The goal of PCA is to find a $d\times k$ matrix with $k \ltlt d$ such that the distance between $Z$ and $Z\w\pinv\w$ is minimized.
Formally, 
%the goal of PCA is to find a subspace of dimension $k<d$ that captures most of the variability of the data.
\begin{equation}
    \label{eq:pca:defn}
    \wpca = \argmin_{\w\in\R^{d\times k}} \ltwo{Z - Z\w\pinv\w}^2
\end{equation}
The optimization \eqref{eq:pca:defn} is non-convex, 
but the solution can be calculated efficiently via the \defn{singular value decomposition} (SVD). 
The SVD of $Z$ is
\begin{equation}
    Z = UD\trans V
    ,
\end{equation}
where $U$ is an orthogonal matrix of dimension $mn\times mn$, 
$D$ is a diagonal matrix of dimension $mn\times d$, 
and $V$ is an orthogonal matrix dimension $d\times d$.
The columns of $U$ are called the left singular vectors, 
the columns of $V$ the right singular vectors,
and the entries in $D$ the singular values.
The solution to \eqref{eq:pca:defn} is given by the first $k$ columns of $V$.
\citet{halko2011finding} provides a method for efficiently calculating approximate SVDs on a single machine when only the first $k$ singular values/vectors are needed.

\cite{qu2002principal} and \cite{liang2013distributed} introduce essentially the same algorithm for distributed PCA.
Each machine $i$ locally calculates the SVD of its local dataset $Z_i = U_i D_i \trans {V_i}$.
Let $D_i^{(k)}$ denote the $k\times k$ submatrix of $D_i$ containing the first $k$ singular values,
and $V_i^\{k\}$ denote the $d\times k$ submatrix of $V_i$ containing the first $k$ columns.
The local machines each transmit $D_i^{(k)}$ and $V_i^{(k)}$ to the master machine.
The master calculates 
\begin{equation}
    S = \sum_{i=1}^m V_i^{(k)}D_i^{(k)}\trans{V_i^{(k)}}
\end{equation}
Performing the SVD on $S$ then gives the approximate principle components of the entire data set $Z$.
\cite{qu2002principal} further provide a modification to the merge procedure that approximately centers the data,
but they do not provide any theoretical guarantees on the performance of their algorithm relative to the single machine oracle.
\cite{liang2013distributed} do not consider the possibility of centering the data,
but they do show that their algorithm is a $1+\varepsilon$ approximation of the single machine algorithm, where $\varepsilon$ depends on properties of the data and the choice of $k$.

A major difficulty in PCA (distributed or not) is selecting a good value for $k$.
There are two reasons to choose a small value of $k$.
The most obvious is computational.
When $k$ is small, future stages in the data processing pipeline will be more efficient because they are working in a lower dimensional space.
But there is a more subtle statistical reason.
When there is a large noise component in the data,
using fewer dimensions removes this noise and improves the statistical efficiency of later stages of the data pipeline.
Perhaps the simplest method of determining $k$ is the scree test \citep{cattell1966scree},
where the data's singular values are plotted and the analyst makes a subjective judgement.
More robust methods make distributional assumptions \citep{bartlett1950tests}.
Under these assumptions, the noise in the data can be estimated directly and $k$ determined appropriately.
When these distributional assumption do not hold, however, the resulting $k$ value can be arbitrarily poor.
The most robust solution uses cross validation and the PRESS statistic.
%%In general, the optimal $k$ value will depend on the task at hand.
%Many practitioners use heuristic techniques of ``what looks good enough''.
%A number of more disciplined approaches make distributional assumptions.
%The most general technique is to use cross validation.
Unfortunately, this is also the most expensive technique computationally.
Considerable work has been done to improve both the theoretical guarantees of cross validation and improve its runtime via approximations
\citep{wold1978cross,eastment1982cross,krzanowski1987cross,mertens1995efficient,diana2002cross,engelen2004fast,josse2012selecting,camacho2012cross}.
Notably, none of these fast cross validation techniques work in the distributed setting.
Thus the distributed fast cross validation technique induced by \cite{qu2002principal} and \cite{liang2013distributed} is both novel and useful.

%%Another vein of work improves the robustness of PCA.
%%\cite{collins2002generalization} generalizes PCA to other exponential family distributions.
%%\cite{ding2004k} shows a close relationship between PCA and $k$-means.
%%
%A final vein of work improves the runtime of PCA.
%One of the simplest approaches is to subsamples the data set into a so-called \emph{core set},
%then run PCA on this smaller data set.
%If the core set is constructed appropriately,
%then the result will be a good approximation of the PCA on the entire data set.
%\cite{garber2015fast} provides the state-of-the-art core set generation method
%along with an excellent survey of prior work.
%A number of works deal with the distributed environment.
%\cite{bai2005principal} distributed PCA that doesn't work in this framework.
%\cite{schizas2015distributed} use ADMM for distributed PCA.
%\cite{liu2016decentralized} uses ADMM for distributed clustering.
%\cite{kannan2014princople} provide a method for PCA that requires only $O(1)$ rounds of communication.
%This method is not suitable for our framework, however, because we require exactly 1 round of communication.

%\cite{liang2013distributed} the good stuff.
%
%Let $X$ be a $d\times j$ matrix with orthonormal columns.
%Let $\varepsilon\in(0,1]$ and $k\in\mathbb N$ satisfying 
%\begin{equation}
    %\ltwo{\wpca X - \wlbk X}^2 \le \varepsilon d(P,L(X))^2
%\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Submodular optimization}
\label{sec:merge:submodular}

Submodular functions are a class of set function that share many important properties with convex functions \citep{lovasz1983submodular}.
In particular, they are easy to optimize and have many applications. 
The maximization of submodular functions has become an important technique in the approximation of NP-hard problems \citep{krause14survey}.
%\citet{mirzasoleiman2016distributed} provide a detailed survey of recent applications in machine learning.
Applications in machine learning include clustering, sparse nonparametric regression, image segmentation, document summarization, and social network modeling \citep[see references within][]{mirzasoleiman2016distributed}.
The last five years has seen work focused on scaling up submodular optimization via distributed algorithms.
Six of these papers fit our semigroup framework
\citep{mirzasoleiman2013distributed,barbosa2015power,malkomes2015fast,bhaskara2016greedy,barbosa2016new,mirzasoleiman2016distributed}
and so induce fast cross validation procedures.
This is the first work addressing fast cross validation in submodular learning algorithms.
In this section we first define submodularity,
then introduce the distributed optimizers.

For a set function $f : \{\Z\} \to \R$,
a set $Z\subset\Z$ and an element $e\in\Z$,
we define the \defn{discrete derivative} of $f$ at $Z$ with respect to $e$ to be
\begin{equation}
    f'(e; Z) = f (Z\cup\{e\}) - f(Z)
    .
\end{equation}
We call the function $f$ \defn{monotone} if for all $e$ and $Z$, $f'(e;Z) \ge 0$.
We further call $f$ \defn{submodular} if for all $A \subseteq B \subseteq Z$ and $e\in Z-B$,
\begin{equation}
    f'(e;A)\ge f'(e;B)
    .
\end{equation}
We first consider the problem of monotone submodular maximization subject to cardinality constraints.
That is, we want to solve
\begin{equation}
    \label{eq:submodular:opt}
    \hat S =
    \argmax_{S\subseteq Z} f(S)
    ~~~~~\text{s.t.}~~~~~
    |S|\le k
    .
\end{equation}
%Many machine learning problems can be cast as a special case of submodular maximization.
Solving \eqref{eq:submodular:opt} is NP-hard in general,
so it is standard to use the greedy approximation algorithm introduced by \citet{nemhauser1978analysis}.
The procedure (which we will refer to as $\greedy$) is shown in Algorithm \ref{alg:submodular:greedy}.
The $\greedy$ algorithm is known to be a $1-1/e$ approximation algorithm\footnote{
The symbol $e$ is Euler's constant and not an approximation variable.},
and no better approximation algorithm exists unless $P=NP$ \citep{krause14survey}.
\begin{algorithm}[t]
    \caption{\greedy(data set $Z$, constraint size $k$)}
    \label{alg:submodular:greedy}
    \vspace{0.1in}
    \begin{algorithmic}[1]
        \State $S \leftarrow \{\}$
        \For {$i = 1$ \TO $k$}
            \State $\z_i \leftarrow \argmax_{\z\in Z-S} f'(\z;S)$
            \State $S \leftarrow S \cup \{\z_i\}$
        \EndFor
        \State \Return $S$
    \end{algorithmic}
\end{algorithm}

\cite{mirzasoleiman2013distributed} introduced the first method of merging independently computed solutions in their $\GreeDi$ (GREEdy DIstributed) algorithm.
$\GreeDi$ works by first running the $\greedy$ algorithm locally on each node to compute local solutions $\wgreedy_i$.
These solutions are transmitted to the master machine.
The master combines the local solutions into a set of size $km$ and reruns $\greedy$ on the combined set.
%That is, the merge procedure is given by the equation
In notation, the merge procedure is given by
\begin{equation}
    \GreeDi(\wgreedy_1,...,\wgreedy_m) = \greedy(\cup_{i=1}^m\wgreedy_i,k)
    .
\end{equation}
\cite{mirzasoleiman2013distributed} show that in the worst case,
$\GreeDi$ achieves an approximation guarantee of 
\begin{equation}
    f(\wGreeDi) \ge \frac{(1-1/e)^2}{\min\{m,k\}} f(\wstar)
    .
\end{equation}
%They also show these bounds can be improved when the set $Z$ has either an associated metric or distribution,
%which is often the case in practice.
\citet{barbosa2015power} improve the analysis of $\GreeDi$ to show that in expectation
\begin{equation}
    f(\wGreeDi) \ge \frac {1-1/e}{2} f(\wstar)
    ,
\end{equation}
which matches the guarantee of the optimal $\greedy$ centralized algorithm up to the $1/2$ constant factor.
Notably, the approximation is independent of the number of machines $m$ or the size of the problem $k$.

Subsequent work has extended the $\GreeDi$ framework to apply to more general submodular optimization problems.
\cite{malkomes2015fast} solves the $k$-centers clustering problem;
\cite{bhaskara2016greedy} solves the column subset selection problem;
and both \cite{barbosa2016new} and \citet{mirzasoleiman2016distributed} solve submodular problems with matroid, $p$-system, and knapsack constraints.
The algorithms presented in each of these papers follows the same basic pattern:
the $\greedy$ algorithm used by the local machines and the merge procedure is replaced by an alternative algorithm that is more appropriate for the new problem setting.

As with continuous statistical optimization problems,
the difficulty of submodular optimization is known to depend on the curvature of the problem
\citep{vondrak2010submodularity}.
The \defn{submodular curvature} is defined to be
\begin{equation}
    c = 1 - \min_{e\in Z} \frac{f'(e;Z-e)}{f(e)}
    .
\end{equation}
When the curvature is small, the $\greedy$ algorithm will perform better.
In particular, when the $c=0$ the problem is said to be \defn{modular} and the $\greedy$ algorithm returns an optimal solution.
\citet{vondrak2010submodularity} shows that for all $c>0$, $\greedy$ returns a $(1-e^{-c})/c$ approximate solution,
and that no better approximation is possible.
There is as yet no work discussing the relationship of curvature to the difficulty of merging local solutions.
It seems likely, however, that a bound analogous to the continuous bound provided by \citet{liu2014distributed} will hold.

%\citet{lovasz1983submodular} shows that submodular functions are closely related to convex functions.
%\subsubsection{Applications of submodular optimization}
%
%active set selection in sparse gaussian processes \citep{mirzasoleiman2016distributed},
%inference for determinental point processes \citep{mirzasoleiman2016distributed}
%
%Submodular functions have been used to create summaries of documents
%\citep{lin2004rouge,lin2010multi,lin2011class,lin2012learning}.
%\citet{tschiatschek2014learning} introduces many submodular loss functions for summarizing image data.
%They also point out that \citet{simon2007scene}
%\citet{sinha2011extractive}
%\citet{sinha2011summarization}
%\citet{denton2004selecting}
%use submodular objectives.
%\citet{kulesza2011k} uses a submodular objective to sample from determinental point processes.
%\citet{iyer2015submodular} introduces submodular point processes,
%and \citet{gotovos2015sampling} shows how to sample from these models.
%Can this be combined with the parallel MCMC procedures?
%\cite{qi2016robust} use distributed submodular optimization to partition a dataset into $m$ clusters.
%Each of these clusters then learns a neural network independently and can be thought of as part of the free monoid framework.

%Techniques in submodular optimization are widely used in machine learning.
%\cite{malkomes2015fast} uses the same framework to solve the $k$-centers clustering problem.
%They provide improved approximation guarantees that are stronger than the general submodular case.
%\cite{bhaskara2016greedy} uses the same framework on a specific class of submodular optimization problems called ``column subset selection.''
%They provide approximation guarantees for this problem that are better than the general case.
%
%\cite{barbosa2016new} considers a more general submodular optimization problem than \eqref{eq:submodular:opt}.
%In particular, they do not require the function be monotone, and they support matroid and $p$-system constraints in addition to cardinality constraints.
%\citet{mirzasoleiman2016distributed} show that their original GreeDi algorithm is also able to handle the case of non-monotone functions under these more general constraints.
%
%active set selection in sparse gaussian processes 
%inference for determinental point processes \citep{mirzasoleiman2016distributed}
%
%\subsubsection{Applications of submodular optimization}
%
%\cite{qi2016robust} use distributed submodular optimization to partition a dataset into $m$ clusters.
%Each of these clusters then learns a neural network independently and can be thought of as part of the free monoid framework.
%
%\cite{lucic2016horizontally} provides an interactive distributed algorithm,
%plus a good table summarizing results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples: Bayesian Methods}

%Bayesian machine learning methods estimate the \defn{posterior distribution} $\p{\theta|Z}$ of a parameter $\theta$ given data set $Z$.
%We assume that our data set is sampled from some distribution $p(\z|\w)$,
%and we have a prior distribution $p(\w)$.
%Then the \defn{posterior distribution} is defined to be $\p{\w|\z}$,
%and by bayes theorem we have
%\begin{equation}
    %p(\w | \z) = \frac{p(\z|\w)p(\w)}{p(\z)}
    %.
%\end{equation}
%In bayesian methods, we make a parametric assumption about the posterior distribution.

In Bayesian inference, 
we treat the data set $Z$ as observed variables and assume there is a hidden variable $\theta$ on which the data depends.
%we are given a model that includes observed variables $\z$ and hidden variables $\theta$.
Our goal is to calculate the \defn{posterior distribution} $p(\theta | Z)$.
%By convention, the notation $p(\theta | Z)$ suppresses the distribution's dependence on parameters $\w\in\W$. 
In this section we discuss three general techniques for learning the posterior.
%The first method uses a simple parametric assumption on the posterior distribution and is the Bayesian equivalent of model averaging.
We warm-up with the Bernstein-von Mises (BvM) method.
BvM uses a simple model based on a parametric approximation and sufficient statistics.
%Distributed learning of the BvM method is the Bayesian analog of parameter averaging.
Next we discuss variational inference (VI).
VI also makes a parametric approximation,
but uses a more complex optimization procedure to choose the parameters.
We shall see that the distributed methods for VI are closely related to those for ERM.
The final method is markov chain monte carlo (MCMC).
MCMC uses sampling to approximate the posterior.
The distributed methods share little in common with the optimization methods.
%We can use bayes theorem to write the posterior as 
%\begin{equation}
    %p(\theta | \z) = \frac{p(\z|\theta)p(\theta)}{p(\z)}
    %.
%\end{equation}
%The conditional probability of the evidence $p(\z|\theta)$ and the prior distribution $p(\theta)$ are typically easy to compute,
%but the evidence distribution $p(\z)$ is often computationally intractable.

%\begin{align}
    %p(\w | Z) 
    %&= \frac{p(Z|\w)p(\w)}{p(Z)}
    %\\
    %&= \frac{\prod_{i=1}^mp(Z_i|\w)p(\w)}{\prod_{i=1}^mp(Z_i)}
    %\\
    %&= \frac{\prod_{i=1}^m\frac{p(\w|Z_i)p(Z_i)}{p(\w)}p(\w)}{\prod_{i=1}^mp(Z_i)}
%\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bernstein-von Mises}

When the posterior distribution $p(\theta|Z_i)$ has a parametric form in the exponential family,
then the local posteriors can easily be combined (Section \ref{sec:merge:ef}).
Under mild conditions, the \defn{Bernstein-von Mises theorem}\footnote{
    The Bernstein-von Mises theorem is also often called the Bayesian central limit theorem.
}
states that as the number of samples $n\to\infty$,
the distribution $p(\theta|Z_i)$ converges to a normal distribution. 
See for example Chapter 10.2 of \cite{vandervaart1998asymptotic} for a formal statement of the theorem with conditions.
\cite{neiswanger2014asymptotically} use this result to create a simple distributed learning procedure that can be thought of as the Bayesian alternative to parameter averaging (Section \ref{sec:merge:ave}). 
%
%That is,
In particular, they assume
\begin{equation}
    \p{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu_i}{\hat\Sigma_i}
\end{equation}
where $\hat\mu_i$ and $\hat\Sigma_i$ are local mean and covariance parameter estimates computed by either variational inference (Section \ref{sec:merge:vi}) or markov chain monte carlo (Section \ref{sec:merge:mcmc}).
The final posterior density is then given by
\begin{equation}
    \phat{\theta | Z}
    =
    \prod_{i=1}^m
    \phat{\theta | Z_i}
    \approx
    \gaussian{\theta}{\hat\mu}{\hat\Sigma}
    ,
\end{equation}
where
\begin{equation}
    \hat\Sigma
    =
    \left(
        \sum_{i=1}^m
        \hat\Sigma_i^{-1}
    \right)^{-1}
    ~~~~~\text{and}~~~~~
    %\\
    \hat\mu
    =
    \hat\Sigma \left(\sum_{i=1}^m \hat\Sigma^{-1} \mu_i \right)
    .
\end{equation}
Like averaging (see Section \ref{sec:merge:ave}),
this method reduces the variance but not the bias of the model parameters $\hat\mu$ and $\hat\Sigma$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Variational inference (VI)}
\label{sec:merge:vi}

Variational inference (VI) is a popular method for approximating intractable posterior distributions \citep{jordan1999introduction,blei2017variational}. 
%\cite{jordan1999introduction} and \cite{blei2017variational} provide detailed introductions.
%In this section we present VI as a form of ERM and three methods for distributed VI that fit our framework.
There are three existing methods of distributed VI that fit our framework.
To describe these methods,
we will present VI as a special case of ERM.
This is a non-standard presentation of VI,
but it highlights the similarities between distributed VI methods and the distributed ERM methods already presented.
In particular, we will see that the distributed VI methods are several years behind the distributed ERM methods,
and the distributed ERM methods can be directly applied to VI for improved performance.%
\footnote{
    It seems to be a trend that the optimization techniques used in variational methods lag behind the state-of-the-art non-bayesian learning approaches by several years.
    For example:
    (1) Stochastic gradient descent (SGD) was an established technique in large scale non-bayesian learning at least as early as \citet{bottou2004large},
    but SGD did not get applied to the variational bayes setting until \citet{hoffman2013stochastic}.
    Some papers \citep[e.g.][]{canini2009online,hoffman2010online,wang2011online} used online learning techniques earlier,
    but they only applied in special cases and still lagged behind the non-bayesian optimization research.
    (2) Differentially private empirical risk minimization has been known since \citet{chaudhuri2011differentially}, with privacy preserving models existing in special cases earlier \citep{chaudhuri2009privacy}.
    Differential privacy appears to have only been applied to variational inference so far in three papers \citep{karwa2015private,park2016variational,jalko2016differentially},
    all of which are currently under review and remain published only on the ArXiv.
    (3) Essentially no papers on variational inference provide learning guarantees (either finite sample or asymptotic) on the quality of the learned parameters,
    whereas hundreds (if not thousands) of these results exist for the ERM case.
}
As far as I know, there is no existing work on fast cross validation procedures for variational methods.
We will derive three new fast cross validation methods based on the distributed VI methods,
and \fixme{} new fast cross validation methods based on the distributed ERM methods.

Variational inference uses optimization to create a deterministic approximation to the posterior that is easy to compute.
Given a surrogate family of distributions $Q = \{ q(\theta|\w) : \w\in\W \}$,
the variational parameters are given by
\begin{equation}
    \label{eq:vi:wvi:kl}
    \wvi = \argmin_{\w\in\W} \kl{q(\theta|\w)}{p(\theta|Z)}
    .
\end{equation}
Solving \eqref{eq:vi:wvi:kl} is equivalent to the ERM problem
\begin{equation}
    \label{eq:vi:wvi:erm}
    \wvi = \argmin_{\w\in\W} \sum_{\z\in Z} \loss(\z;\w) + \lambda\reg(\w)
\end{equation}
where
\begin{equation}
    \loss(\z;\w) = -\int_{\theta\in\Theta} q(\theta|\w)\log p(\z|\theta) \dd\theta
    ~~~~~\text{and}~~~~~
    \reg(\w) = \kl{q(\theta|\w)}{p(\theta)}
    .
\end{equation}
The loss $\loss$ above is commonly known as the negative cross entropy.
Practitioners commonly choose the distributions $p$ and $q$ to be conjugate members of the exponential family,
in which case $\loss$ and $\reg$ will have closed form representations.
The ERM formulation has two main advantages.
First, the $\lambda$ hyperparameter directly controls the strength of the prior distribution and can be tuned via cross validation.
Second, we can apply standard tools from ERM to VI,
which seemingly hasn't been done before.
For example, all the previous methods for distributed ERM can now be directly applied to VI.

The equivalence between \eqref{eq:vi:wvi:kl} and \eqref{eq:vi:wvi:erm} follows because 
\begin{align}
    &\kl{q(\theta|\w)}{p(\theta|Z)}
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log\frac{q(\theta|\w)}{p(\theta|Z)} \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log q(\theta|\w)\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(\theta|Z) \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log q(\theta|\w)\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log \frac{p(Z|\theta)p(\theta)}{p(Z)} \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(Z|\theta) \dd\theta
   +\int_{\theta\in\Theta} q(\theta|\w)\log p(Z) \dd\theta
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log p(Z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\int_{\theta\in\Theta} q(\theta|\w)\log \prod_{\z\in Z}p(\z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \int_{\theta\in\Theta} q(\theta|\w)\log \frac{q(\theta|\w)}{p(\theta)}\dd\theta 
   -\sum_{\z\in Z}\int_{\theta\in\Theta} q(\theta|\w)\log p(\z|\theta) \dd\theta
   +\log p(Z)
    \\&=
    \reg(\w)
   +\sum_{\z\in Z} \loss(\z;\w)
   +\log p(Z)
\end{align}
Finally, since the term $\log p(Z)$ does not depend on $\w$ it can be removed from the optimization in \eqref{eq:vi:wvi:erm}. 

%we define the \defn{evidence lower bound} (ELBO) function as
%\begin{align}
    %\ELBO(\w) = \E\log p(\theta,\z) - \E\log q(\theta|\w)
    %%\ELBO(\w) &= \E\log p(\z|\theta) + \E\log p(\theta) - \E\log q(\theta|\w)
    %%\\
    %%&= -\KL{q(\theta|\w)}{p{\theta|Z}} + \log p(\z)
%\end{align}
%where the expectations are taken with respect to $q$.
%The variational parameters are then given by
%\begin{equation}
    %\wvi = \argmax_{\w\in\W} \ELBO(\w)
    %.
%\end{equation}
%The justification for this optimization is that maximizing the $\ELBO$ is equivalent to minimizing the KL-divergence between $q$ and $p$ \citep{blei2017variational}.
%The optimization is traditionally solved in the single machine setting using coordinate ascent.
%is used to approximate the posterior,
%and the variational parameter $\wvi$ is given by minimizing the KL divergence: 
%\begin{align}
    %\label{eq:vi:wvi}
    %\wvi 
    %&= \argmin_{\w\in\W} \kl{\q{\theta | \w}}{\p{\theta | Z}}
    %%\\
    %%&= \argmin_{\w\in\W} \E_q \q{\theta | \w} \log\frac{\q{\theta|\w}}{\p{\theta|Z}}
    %%\wvi = \argmin_{\w\in\W} \int_{\theta\in\Theta}\q{\theta | \w}\log\frac{\q{\theta | \w}}{\p{\theta | Z}}\dd\theta
    %.
%\end{align}
%To make the optimization in \eqref{eq:vi:wvi} tractable, we define the \defn{evidence lower bound} ($\ELBO$) function
%The most common form of VI is the mean field approximation.
%Here, the $q$ distribution factorizes as
%\begin{equation}
    %\q{\theta|\w} = \prod_{i=1}^d \qi{\theta^{(i)}|\w^{(i)}}
%\end{equation}
%where the superscript ${}^{(i)}$ denotes the $i$th component in the vector and
%the $q^{(i)}$ are univariate distributions.
%It is further common to assume that the $q^{(i)}$ are in the exponential family.

We now present the three methods for distributed VI that fit our framework.
\citet{broderick2013streaming} proposed the first method, 
\defn{streaming distributed asynchronous Bayes} (SDA-Bayes). 
SDA-Bayes is effectively a form of parameter averaging (see Section \ref{sec:merge:ave}) that also updates the regularization. 
Each local machine calculates the variational approximation $q(\theta|\wvi_i)$ locally,
then the merged parameters are given by
\begin{equation}
    \wsda = (1-m)\wvi_0+\sum_{i=1}^m\wvi_i
\end{equation}
where $\wvi_0$ denotes the hyper parameters of the prior distribution $p(\theta)$,
which is assumed to be in the same exponential family as the variational approximation $q(\theta|\wvi_i)$.

The merge formula for SDA-Bayes has the following probabilistic justification.
We can factor the global posterior into local posteriors and apply the exponential family assumption to get
\begin{align}
    p(\theta|Z) 
    &= p(\theta|Z_1,...,Z_m)
    \\
    &\propto \left( \prod_{i=1}^m p(Z_i|\theta) \right) p(\theta)
    \\
    &\propto \left( \prod_{i=1}^m \frac{p(\theta|Z_i)}{p(\theta)} \right) p(\theta)
    \\
    &= p(\theta)^{1-m} \prod_{i=1}^m p(\theta|Z_i)
    \\
    &\approx p(\theta)^{1-m} \prod_{i=1}^m q(\theta|\wvi_i)
    \\&=
    h(\theta)\exp\left(\trans{\left((1-m)\wvi_0+\sum_{i=1}^m\wvi_i\right)} T(\theta) - (1-m)\psi(\wvi_0)-\sum_{i=1}^m\psi(\wvi_i)\right)
    \\&\approx
    h(\theta)\exp\left(\trans{\left((1-m)\wvi_0+\sum_{i=1}^m\wvi_i\right)} T(\theta) - \psi\left((1-m)\wvi_0-\sum_{i=1}^m\wvi_i\right)\right)
    \\&=q(\theta|\wsda)
\end{align}
SDA-Bayes unfortunately provides no insight into how $\wsda$ compares to the single machine oracle VI parameters $\wvi$.
With parameter averaging, at least we know that the variance shrinks at the optimal rate even if the bias does not improve,
and we can use this result to determine how many machines we can realistically use for distributed computing.
For SDA, there is no similar result.

%The remaining two methods for distributed VI are improvements to SDA-Bayes.
%Parameter averaging (and hence SDA-Bayes) fails spectacularly when the model is non-identifiable.
%This rarely occurs in the models of Section \ref{sec:merge:ave}, 
%but nonidentifiability is common in the sort of models that require variational methods.

One limitation of SDA-Bayes is that it only works when the parameters of the variational family are identifiable.
We say that the variational family is \defn{identifiable} if for all $\w_1 \ne \w_2 \in W$, $q(\theta|\w_1)\ne q(\theta|\w_2)$.
One important class of nonidentifiability is parameter symmetry.
Let $S_d$ denote the group of permutation matrices of dimension $d$ 
(called the symmetric group of order $d$),
and recall that the variational parameter space $\W=\R^d$.
We say that the parameter space exhibits \defn{symmetry} if for any $P\in S_d$, $q(\theta|\w) = q(\theta|P\w)$.
Symmetry is commonly found in mixture models, 
and it complicates their learning.
\citet{campbell2014approximate} propose an extension to SDA-Bayes called \defn{approximate merging of posteriors with symmetries} (AMPS).
The AMPS estimator is given by
\begin{equation}
\wamps = (1-m)\wvi_0+\sum_{i=1}^mP_i\wvi_i
\end{equation}
where the $P_i$ are given by
\begin{equation}
    \{P_i\} = \argmax_{P_i\in S_d} \psi\left((1-m)\wvi_0+\sum_{i=1}^mP_i\wvi_i\right)
\end{equation}
The intuition behind AMPS is that we should permute all the matrices in such a way that they have the highest probability of being aligned correctly.
Maximizing the log partition function $\psi$ does this.
The idea of optimizing over permutation matrices to handle symmetry non-identifiabilities is useful in general ERM, not only variational methods.
No similar result exists for merging procedures for general ERM.

A second limitation of SDA-Bayes is that it requires the prior and variational families be conjugate.
\citet{gershman2012nonparametric} introduce \defn{nonparametric variational inference} (NVI), 
which is a single machine method that does not require conjugacy.
\citet{neiswanger2015embarrassingly} propose a distributed extension to NVI that fits our framework.
The central idea is that the variational family is the mixture 
\begin{equation}
    Q^\textit{nvi}= \left\{\frac{1}{k}\prod_{i=1}^k \normal{\theta}{\mu}{\Sigma} : \mu\in\R^d, \Sigma\text{ is a $d$ dimensional covariance matrix}\right\}
\end{equation}
and $k$ is a hyperparameter determining the number of mixture components.
Since $Q^\textit{nvi}$ is a family of mixture models, 
it exhibits parameter symmetry.
Rather than optimizing over rotation matrices like AMPS,
\citet{neiswanger2015embarrassingly} choose to use a merge procedure that uses sampling to align the components.
As the details are rather complicated,
we do not describe them here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Markov chain monte carlo (MCMC)}
\label{sec:merge:mcmc}

\defn{Markov chain monte carlo} (MCMC) is a stochastic method for approximating the posterior of a distribution \citep{andrieu2003introduction}.
MCMC provides a way to create samples from the posterior distribution,
and these samples can be used as a surrogate to a parametric form.
MCMC is typically slower than variational methods but more accurate.
As the number of MCMC samples approaches infinity,
the approximation error of MCMC shrinks to zero;
whereas the approximation error for variational methods is always some nonzero constant determined by the approximating distribution.
In this section, we will see two distributed MCMC methods that fit our framework.
A number of recent papers have developed approximate cross validation techniques for MCMC
\citep{marshall2003approximate,
bhattacharya2007importance,
bornn2010efficient,
held2010posterior,
vehtari2012survey,
li2016approximating}.
None of these fast cross validation algorithms is suitable for the distributed environment,
unlike the methods induced by our framework.

%Recent work has focused on improving non-asymptotic bounds of MCMC 
%\citep[e.g.][]{latuszynski2013nonasymptotic,andrieu2015convergence,adamczak2015exponential,andrieu2016establishing}.

\cite{neiswanger2014asymptotically} also present asymptotically efficient estimators.
We can write the kernel density estimate of the posterior as
\begin{equation}
    \pkde{\theta|Z}
    =
    \frac{1}{Z}\sum_{z\in Z}
    \frac{1}{h}k(\theta,z)
    =
    \frac{1}{Z}\sum_{z\in Z}
    \gaussian{\theta}{z}{h^2 \eye d}
    ,
\end{equation}
where $\eye d$ is the $d$ dimensional identity matrix.
\begin{align}
    \pnp{\theta|Z}
    &=
    \prod_{i=1}^m \pkde{\theta|Z_i}
    \\
    &=
    \frac{1}{n^m}
    \prod_{i=1}^m 
    \sum_{z\in Z_i}
    \gaussian{\theta}{z}{h^2 \eye d}
    \\
    &\propto
    \sum_{z_1\in Z_1}
    \dots
    \sum_{z_m\in Z_m}
    \gaussian{\theta}{z_{\{z_1,...,z_m\}}}{h^2 \eye d}
\end{align}
where
\begin{align}
    \bar\theta_{\{z_1,...,z_m\}} 
    &= 
    \frac{1}{m}\sum_{z_i\in\{z_1,...,z_m\}} z_i
    ,
    \\
    w_{\{z_1,...,z_m\}} 
    &=
    \prod_{i=1}^m\gaussian{z_i}{\bar\theta_{\{z_1,...,z_m\}}}{h^2\eye d}
\end{align}

\begin{theorem}
    If $h \asymp T^{-1/d}$,
    then
    \begin{equation}
        \mse{}
        \E((\phat{\theta} - \p{\theta})^2)
    \end{equation}
\end{theorem}

\citet{meeds2015optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\fixme{}
%\section{Examples}
%
%\subsection{The Free Monoid}
%
%\subsection{Moment Estimators}
%
%\subsection{Cupulae}
%
%\subsection{Empirical Characteristic function}
%\cite{yu2004empirical}
%
%
%\subsection{Information Theoretic Metric Learning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{bibfile}

\end{document}
